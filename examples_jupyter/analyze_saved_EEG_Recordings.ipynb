{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "tags": [
          "run_group_main",
          "run_unpickle_pipeline",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "%config IPCompleter.use_jedi = False\n",
        "%pdb off\n",
        "%load_ext autoreload\n",
        "%autoreload 3\n",
        "# %matplotlib inline\n",
        "%matplotlib qt5\n",
        "import mne\n",
        "mne.viz.set_browser_backend(\"qt\")  # or \"matplotlib\"\n",
        "mne.set_config(\"MNE_BROWSER_BACKEND\", \"qt\")  # or \"matplotlib\"\n",
        "%gui qt\n",
        "\n",
        "import IPython\n",
        "\n",
        "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Use MNE to load and analyze saved EEG and Motion recordings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "tags": [
          "run_group_main",
          "run_unpickle_pipeline",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import uuid\n",
        "from copy import deepcopy\n",
        "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
        "from nptyping import NDArray\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "import mne\n",
        "from mne import set_log_level\n",
        "from copy import deepcopy\n",
        "import mne\n",
        "\n",
        "from mne.io import read_raw\n",
        "\n",
        "datasets = []\n",
        "# mne.viz.set_browser_backend(\"Matplotlib\")\n",
        "mne.viz.set_browser_backend(\"qt\")\n",
        "\n",
        "from mne_lsl.player import PlayerLSL as Player\n",
        "from mne_lsl.stream import StreamLSL as Stream\n",
        "\n",
        "from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers\n",
        "from phoofflineeeganalysis.analysis.historical_data import HistoricalData\n",
        "from phoofflineeeganalysis.analysis.motion_data import MotionData\n",
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "from phoofflineeeganalysis.analysis.anatomy_and_electrodes import ElectrodeHelper\n",
        "# from ..EegProcessing import bandpower\n",
        "# from phoofflineeeganalysis.EegProcessing import analyze_eeg_trends\n",
        "from phoofflineeeganalysis.EegVisualization import VisHelpers\n",
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import SavedSessionsProcessor, SessionModality, DataModalityType\n",
        "\n",
        "set_log_level(\"WARNING\")\n",
        "\n",
        "# eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()\n",
        "# headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()\n",
        "\n",
        "# assert eeg_recordings_file_path.exists()\n",
        "# assert headset_motion_recordings_file_path.exists()\n",
        "\n",
        "eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()\n",
        "flutter_eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings').resolve()\n",
        "flutter_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/MOTION_RECORDINGS').resolve()\n",
        "flutter_GENERIC_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/GENERIC_RECORDINGS').resolve()\n",
        "\n",
        "headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()\n",
        "WhisperVideoTranscripts_LSL_Converted = Path(r\"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted\").resolve()\n",
        "pho_log_to_LSL_recordings_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/PhoLogToLabStreamingLayer_logs').resolve()\n",
        "## These contain little LSL .fif files with names like: '20250808_062814_log.fif', \n",
        "\n",
        "eeg_analyzed_parent_export_path = Path(\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed\").resolve()\n",
        "pickled_data_path = Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/PICKLED_COLLECTION\").resolve()\n",
        "assert pickled_data_path.exists()\n",
        "\n",
        "# n_most_recent_sessions_to_preprocess: int = None # None means all sessions\n",
        "# n_most_recent_sessions_to_preprocess: int = 35\n",
        "# n_most_recent_sessions_to_preprocess: int = 5\n",
        "n_most_recent_sessions_to_preprocess = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SavedSessionProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_main",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "sso: SavedSessionsProcessor = SavedSessionsProcessor(eeg_recordings_file_path=eeg_recordings_file_path,\n",
        "                                                     headset_motion_recordings_file_path=headset_motion_recordings_file_path, WhisperVideoTranscripts_LSL_Converted_file_path=WhisperVideoTranscripts_LSL_Converted, pho_log_to_LSL_recordings_path=pho_log_to_LSL_recordings_path,\n",
        "                                                    eeg_analyzed_parent_export_path=eeg_analyzed_parent_export_path, \n",
        "                                                     n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess, \n",
        "                                                    should_load_data=True, should_load_preprocessed=False,\n",
        "                                                    #  should_load_data=True, should_load_preprocessed=True,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_main"
        ]
      },
      "outputs": [],
      "source": [
        "(flat_data_modality_dict, found_recording_file_modality_dict) = sso.setup_specific_modality(modality_type=[DataModalityType.EEG, DataModalityType.MOTION, DataModalityType.PHO_LOG_TO_LSL, DataModalityType.WHISPER], should_load_data=False)\n",
        "\n",
        "## 3m 0.3s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2025-09-09 - Manual Read to determine only the new/modified EEG files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.historical_data import HistoricalData\n",
        "\n",
        "updated_file_paths, (pending_updated_recording_file_df, modern_found_EEG_recording_file_df, pre_processed_EEG_recording_file_df) = HistoricalData.discover_updated_recording_files(eeg_recordings_file_path=sso.eeg_recordings_file_path, eeg_analyzed_parent_export_path=sso.eeg_analyzed_parent_export_path)\n",
        "# updated_file_paths\n",
        "flat_data_modality_dict = HistoricalData.read_recording_files(found_recording_file_modality_dict={DataModalityType.EEG.name: updated_file_paths}, should_load_data=True)\n",
        "flat_data_modality_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(\n",
        "    eeg_recordings_file_path = sso.eeg_analyzed_parent_export_path,\n",
        "    # headset_motion_recordings_file_path = self.headset_motion_recordings_file_path,\n",
        "    # WhisperVideoTranscripts_LSL_Converted = self.WhisperVideoTranscripts_LSL_Converted_file_path,\n",
        "    # pho_log_to_LSL_recordings_path = self.pho_log_to_LSL_recordings_path,\n",
        "    should_load_data=False,\n",
        "    # should_load_data=sso.should_load_data,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# 1m 7sec\n",
        "\n",
        "\n",
        "\n",
        "# ## #TODO 2025-09-09 16:14: - [ ] Find the files that changed since last processing, and only load those:\n",
        "# flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(\n",
        "#     eeg_recordings_file_path = sso.eeg_recordings_file_path,\n",
        "#     headset_motion_recordings_file_path = sso.headset_motion_recordings_file_path,\n",
        "#     WhisperVideoTranscripts_LSL_Converted = sso.WhisperVideoTranscripts_LSL_Converted_file_path,\n",
        "#     pho_log_to_LSL_recordings_path = sso.pho_log_to_LSL_recordings_path,\n",
        "#     should_load_data=sso.should_load_data,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# prev_processed_modality_EEG: SessionModality = flat_data_modality_dict[DataModalityType.EEG.name]  ## Unpacking\n",
        "_, prev_processed_datasets, prev_processed_df_EEG = flat_data_modality_dict[DataModalityType.EEG.name]  ## Unpacking\n",
        "prev_processed_datasets\n",
        "# prev_processed_df_EEG\n",
        "\n",
        "\n",
        "[a_raw for a_raw in prev_processed_datasets]\n",
        "a_raw = prev_processed_datasets[-1]\n",
        "a_raw\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prev_processed_df_EEG: pd.DataFrame = prev_processed_modality_EEG.df  ## Unpacking\n",
        "prev_processed_df_EEG['duration'] = prev_processed_df_EEG['end_time'] - prev_processed_df_EEG['start_time']\n",
        "prev_processed_df_EEG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resume main pipleine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_main"
        ]
      },
      "outputs": [],
      "source": [
        "sso.perform_post_processing() # Filename '2024-09-11 15-27-36.lsl.fif' does not contain a recognized datetime format.\n",
        "## 3m 7.5s\n",
        "## 33m 42s - 578 records"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "written_EDF_file_paths = sso.save_to_EDF()\n",
        "written_EDF_file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keep record of the processed files so they don't have to be re-processed each time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Get the records of all procesed files\n",
        "eeg_df: pd.DataFrame = deepcopy(sso.modalities['EEG'].df)\n",
        "eeg_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for a_name in ('EEG', 'MOTION'):\n",
        "    if 'duration' not in sso.modalities[a_name].df.columns:\n",
        "        sso.modalities[a_name].df['duration'] = sso.modalities[a_name].df['end_time'] - sso.modalities[a_name].df['start_time']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso.modalities['PHO_LOG_TO_LSL'].df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "active_MOTION_idx: int = -3\n",
        "# a_raw_data: mne.io.Raw = sso.modalities['PHO_LOG_TO_LSL'].datasets[active_PHO_LOG_TO_LSL_idx]\n",
        "active_modality = sso.modalities['MOTION']\n",
        "an_active_idx = active_modality.active_indices[active_MOTION_idx]\n",
        "a_raw: mne.io.Raw = active_modality.datasets[an_active_idx]\n",
        "an_annotations = deepcopy(a_raw.annotations)\n",
        "an_annotations\n",
        "a_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw.info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.event_data import EventData\n",
        "\n",
        "## Get high-accel periods from the motion:\n",
        "active_modality: SessionModality = sso.modalities['MOTION']\n",
        "high_accel_periods = [active_modality.analysis_results[a_motion_IDX]['bad_periods_annotations']['high_accel'] for i, a_motion_IDX in enumerate(active_modality.active_indices)]\n",
        "high_accel_periods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "all_MOTION_df: pd.DataFrame = EventData.complete_correct_COMMON_annotation_df(a_logging_modality=active_modality, dataset_idx_col_name='MOTION_idx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_MOTION_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "active_EEG_idx: int = -15\n",
        "# a_raw_data: mne.io.Raw = sso.modalities['PHO_LOG_TO_LSL'].datasets[active_PHO_LOG_TO_LSL_idx]\n",
        "an_active_idx = sso.modalities['EEG'].active_indices[active_EEG_idx]\n",
        "a_raw: mne.io.Raw = sso.modalities['EEG'].datasets[an_active_idx]\n",
        "an_annotations: mne.Annotations = deepcopy(a_raw.annotations)\n",
        "an_annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_df: pd.DataFrame = deepcopy(sso.modalities['EEG'].df)\n",
        "eeg_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers\n",
        "\n",
        "(a_df, an_annotations_df) = MNEHelpers.get_raw_datetime_indexed_df(a_raw=a_raw, dt_col_names=['time'])\n",
        "a_df\n",
        "an_annotations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "an_annotations_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_simplify_MOTION_annotation_df(an_annotations_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    MOTION_idx_col_name: str = 'MOTION_idx'\n",
        "    ## INPUTS: an_annotations_df\n",
        "    groups = an_annotations_df.groupby((an_annotations_df['description'] != an_annotations_df['description'].shift()).cumsum()) # grouped by 'description' value\n",
        "    results = []\n",
        "    for _, g in groups:\n",
        "        if len(g) > 1:\n",
        "            first, last = g.iloc[0], g.iloc[-1]\n",
        "            duration: float = last['onset'] - first['onset']\n",
        "            # results.append({'description': first['description'], 'start': first['onset'], 'end': last['onset'], 'duration': duration})\n",
        "            _additional_col_names = ['filepath', 'filename', MOTION_idx_col_name, 'file_meas_date']\n",
        "            # _additional_cols = {'filepath': first['filepath'], 'filename': first['filename'], 'WHISPER_idx': first['WHISPER_idx'], 'file_meas_date': first['file_meas_date']}                \n",
        "            _additional_cols = {k:first[k] for k in _additional_col_names if (first.get(k, None) is not None)}\n",
        "            results.append({'onset': first['onset'], 'duration': duration, 'description': first['description'], **_additional_cols})\n",
        "\n",
        "    dedup_an_annotations_df = pd.DataFrame(results)\n",
        "    return dedup_an_annotations_df\n",
        "## OUTPUTS: dedup_an_annotations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.event_data import EventData\n",
        "\n",
        "## Get high-accel periods from the motion:\n",
        "active_modality: SessionModality = sso.modalities['MOTION']\n",
        "# high_accel_periods = [active_modality.analysis_results[a_motion_IDX]['bad_periods_annotations']['high_accel'].to_data_frame(time_format='datetime') for i, a_motion_IDX in enumerate(active_modality.active_indices)]\n",
        "high_accel_period_dfs = [perform_simplify_MOTION_annotation_df(an_annotations_df=active_modality.analysis_results[a_motion_IDX]['bad_periods_annotations']['high_accel'].to_data_frame(time_format='datetime')) for i, a_motion_IDX in enumerate(active_modality.active_indices)]\n",
        "high_accel_period_dfs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers\n",
        "\n",
        "## INPUTS: all_WHISPER_df, all_pho_log_to_lsl_df, high_accel_periods\n",
        "eeg_df: pd.DataFrame = deepcopy(sso.modalities['EEG'].df)\n",
        "\n",
        "# curr_eeg_min_dt = eeg_df[['start_time', 'end_time']]\n",
        "# curr_eeg_min_dt = eeg_df['start_time'].to_numpy()\n",
        "# curr_eeg_max_dt = eeg_df['end_time'].to_numpy()\n",
        "# curr_eeg_min_dt = eeg_df['start_time'].dt.to_pydatetime()\n",
        "# curr_eeg_max_dt = eeg_df['end_time'].dt.to_pydatetime()\n",
        "curr_eeg_min_dts = eeg_df['start_time'].dt.as_unit('s').dt.tz_localize(\"UTC\")\n",
        "curr_eeg_max_dts = eeg_df['end_time'].dt.as_unit('s').dt.tz_localize(\"UTC\")\n",
        "\n",
        "# np.shape(curr_eeg_min_dts)\n",
        "# np.shape(curr_eeg_max_dts)\n",
        "# curr_eeg_min_dt\n",
        "# all_pho_log_to_lsl_df['onset'].to_numpy()\n",
        "# curr_eeg_min_dt, curr_eeg_max_dt = (a_df['time'].min(), a_df['time'].max())\n",
        "\n",
        "# curr_eeg_min_dt, curr_eeg_max_dt = (a_df['time'].min(), a_df['time'].max())\n",
        "\n",
        "assert len(curr_eeg_min_dts) == len(curr_eeg_max_dts)\n",
        "n_eeg_start_end_entries: int = len(curr_eeg_max_dts)\n",
        "# found_pho_log_to_lsl_df_matches = []\n",
        "# found_WHISPER_df_matches = []\n",
        "found_pho_log_to_lsl_df_matches = {}\n",
        "found_WHISPER_df_matches = {}\n",
        "\n",
        "## INPUTS: motion_df, all_pho_log_to_lsl_df, all_WHISPER_df\n",
        "\n",
        "needs_modification_eeg_IDXs = []\n",
        "\n",
        "for an_xdf_dataset_idx in np.arange(n_eeg_start_end_entries):\n",
        "    curr_eeg_dataset_IDX: int = eeg_df['dataset_IDX'].to_numpy()[an_xdf_dataset_idx]\n",
        "    curr_eeg_min_dt = curr_eeg_min_dts[an_xdf_dataset_idx]\n",
        "    curr_eeg_max_dt = curr_eeg_max_dts[an_xdf_dataset_idx]\n",
        "    \n",
        "    # a_raw = sso.modalities['EEG'].datasets[curr_eeg_dataset_IDX]\n",
        "    a_raw: mne.io.Raw = sso.modalities['EEG'].datasets[curr_eeg_dataset_IDX]\n",
        "    an_annotations = deepcopy(a_raw.annotations)\n",
        "    (a_df, an_annotations_df) = MNEHelpers.get_raw_datetime_indexed_df(a_raw=a_raw, dt_col_names=['time'], debug_print=False)\n",
        "        \n",
        "\n",
        "    # np.shape(input_col) # 986\n",
        "    is_in_curr_eeg_sess = np.logical_and((curr_eeg_min_dt <= all_pho_log_to_lsl_df['onset'].to_numpy()), (all_pho_log_to_lsl_df['onset'].to_numpy() <= curr_eeg_max_dt))\n",
        "    # found_pho_log_to_lsl_df_matches.append(np.where(is_in_curr_eeg_sess)[0])\n",
        "    found_pho_log_to_lsl_df_matches[curr_eeg_dataset_IDX] = np.where(is_in_curr_eeg_sess)[0]\n",
        "    ## Add Pho_LOG Records:\n",
        "    # all_pho_log_to_lsl_df[is_in_curr_eeg_sess]\n",
        "    \n",
        "    is_in_curr_WHISPER_eeg_sess = np.logical_and((curr_eeg_min_dt <= all_WHISPER_df['onset'].to_numpy()), (all_WHISPER_df['onset'].to_numpy() <= curr_eeg_max_dt))\n",
        "    # found_WHISPER_df_matches.append(np.where(is_in_curr_WHISPER_eeg_sess)[0])\n",
        "    found_WHISPER_df_matches[curr_eeg_dataset_IDX] = np.where(is_in_curr_WHISPER_eeg_sess)[0]\n",
        "    ## Add Whisper Records:\n",
        "    # all_WHISPER_df[is_in_curr_WHISPER_eeg_sess]\n",
        "    if (len(found_pho_log_to_lsl_df_matches[curr_eeg_dataset_IDX]) > 0) or (len(found_WHISPER_df_matches[curr_eeg_dataset_IDX]) > 0):\n",
        "        print(f'i: curr_eeg_dataset_IDX: {curr_eeg_dataset_IDX} needs modification.')\n",
        "        needs_modification_eeg_IDXs.append(curr_eeg_dataset_IDX)\n",
        "\n",
        "## OUTPUTS: found_pho_log_to_lsl_df_matches, found_WHISPER_df_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_old_annotations_obj = {}\n",
        "_new_annotations_obj = {}\n",
        "\n",
        "for an_xdf_dataset_idx, curr_eeg_dataset_IDX in enumerate(needs_modification_eeg_IDXs):\n",
        "    # if i in [0, 1, 2]:\n",
        "    # curr_eeg_dataset_IDX: int = eeg_df['dataset_IDX'].to_numpy()[i]\n",
        "    # curr_eeg_min_dt = curr_eeg_min_dts[i]\n",
        "    # curr_eeg_max_dt = curr_eeg_max_dts[i]\n",
        "    \n",
        "    # a_raw = sso.modalities['EEG'].datasets[curr_eeg_dataset_IDX]\n",
        "    a_raw: mne.io.Raw = sso.modalities['EEG'].datasets[curr_eeg_dataset_IDX]\n",
        "    a_meas_date = a_raw.info.get('meas_date')\n",
        "    an_annotations = deepcopy(a_raw.annotations)\n",
        "    (a_df, an_annotations_df) = MNEHelpers.get_raw_datetime_indexed_df(a_raw=a_raw, dt_col_names=['time'], also_process_annotations=True, debug_print=False)\n",
        "    _old_annotations_obj[curr_eeg_dataset_IDX] = an_annotations\n",
        "    _old_num_annotations: int = len(an_annotations_df)\n",
        "    \n",
        "    all_annotations_df = [an_annotations_df]\n",
        "    ## Add Pho_LOG Records:\n",
        "    a_found_pho_log_to_lsl_df_matches = found_pho_log_to_lsl_df_matches[curr_eeg_dataset_IDX]\n",
        "    all_annotations_df.append(all_pho_log_to_lsl_df.iloc[a_found_pho_log_to_lsl_df_matches])\n",
        "\n",
        "    ## Add Whisper Records:\n",
        "    a_found_WHISPER_df_matches = found_WHISPER_df_matches[curr_eeg_dataset_IDX]\n",
        "    all_annotations_df.append(all_WHISPER_df.iloc[a_found_WHISPER_df_matches])\n",
        "    # all_annotations_df\n",
        "    \n",
        "    ## append all:\n",
        "    all_annotations_df = pd.concat(all_annotations_df, ignore_index=True, axis='index')\n",
        "    new_annotation_df_len: int = len(all_annotations_df)\n",
        "    if new_annotation_df_len > 0:\n",
        "        # Set new annotations object:\n",
        "        curr_onset = (all_annotations_df['onset'].to_numpy() - a_meas_date)\n",
        "        curr_onset_seconds = np.array([td.total_seconds() for td in curr_onset])\n",
        "        # new_annotations_obj = mne.Annotations(onset=all_annotations_df['onset'].to_numpy(), duration=all_annotations_df['duration'].to_numpy(), description=all_annotations_df['description'].to_numpy(), orig_time=a_meas_date)\n",
        "        new_annotations_obj = mne.Annotations(onset=curr_onset_seconds, duration=all_annotations_df['duration'].to_numpy(), description=all_annotations_df['description'].to_numpy(), orig_time=a_meas_date)\n",
        "        _new_annotations_obj[curr_eeg_dataset_IDX] = new_annotations_obj\n",
        "        # a_raw.set_annotations(new_annotations_obj) ## set annotations:\n",
        "\n",
        "# a_df\n",
        "# len(a_raw.annotations)\n",
        "# a_raw.set_annotations(\n",
        "# len(all_annotations_df)\n",
        "# all_annotations_df\n",
        "# Set new annotations object:\n",
        "# new_annotations_obj = mne.Annotations(onset=an_annotations_df['onset'].to_numpy(), duration=an_annotations_df['duration'].to_numpy(), description=an_annotations_df['description'].to_numpy(), orig_time=a_meas_date)\n",
        "# a_raw.set_annotations(new_annotations_obj)\n",
        "\n",
        "\n",
        "# an_annotations_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curr_onset_seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# _new_annotations_obj\n",
        "len(new_annotations_obj)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(an_annotations)\n",
        "\n",
        "len(an_annotations_df)\n",
        "an_annotations_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "all_pho_log_to_lsl_df[is_in_curr_eeg_sess]\n",
        "\n",
        "all_WHISPER_df[is_in_curr_WHISPER_eeg_sess]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "found_pho_log_to_lsl_df_matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "abs_start_t = a_ds.times[0] + a_ds.info['meas_date'].timestamp()\n",
        "abs_end_t = a_ds.times[-1] + a_ds.info['meas_date'].timestamp()\n",
        "\n",
        "# abs_start_t = pd.to_timedelta(a_ds.times[0]) + a_ds.info['meas_date']\n",
        "# abs_end_t = pd.to_timedelta(a_ds.times[-1]) + a_ds.info['meas_date']\n",
        "\n",
        "\n",
        "# type(abs_start_t)\n",
        "(abs_start_t, abs_end_t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "abs_start_dt = pd.to_datetime(abs_start_t)\n",
        "abs_end_dt = pd.to_datetime(abs_end_t)\n",
        "\n",
        "(abs_start_dt, abs_end_dt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "an_annotations.count()\n",
        "# an_annotations.to_data_frame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# np.shape(sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].get_data()) # (1, 94975)\n",
        "# sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].to_data_frame()\n",
        "# sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].to_data_frame()\n",
        "sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].annotations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025-09-18 - Complete Event Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_post_processing",
          "run_unpickle_pipeline"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.event_data import EventData\n",
        "\n",
        "all_pho_log_to_lsl_df: pd.DataFrame = EventData.complete_correct_Pho_Log_To_LSL_annotation_df(a_logging_modality=sso.modalities['PHO_LOG_TO_LSL'])\n",
        "all_pho_log_to_lsl_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_folder: Path = sso.eeg_analyzed_parent_export_path.joinpath('outputs')\n",
        "output_folder.mkdir(exist_ok=True)\n",
        "\n",
        "all_pho_log_to_lsl_df_csv_path = output_folder.joinpath('2025-09-18_PHO_LOG_TO_LSL.csv').resolve()\n",
        "all_pho_log_to_lsl_df_csv_path\n",
        "\n",
        "all_WHISPER_df_csv_path = output_folder.joinpath('2025-09-18_WHISPER.csv').resolve()\n",
        "all_WHISPER_df_csv_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_pho_log_to_lsl_df.to_csv(all_pho_log_to_lsl_df_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_post_processing",
          "run_unpickle_pipeline"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.event_data import EventData\n",
        "\n",
        "all_WHISPER_df: pd.DataFrame = EventData.complete_correct_WHISPER_annotation_df(a_logging_modality=sso.modalities['WHISPER'])\n",
        "all_WHISPER_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_WHISPER_df.to_csv(all_WHISPER_df_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "motion_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hdf5_out_path: Path = Path('E:\\Dropbox (Personal)\\Databases\\AnalysisData\\MNE_preprocessed').joinpath('2025-09-18_all_HDF.h5').resolve()\n",
        "hdf5_out_path\n",
        "\n",
        "\n",
        "all_WHISPER_df.drop(columns=['filepath']).to_hdf(hdf5_out_path, key='modalities/WHISPER/df', append=True)\n",
        "all_pho_log_to_lsl_df.drop(columns=['filepath']).to_hdf(hdf5_out_path, key='modalities/PHO_LOG_TO_LSL/df', append=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_pho_log_to_lsl_df.drop(columns=['filepath']).to_hdf(hdf5_out_path, key='modalities/PHO_LOG_TO_LSL/df', append=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "an_annotations = deepcopy(sso.modalities['EEG'].datasets[-1].annotations)\n",
        "an_annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "an_annotations.orig_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_to_motion_idx_list = eeg_df['motion_idxs'].to_list()\n",
        "eeg_to_motion_idx_list ## get the hard-index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_abs_motion_indicies = [[motion_active_indices[v] for v in k_list] for k_list in eeg_to_motion_idx_list] ## convert to absolute motion indicies\n",
        "eeg_abs_motion_indicies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "motion_df: pd.DataFrame = deepcopy(sso.modalities['MOTION'].df)\n",
        "motion_active_indices = deepcopy(sso.modalities['MOTION'].active_indices)\n",
        "\n",
        "motion_df\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_motion_artifacts(raw, picks=None, threshold_factor=6, min_duration=0.05, description='MOTION'):\n",
        "    picks = mne.pick_types(raw.info, eeg=True, meg=False, exclude='bads') if picks is None else picks\n",
        "    data, times = raw.get_data(picks=picks, return_times=True)\n",
        "    sfreq = raw.info['sfreq']\n",
        "    rms = np.sqrt(np.mean(data**2, axis=0))                        # cross-channel RMS time-series\n",
        "    med = np.median(rms)\n",
        "    mad = np.median(np.abs(rms - med)) or 1e-12                   # robust scale\n",
        "    thresh = med + threshold_factor * mad\n",
        "    mask = rms > thresh                                            # candidate samples\n",
        "    diff = np.diff(mask.astype(int))\n",
        "    starts = np.where(diff == 1)[0] + 1\n",
        "    ends = np.where(diff == -1)[0] + 1\n",
        "    if mask[0]:\n",
        "        starts = np.r_[0, starts]\n",
        "    if mask[-1]:\n",
        "        ends = np.r_[len(mask), ends]\n",
        "    if len(starts) == 0:\n",
        "        return mne.Annotations([], [], [])                         # no artifacts found\n",
        "    durations = (ends - starts) / sfreq\n",
        "    keep = durations >= min_duration\n",
        "    starts = starts[keep]\n",
        "    ends = ends[keep]\n",
        "    onsets = times[starts]\n",
        "    durs = (ends - starts) / sfreq\n",
        "    ann = mne.Annotations(onset=onsets.tolist(), duration=durs.tolist(), description=[description] * len(onsets))\n",
        "    raw.set_annotations(raw.annotations + ann)\n",
        "    return ann\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pickle Entire Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_path = Path(r\"C:/Users/pho/repos/EmotivEpoc/PhoLabStreamingReceiver/data\").resolve()\n",
        "# assert data_path.exists()\n",
        "\n",
        "# pkl_path = pickled_data_path.joinpath(\"2025-09-02_50records_SSO_all.pkl\").resolve()\n",
        "# pkl_path = pickled_data_path.joinpath(\"2025-09-08_ALL_records_SSO_all.pkl\").resolve()\n",
        "# pkl_path = pickled_data_path.joinpath(\"2025-09-10_35_records_SSO_all.pkl\").resolve()\n",
        "# pkl_path = pickled_data_path.joinpath(\"2025-09-12_35_records_SSO_all.pkl\").resolve()\n",
        "pkl_path = pickled_data_path.joinpath(\"2025-09-17_ALL_records_SSO_all.pkl\").resolve()\n",
        "sso.save(pkl_path=pkl_path)\n",
        "\n",
        "# 8m 20s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "## sso\n",
        "(all_data_EEG, all_times_EEG), datasets_EEG, df_EEG = sso.flat_data_modality_dict[DataModalityType.EEG.name]  ## Unpacking\n",
        "datasets_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso.modalities['EEG']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUTS: datasets_EEG\n",
        "\n",
        "# Assume raws is your list of Raw objects\n",
        "times = [raw.info['meas_date'] for raw in datasets_EEG]\n",
        "df = pd.DataFrame({'start': times})\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "ax.scatter(df['start'], [1]*len(df), marker='o')\n",
        "\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
        "ax.set_yticks([])\n",
        "\n",
        "plt.title(\"EEG Recording Sessions by Date\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot All Sessions on a timeline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computations (Spectogram, etc) for all sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modality_EEG: SessionModality = sso.modalities[DataModalityType.EEG.name]  ## Unpacking\n",
        "df_EEG: pd.DataFrame = modality_EEG.df  ## Unpacking\n",
        "df_EEG['duration'] = df_EEG['end_time'] - df_EEG['start_time']\n",
        "df_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a_raw = modality_EEG.datasets[-1]# modality_EEG.active_indices\n",
        "a_raw = modality_EEG.datasets[-3]\n",
        "\n",
        "## feels yucky, like head is very hot. Sad about pills.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_channel_groups_dict = {\n",
        "'frontal': (['AF3', 'F7', 'FC5', 'F3'], ['AF4', 'F8', 'FC6', 'F4'])\n",
        "}\n",
        "\n",
        "\n",
        "ch_name_list = deepcopy(a_raw.info.ch_names)\n",
        "ch_name_to_idx_dict = dict(zip(a_raw.info.ch_names, np.arange(len(a_raw.info.ch_names))))\n",
        "ch_name_to_idx_dict\n",
        "\n",
        "\n",
        "cust_L, cust_R = custom_channel_groups_dict['frontal']\n",
        "cust_BOTH = cust_L + cust_R\n",
        "\n",
        "cust_L_idxs, cust_R_idxs = (np.array([ch_name_to_idx_dict[ch] for ch in cust_L]), np.array([ch_name_to_idx_dict[ch] for ch in cust_R]))\n",
        "cust_BOTH_idxs = np.array([ch_name_to_idx_dict[ch] for ch in cust_BOTH])\n",
        "cust_BOTH_idxs\n",
        "# .channel_names\n",
        "\n",
        "## OUTPUTS: cust_BOTH_idxs,\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "\n",
        "## INPUTS: concatenated_raw\n",
        "a_raw = concatenated_raw\n",
        "cust_L, cust_R = custom_channel_groups_dict['frontal']\n",
        "cust_L_idxs, cust_R_idxs = (np.array([ch_name_to_idx_dict[ch] for ch in cust_L]), np.array([ch_name_to_idx_dict[ch] for ch in cust_R]))\n",
        "cust_BOTH_idxs = np.array([ch_name_to_idx_dict[ch] for ch in cust_BOTH])\n",
        "a_raw_outputs = EEGComputations.run_all(raw=a_raw, should_suppress_exceptions=False)\n",
        "a_raw_outputs # 27s (epoch_step: float = 0.250, moving_avg_epochs: int = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.shape(a_raw_outputs['raw_data_topo']['epoch_avg']) # (n_channels, 512) \n",
        "\n",
        "np.shape(a_raw_outputs['raw_data_topo']['mov_avg']) # (n_time_windows, n_channels, 512) \n",
        "\n",
        "## Process for desired channels\n",
        "## INPUTS: cust_BOTH_idxs, cust_L_idxs, cust_R_idxs\n",
        "\n",
        "active_channel_names = ch_name_to_idx_dict\n",
        "mov_avg_filtered = a_raw_outputs['raw_data_topo']['mov_avg'][:, cust_BOTH_idxs, :]\n",
        "np.shape(mov_avg_filtered)\n",
        "# cust_L_idxs]\n",
        "\n",
        "\n",
        "\n",
        "# mov_avg_all_channels_stack = \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import napari\n",
        "\n",
        "viewer = napari.Viewer(ndisplay=3)\n",
        "viewer.add_image(mov_avg_filtered)\n",
        "# viewer.add_points(mov_avg_filtered, size=2, face_color='red', name='mov_avg_filtered')\n",
        "# viewer.add_points(data2, size=2, face_color='green', name='dataset2')\n",
        "# viewer.add_points(data3, size=2, face_color='blue', name='dataset3')\n",
        "\n",
        "napari.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyqtgraph as pg\n",
        "\n",
        "app = pg.mkQApp(\"Plotting Example\")\n",
        "#mw = QtWidgets.QMainWindow()\n",
        "#mw.resize(800,800)\n",
        "\n",
        "win = pg.GraphicsLayoutWidget(show=True, title=\"Basic plotting examples\")\n",
        "win.resize(1000,600)\n",
        "win.setWindowTitle('pyqtgraph example: Plotting')\n",
        "\n",
        "# Enable antialiasing for prettier plots\n",
        "pg.setConfigOptions(antialias=True)\n",
        "\n",
        "view = {}\n",
        "img = {}\n",
        "\n",
        "active_channel_names = cust_BOTH\n",
        "active_data = a_raw_outputs['raw_data_topo']['mov_avg']\n",
        "n_time_windows, n_channels, n_unknown = np.shape(active_data)\n",
        "\n",
        "for channel_idx in np.arange(n_channels):\n",
        "    ch_name: str = ch_name_list[channel_idx]\n",
        "    if ch_name in active_channel_names:\n",
        "        variable_name = f'epoch.avgr[{ch_name}]'\n",
        "        view[variable_name] = win.addViewBox()\n",
        "        view[variable_name].setAspectLocked(False)\n",
        "        img[variable_name] = pg.ImageItem(np.squeeze(active_data[:, channel_idx, :]).T)\n",
        "        view[variable_name].addItem(img[variable_name])\n",
        "\n",
        "\t\n",
        "# variable_name = 'epoch.avgr'\n",
        "# view[variable_name] = win.addViewBox()\n",
        "# view[variable_name].setAspectLocked(True)\n",
        "# img[variable_name] = pg.ImageItem(_all_outputs['raw_data_topo']['epoch_avg'].T)\n",
        "# view[variable_name].addItem(img[variable_name])\n",
        "\n",
        "\n",
        "# mov_avg_filtered\n",
        "\n",
        "# variable_name = 'cwt.power'\n",
        "# np.shape(_all_outputs['cwt']['power']) # (n_channels, n_freq_bands, n_timesteps) \n",
        "# view[variable_name] = win.addViewBox()\n",
        "# view[variable_name].setAspectLocked(True)\n",
        "# img[variable_name] = pg.ImageItem(_all_outputs['cwt']['power'])\n",
        "# view[variable_name].addItem(img[variable_name])\n",
        "\n",
        "win.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "win.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "p1 = win.addPlot(title=\"Basic array plotting\", y=np.random.normal(size=100))\n",
        "\n",
        "p2 = win.addPlot(title=\"Multiple curves\")\n",
        "p2.plot(np.random.normal(size=100), pen=(255,0,0), name=\"Red curve\")\n",
        "p2.plot(np.random.normal(size=110)+5, pen=(0,255,0), name=\"Green curve\")\n",
        "p2.plot(np.random.normal(size=120)+10, pen=(0,0,255), name=\"Blue curve\")\n",
        "\n",
        "p3 = win.addPlot(title=\"Drawing with points\")\n",
        "p3.plot(np.random.normal(size=100), pen=(200,200,200), symbolBrush=(255,0,0), symbolPen='w')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "pg.Plo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "import neurokit2 as nk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.filter import filter_data  # Slightly faster for large data as direct fn\n",
        "import scipy.ndimage\n",
        "\n",
        "# a_raw = concatenated_raw\n",
        "raw = a_raw\n",
        "epoch_step = 0.250\n",
        "moving_avg_epochs: int = 8\n",
        "\n",
        "# # 1. Temporal filter: filter modifies in-place if not copying\n",
        "# raw_filtered = raw.copy().filter(l_freq=1, h_freq=58, fir_design='firwin', n_jobs='cuda')\n",
        "\n",
        "# # 2. Epoching (sliding window)\n",
        "# sfreq = raw.info['sfreq']\n",
        "# step_samples = int(epoch_step * sfreq)\n",
        "# window_samples = int(epoch_dur * sfreq)\n",
        "# data = raw_filtered.get_data()\n",
        "# n_ch, n_times = data.shape\n",
        "\n",
        "# # Use strided view for efficient sliding windows (no explicit python loops)\n",
        "# def epoch_strided(data, window_samples, step_samples):\n",
        "#     n_epochs = (n_times - window_samples) // step_samples + 1\n",
        "#     s0, s1 = data.strides\n",
        "#     return np.lib.stride_tricks.as_strided(\n",
        "#         data,\n",
        "#         shape=(n_epochs, n_ch, window_samples),\n",
        "#         strides=(step_samples * s1, s0, s1),\n",
        "#         writeable=False\n",
        "#     )\n",
        "\n",
        "# epochs = epoch_strided(data, window_samples, step_samples)\n",
        "\n",
        "# shape: (n_epochs, n_ch, n_samples)\n",
        "out_dict = a_raw_outputs['raw_data_topo']\n",
        "epochs = a_raw_outputs['raw_data_topo']['epochs']\n",
        "\n",
        "# 3. xÂ²\n",
        "epochs_squared = np.square(out_dict['epochs'])\n",
        "\n",
        "# 4. Moving epoch average (vectorized via convolution)\n",
        "# kernel = np.ones(moving_avg_epochs) / moving_avg_epochs\n",
        "# pad so output is same shape (len=n_epochs)\n",
        "# mov_avg = np.apply_along_axis(\n",
        "#     lambda m: np.vstack([np.convolve(m[:, ch, samp], kernel, mode='full')[:len(m)] \n",
        "#                         for ch in range(n_ch) for samp in range(window_samples)]\n",
        "#                     ).reshape((n_ch, window_samples, len(m))).transpose(2, 0, 1),\n",
        "#     axis=0, arr=epochs_squared[None,...]).squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# concatenated_raw.annotations\n",
        "concatenated_raw.plot(block=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bads, an_info = nk.eeg_badchannels(concatenated_raw, distance_threshold=0.95, show=False)\n",
        "bads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "custom_channel_groups_dict = {\n",
        "'frontal': (['AF3', 'F7', 'FC5', 'F3'], ['AF4', 'F8', 'FC6', 'F4'])\n",
        "}\n",
        "\n",
        "sampling_rate = concatenated_raw.info[\"sfreq\"]  # Store the sampling rate\n",
        "sampling_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Channels of interest\n",
        "# channels = [\"EEG 011\", \"EEG 012\", \"EEG 013\", \"EEG 014\"]\n",
        "# channels = [\"EEG 0\" + str(i) for i in range(11, 15)]  # Alternative\n",
        "channels = ['AF3', 'F7', 'FC5', 'F3', 'AF4', 'F8', 'FC6', 'F4']\n",
        "# Extract channel, convert to array, and average the channels\n",
        "signal = concatenated_raw.pick_channels(channels).get_data().mean(axis = 0)\n",
        "\n",
        "nk.signal_plot(signal, sampling_rate=sampling_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "delay = int(27 / (1000 / sampling_rate))  # 27 is the delay in milliseconds\n",
        "\n",
        "# Note that for the example we cropped the signal to cut computation time\n",
        "svden, an_info = nk.entropy_svd(signal[0:500], delay=delay, dimension=3) # , show=True\n",
        "\n",
        "print(f\"SVDEn score: {svden:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Extract microstates\n",
        "microstates = nk.microstates_segment(concatenated_raw, n_microstates=4)\n",
        "\n",
        "# Visualize the extracted microstates\n",
        "nk.microstates_plot(microstates, epoch=(0, 500))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "microstates_all = nk.microstates_segment(concatenated_raw, n_microstates=4, train=\"all\")\n",
        "nk.microstates_plot(microstates_all, epoch=(0, 500))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "events_from_annot, event_dict = mne.events_from_annotations(raw)# Get events and event_id from an Annotations object.\n",
        "event_dict = {'74':74, '75':75, '76':76}                        # Event dictionaries to extract epochs from continuous data, \n",
        "reject_criteria = dict(eeg=100e-6)                              # Absolute Amplitude of each epoch sould be smaller than 100 Î¼V\n",
        "                                                                # tmin is start time before event, tmax is end time after event\n",
        "                                                                # - 100 ms (baseline) of cue's onset to 600 ms\n",
        "epochs = mne.Epochs(raw, events_from_annot, event_id=event_dict, tmin=-0.1, tmax=1.6,\n",
        "                    reject=reject_criteria, baseline = (None,0), preload=True, picks=['eeg'])\n",
        "epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw_outputs['cwt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Usage\n",
        "# raw = a_raw\n",
        "freqs, power = a_raw_outputs['cwt']['freqs'], a_raw_outputs['cwt']['power']\n",
        "EEGComputations.plot_eeg_with_spectrogram(a_raw, freqs, power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "raw = a_raw\n",
        "data_topo = a_raw_outputs['raw_data_topo']['topo']\n",
        "mne.viz.plot_topomap(data_topo, raw.info, show=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mne\n",
        "from mne import create_info\n",
        "from mne.io import RawArray\n",
        "mne.viz.set_browser_backend(\"qt\")  # or \"matplotlib\"\n",
        "mne.set_config(\"MNE_BROWSER_BACKEND\", \"qt\")  # or \"matplotlib\"\n",
        "# from mne_qt_browser import MNEQtBrowser\n",
        "\n",
        "raw = a_raw\n",
        "epoch_avg = a_raw_outputs['raw_data_topo']['epoch_avg']\n",
        "\n",
        "# Assuming epoch_avg from the previous pipeline\n",
        "n_ch, n_samples = epoch_avg.shape\n",
        "sfreq = raw.info['sfreq']\n",
        "\n",
        "# create new RawArray\n",
        "an_info = create_info(ch_names=raw.ch_names, sfreq=sfreq, ch_types='eeg')\n",
        "raw_processed = RawArray(epoch_avg, an_info)\n",
        "\n",
        "# launch scrollable Qt browser\n",
        "# browser = QtBrowser(raw_processed)\n",
        "# browser = raw_processed.plot(block=True)\n",
        "# browser.show()\n",
        "\n",
        "browser = raw_processed.plot(block=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modality_PHO_LOG: SessionModality = sso.modalities[DataModalityType.PHO_LOG_TO_LSL.name]  ## Unpacking\n",
        "df_PHO_LOG: pd.DataFrame = modality_PHO_LOG.df  ## Unpacking\n",
        "df_PHO_LOG['duration'] = df_PHO_LOG['end_time'] - df_PHO_LOG['start_time']\n",
        "df_PHO_LOG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modality_WHISPER: SessionModality = sso.modalities[DataModalityType.WHISPER.name]  ## Unpacking\n",
        "df_WHISPER: pd.DataFrame = modality_WHISPER.df  ## Unpacking\n",
        "df_WHISPER['duration'] = df_WHISPER['end_time'] - df_WHISPER['start_time']\n",
        "df_WHISPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modality_WHISPER.analysis_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goal: find just the sessions that happened today, from all modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import EntireDayMergedData\n",
        "\n",
        "## Flatten the EEG sessions into a single dataset for the entire day\n",
        "search_day_date = datetime(2025, 9, 8)\n",
        "# search_day_date = datetime(2025, 9, 10)\n",
        "\n",
        "today_only_eeg_modality = sso.modalities[\"EEG\"].filtered_by_day_date(search_day_date=search_day_date)\n",
        "today_only_eeg_modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso.modalities['PHO_LOG_TO_LSL'].df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].to_data_frame()\n",
        "\n",
        "sso.modalities['PHO_LOG_TO_LSL'].datasets[-1].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "today_only_eeg_modality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "concatenated_raw = EntireDayMergedData.find_and_merge_for_day_date(sso=sso, search_day_date=search_day_date)\n",
        "concatenated_raw\n",
        "\n",
        "# concatenated_raw = EntireDayMergedData.concatenate_datasets(today_only_modalities['EEG'].datasets)\n",
        "# concatenated_raw\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_raw.plot(block=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import EntireDayMergedData\n",
        "\n",
        "\n",
        "search_day_date = datetime(2025, 9, 8)\n",
        "# search_day_date = datetime(2025, 9, 5)\n",
        "# filtered_df_EEG = df_EEG[df_EEG['day'] == search_day_date]\n",
        "\n",
        "# today_only_modalities = deepcopy(sso.modalities)\n",
        "today_only_modalities = {}\n",
        "\n",
        "# a_modality_name = 'EEG'\n",
        "# a_modality = sso.modalities[a_modality_name]\n",
        "\n",
        "for a_modality_name, a_modality in sso.modalities.items():\n",
        "# print(f'a_modality_name: {a_modality_name}, n_datasets: {len(a_modality.datasets)}')\n",
        "    if ('dataset_IDX' not in a_modality.df.columns) and ('motion_dataset_IDX' in a_modality.df.columns):\n",
        "        a_modality.df['dataset_IDX'] = a_modality.df['motion_dataset_IDX']\n",
        "    today_only_modalities[a_modality_name] = a_modality.filtered_by_day_date(search_day_date=search_day_date)        \n",
        "\n",
        "## OUT: today_only_modalities\n",
        "\n",
        "## 41.5s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "today_only_modalities['PHO_LOG_TO_LSL'].datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_modality.df # 'motion_dataset_IDX'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import EntireDayMergedData\n",
        "\n",
        "\n",
        "a_modality_name = 'MOTION'\n",
        "concatenated_motion = EntireDayMergedData.concatenate_datasets(today_only_modalities[a_modality_name].datasets)\n",
        "concatenated_motion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_motion.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enhanced Motion Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_channels_only = concatenated_motion.pick(picks=['AccX', 'AccY', 'AccZ'])\n",
        "# acc_channels_only.plot(scalings='auto', block=True)\n",
        "\n",
        "acc_channels_only_df: pd.DataFrame = acc_channels_only.to_data_frame()\n",
        "acc_channels_only_df\n",
        "acc_column_names = ['time', 'AccX', 'AccY', 'AccZ']\n",
        "diff_column_names = ['dt', 'dx', 'dy', 'dz']\n",
        "acc_channels_only_df[diff_column_names] = acc_channels_only_df[acc_column_names].diff(axis='index')\n",
        "# acc_channels_only_df[['dx', 'dy', 'dz']] = acc_channels_only_df[['dx', 'dy', 'dz']] / acc_channels_only_df['dt']  # velocity = distance / time\n",
        "# acc_channels_only_df['dx'] = acc_channels_only_df['dx'] / acc_channels_only_df['dt']  # velocity = distance / time\n",
        "# acc_channels_only_df['dy'] = acc_channels_only_df['dy'] / acc_channels_only_df['dt']\n",
        "# acc_channels_only_df['dz'] = acc_channels_only_df['dz'] / acc_channels_only_df['dt']\n",
        "\n",
        "diff_to_smoothed_diff_mapping = {}\n",
        "for a_chan_name in diff_column_names[1:]:\n",
        "    a_smoothed_channel_name = f'smoothed_{a_chan_name}'\n",
        "    diff_to_smoothed_diff_mapping[a_chan_name] = a_smoothed_channel_name\n",
        "    acc_channels_only_df[a_smoothed_channel_name] = acc_channels_only_df[a_chan_name].rolling(window=5, min_periods=1).mean()\n",
        "\n",
        "acc_channels_only_df\n",
        "acc_channels_only_df['total_motion'] = np.sqrt(acc_channels_only_df['dx']**2 + acc_channels_only_df['dy']**2 + acc_channels_only_df['dz']**2)\n",
        "# acc_channels_only_df['smoothed_total_motion'] = acc_channels_only_df['total_motion'].rolling(window=10, min_periods=1).mean()\n",
        "acc_channels_only_df['smoothed_total_motion'] = acc_channels_only_df['total_motion'].rolling(window=5, min_periods=1).mean()\n",
        "acc_channels_only_df['max_smoothed_total_motion'] = acc_channels_only_df['total_motion'].rolling(window=5, min_periods=1).max()\n",
        "acc_channels_only_df['total_motion'].plot.hist(bins=100, alpha=0.5)\n",
        "acc_channels_only_df['smoothed_total_motion'].plot.hist(bins=100, alpha=0.5)\n",
        "acc_channels_only_df['max_smoothed_total_motion'].plot.hist(bins=100, alpha=0.5)\n",
        "plt.legend(['total_motion', 'smoothed_total_motion', 'max_smoothed_total_motion'])\n",
        "\n",
        "acc_channels_only_df\n",
        "acc_channels_only_df.describe()\n",
        "max_high_motion_threshold: float = 2.5\n",
        "is_high_motion = (acc_channels_only_df['max_smoothed_total_motion'] > max_high_motion_threshold)\n",
        "\n",
        "high_motion_times = acc_channels_only_df[is_high_motion]['time'].values\n",
        "high_motion_times\n",
        "\n",
        "## OUTPUTS: high_motion_times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _data_to_sonification_mapper_fn(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    amplitudes = None \n",
        "    \n",
        "    map_col_names = ['dx', 'dy', 'dz']\n",
        "    n_map_cols = len(map_col_names)\n",
        "    # frequencies = np.zeros(shape=(n_map_cols, ))\n",
        "    frequencies = [220, 440, 880]  # A3, A4, A5\n",
        "    amplitudes = np.zeros(shape=(n_map_cols, ))                         \n",
        "\n",
        "    data = data.copy()\n",
        "    data['sonification_value'] = data['total_motion']  # Example: direct mapping\n",
        "    \n",
        "\n",
        "    ## returns (frequencies, amplitudes, resolution)\n",
        "\n",
        "    # frequencies, amplitudes, resolution\n",
        "    \n",
        "        \n",
        "\n",
        "    return (frequencies, amplitudes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_channels_only_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_channels_only_df['total_motion'].plot.hist(bins=100, alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for a_modality_name, a_modality in sso.modalities.items():\n",
        "# print(f'a_modality_name: {a_modality_name}, n_datasets: {len(a_modality.datasets)}')\n",
        "today_only_modalities[a_modality_name] = a_modality.filtered_by_day_date(search_day_date=search_day_date)\n",
        "today_only_modalities[a_modality_name].df\n",
        "## OUT: today_only_modalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "perform_extended_post_processing_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso.modalities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import EntireDayMergedData\n",
        "\n",
        "\n",
        "## INPUTS: search_day_date\n",
        "today_only_sso = deepcopy(sso)\n",
        "\n",
        "# today_only_modalities = deepcopy(sso.modalities)\n",
        "today_only_modalities = {}\n",
        "\n",
        "for a_modality_name, a_modality in sso.modalities.items():\n",
        "    # print(f'a_modality_name: {a_modality_name}, n_datasets: {len(a_modality.datasets)}')\n",
        "    # if ('dataset_IDX' not in a_modality.df.columns) and ('motion_dataset_IDX' in a_modality.df.columns):\n",
        "    #     a_modality.df['dataset_IDX'] = a_modality.df['motion_dataset_IDX']\n",
        "    today_only_modalities[a_modality_name] = a_modality.filtered_by_day_date(search_day_date=search_day_date)        \n",
        "\n",
        "## OUT: today_only_modalities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_raw.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "concatenated_raw.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "today_only_modalities['EEG'].datasets[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Save out all files that have been successfully pre-processed\n",
        "all_loaded_files = []\n",
        "for a_modality_name, a_modality in sso.modalities.items():\n",
        "    print(f'a_modality_name: {a_modality_name}, n_datasets: {len(a_modality.datasets)}')\n",
        "    all_loaded_files.extend([a_ds.filenames[0] for a_ds in a_modality.datasets if a_ds is not None])\n",
        "    \n",
        "# all_loaded_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_loaded_files\n",
        "\n",
        "## List of loaded files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Pickled Entire Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_unpickle_pipeline"
        ]
      },
      "outputs": [],
      "source": [
        "# sso: SavedSessionsProcessor = SavedSessionsProcessor.load(pkl_file=Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/PICKLED_COLLECTION/2025-09-02_50records_SSO_all.pkl\").resolve())\n",
        "# pkl_path = pickled_data_path.joinpath(\"2025-09-10_35_records_SSO_all.pkl\").resolve()\n",
        "pkl_path = pickled_data_path.joinpath(\"2025-09-17_ALL_records_SSO_all.pkl\").resolve()\n",
        "sso: SavedSessionsProcessor = SavedSessionsProcessor.load(pkl_file=pkl_path)\n",
        "sso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup single modality independently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(flat_data_modality_dict, found_recording_file_modality_dict) = sso.setup_specific_modality(modality_type=[DataModalityType.WHISPER], should_load_data=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load RAW (original) data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(\n",
        "                eeg_recordings_file_path = eeg_recordings_file_path,\n",
        "                headset_motion_recordings_file_path = headset_motion_recordings_file_path,\n",
        "                # WhisperVideoTranscripts_LSL_Converted = WhisperVideoTranscripts_LSL_Converted,\n",
        "                pho_log_to_LSL_recordings_path = pho_log_to_LSL_recordings_path,\n",
        "                # should_load_data=True,\n",
        "\t\t\t\tshould_load_data=False,\n",
        ")\n",
        "\n",
        "\n",
        "# flat_data_modality_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load pre-proocessed EEG data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Load pre-proocessed EEG data:\n",
        "flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(\n",
        "                eeg_recordings_file_path = eeg_analyzed_parent_export_path,\n",
        "                headset_motion_recordings_file_path = headset_motion_recordings_file_path,\n",
        "                WhisperVideoTranscripts_LSL_Converted = WhisperVideoTranscripts_LSL_Converted,\n",
        "                pho_log_to_LSL_recordings_path = pho_log_to_LSL_recordings_path,\n",
        "                # should_load_data=True,\n",
        "                should_load_data=False,\n",
        ")\n",
        "\n",
        "# 1m 10s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Continue with processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'WHISPER'\n",
        "_, datasets_WHISPER, _ = sso.flat_data_modality_dict['WHISPER']  ## Unpacking\n",
        "(active_WHISPER_IDXs, analysis_results_WHISPER) = sso.modalities['WHISPER'].active_indices, sso.modalities['WHISPER'].analysis_results\n",
        "# datasets_WHISPER\n",
        "analysis_results_WHISPER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets_PHO_LOG[-1].filenames[0].name # '20250820_035626_log.fif'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### EEG Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(all_data_EEG, all_times_EEG), datasets_EEG, df_EEG = sso.flat_data_modality_dict['EEG']  ## Unpacking\n",
        "datasets_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGData\n",
        "\n",
        "\n",
        "## BEGIN ANALYSIS of EEG Data\n",
        "preprocessed_EEG_save_path: Path = eeg_analyzed_parent_export_path.joinpath('preprocessed_EEG').resolve()\n",
        "preprocessed_EEG_save_path.mkdir(exist_ok=True)\n",
        "\n",
        "## INPUTS: flat_data_modality_dict\n",
        "(all_data_EEG, all_times_EEG), datasets_EEG, df_EEG = sso.flat_data_modality_dict['EEG']  ## Unpacking\n",
        "\n",
        "active_EEG_IDXs, analysis_results_EEG = EEGData.preprocess(datasets_EEG=datasets_EEG,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t    # preprocessed_EEG_save_path=preprocessed_EEG_save_path,\n",
        "                                                            preprocessed_EEG_save_path=None, ## do not write out to disk yet, do after aligning motion data:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t    n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess)\n",
        "\n",
        "\n",
        "# datasets_EEG\n",
        "\n",
        "# ~30m 30s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.historical_data import HistoricalData\n",
        "\n",
        "## BEGIN ANALYSIS of EEG Data\n",
        "preprocessed_EEG_save_path: Path = eeg_analyzed_parent_export_path.joinpath('preprocessed_EEG').resolve()\n",
        "preprocessed_EEG_save_path.mkdir(exist_ok=True)\n",
        "\n",
        "dataset_MOTION_df, dataset_EEG_df = HistoricalData.add_bad_periods_from_MOTION_data(active_EEG_IDXs=active_EEG_IDXs, datasets_EEG=datasets_EEG,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t active_motion_IDXs=active_motion_IDXs, datasets_MOTION=datasets_MOTION, analysis_results_MOTION=analysis_results_MOTION,\n",
        "                                                preprocessed_EEG_save_path=preprocessed_EEG_save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_MOTION_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# datasets_EEG[-3].plot()\n",
        "raw = datasets_EEG[-2]\n",
        "meas_datetime = raw.info.get('meas_date', None)\n",
        "meas_datetime\n",
        "# raw.plot(block=True)\n",
        "\n",
        "raw.describe(data_frame=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "eeg_CSV_export_path = eeg_CSV_export_parent_path.joinpath('2025-08-14_eeg_MNE_merged.csv')\n",
        "# df_EEG.to_csv(eeg_CSV_export_path) ## way too big\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eeg = datasets_EEG[-1]\n",
        "raw_eeg = raw_eeg.pick([\"eeg\"], verbose=False)\n",
        "raw_eeg.load_data()\n",
        "sampling_rate = raw_eeg.info[\"sfreq\"]  # Store the sampling rate\n",
        "# raw_eeg.set_montage(emotiv_epocX_montage)\n",
        "raw_eeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eeg.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# raw_eeg.plot_psd_topomap()\n",
        "raw_eeg.plot_psd()\n",
        "raw_eeg.plot_projs_topomap()\n",
        "raw_eeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGData\n",
        "\n",
        "edf_export_parent_path: Path = Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/exported_EDF\").resolve()\n",
        "edf_export_parent_path.mkdir(exist_ok=True)\n",
        "\n",
        "for an_xdf_dataset_idx,raw_eeg in enumerate(datasets_EEG):\n",
        "    ## INPUTS: raw_eeg\n",
        "    ## Get paths for current raw:\n",
        "    curr_fif_file_path: Path = Path(raw_eeg.filenames[0]).resolve()\n",
        "    curr_file_edf_name: str = curr_fif_file_path.with_suffix('.edf').name\n",
        "    curr_file_edf_path: Path = edf_export_parent_path.joinpath(curr_file_edf_name).resolve()\n",
        "    EEGData.save_mne_raw_to_edf(raw_eeg, curr_file_edf_path)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EEG Trends Across Recordings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_post_processing"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.EegProcessing import analyze_eeg_trends\n",
        "\n",
        "# df_feat, models = analyze_eeg_trends(raw_list=datasets_EEG[-n_most_recent_sessions_to_preprocess:])\n",
        "datasets_EEG = sso.modalities['EEG'].datasets\n",
        "df_feat, models = analyze_eeg_trends(raw_list=datasets_EEG)\n",
        "# 18m 23s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from fastparquet import write \n",
        "\n",
        "# write('outfile.parq', df_feat)\n",
        "\n",
        "df_feat.to_csv('2025-08-25_All_EEG_analyze_eeg_trends.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feat.to_pickle('2025-08-25_All_EEG_analyze_eeg_trends.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.EegVisualization import VisHelpers\n",
        "\n",
        "VisHelpers.plot_clock_values(df_feat, hour_col_name='time_of_day')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hour_by_hour_bins = np.arange(24)\n",
        "# s = pd.cut(df_feat['time_of_day'], bins=hour_by_hour_bins).value_counts()\n",
        "# s = pd.cut(df_feat, bins=hour_by_hour_bins).value_counts()\n",
        "# s\n",
        "\n",
        "# Bin the Age column into 3 equal-sized bins\n",
        "df_feat['UnitGroup'] = pd.cut(df_feat['time_of_day'], bins=hour_by_hour_bins)\n",
        "df_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_feat.groupby(['UnitGroup']).agg(theta_mean=('theta', 'mean'), alpha_mean=('alpha', 'mean'), beta_mean=('beta', 'mean')).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2025-09-05 - ChatGPT-generated across-session `hour-of-day` analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# requirements: mne, numpy, pandas, scipy, statsmodels, matplotlib\n",
        "import mne, numpy as np, pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import statsmodels.formula.api as smf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BANDS = dict(delta=(0.5,4), theta=(4,8), alpha=(8,13), beta=(13,30), gamma=(30,80))\n",
        "\n",
        "def band_power_from_psd(psd, freqs, band):\n",
        "    fmin,fmax = band\n",
        "    idx = np.where((freqs>=fmin)&(freqs<=fmax))[0]\n",
        "    return np.trapz(psd[:,idx], freqs[idx], axis=1)  # psd per epoch averaged over channels before passing\n",
        "\n",
        "def process_raw_list(raw_list, epoch_length=10.0, picks=None, resample_sfreq=128.0):\n",
        "    records = []\n",
        "    for i, raw in enumerate(raw_list):\n",
        "        try:\n",
        "            raw.load_data()\n",
        "            if resample_sfreq is not None:\n",
        "                raw.resample(resample_sfreq)\n",
        "            if picks is not None:\n",
        "                raw.pick(picks)\n",
        "            epochs = mne.make_fixed_length_epochs(raw, duration=epoch_length, preload=True, reject_by_annotation=False)\n",
        "            # psds, freqs = mne.time_frequency.psd_welch(epochs, fmin=0.5, fmax=80, n_fft=int(epoch_length*raw.info['sfreq']), average='mean')\n",
        "\n",
        "            spectrum = epochs.compute_psd(method=\"welch\", fmin=1.0, fmax=58)\n",
        "            psds, freqs = spectrum.get_data(return_freqs=True)\n",
        "\n",
        "            # psds shape (n_epochs, n_channels, n_freqs)\n",
        "            psd_epoch_chan_mean = psds.mean(axis=1)  # (n_epochs, n_freqs)\n",
        "            # timestamp for each epoch: event sample / sfreq + meas_date\n",
        "            sfreq = raw.info['sfreq']\n",
        "            event_samples = epochs.events[:,0]\n",
        "            epoch_seconds = event_samples / sfreq\n",
        "            meas_date = raw.info.get('meas_date', None)\n",
        "            if isinstance(meas_date, (tuple,list)):\n",
        "                meas_date = datetime.fromtimestamp(meas_date[0])\n",
        "            if isinstance(meas_date, float) or isinstance(meas_date, np.floating):\n",
        "                meas_date = datetime.fromtimestamp(meas_date)\n",
        "            for ei, sec in enumerate(epoch_seconds):\n",
        "                if meas_date is None:\n",
        "                    epoch_dt = None\n",
        "                    hour = (sec/3600.0) % 24\n",
        "                else:\n",
        "                    epoch_dt = meas_date + timedelta(seconds=float(sec))\n",
        "                    hour = epoch_dt.hour + epoch_dt.minute/60.0 + epoch_dt.second/3600.0\n",
        "                band_vals = {b: band_power_from_psd(psd_epoch_chan_mean[ei:ei+1], freqs, BANDS[b])[0] for b in BANDS}\n",
        "                rec = dict(session_index=i, epoch_index=ei, epoch_seconds=float(sec), epoch_datetime=epoch_dt, hour_of_day=hour)\n",
        "                rec.update(band_vals)\n",
        "                records.append(rec)\n",
        "        except (ValueError, AttributeError) as ve:\n",
        "            print(f\"Skipping raw {i} due to error: {ve}\")\n",
        "            continue                \n",
        "\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "    ## END for i, raw in enumerate(raw_list)...\n",
        "    \n",
        "    df = pd.DataFrame.from_records(records)\n",
        "    # make ratios and circular hour encodings\n",
        "    df['alpha_beta'] = df['alpha'] / (df['beta'] + 1e-12)\n",
        "    df['theta_alpha'] = df['theta'] / (df['alpha'] + 1e-12)\n",
        "    df['hour_sin'] = np.sin(2*np.pi*df['hour_of_day']/24.0)\n",
        "    df['hour_cos'] = np.cos(2*np.pi*df['hour_of_day']/24.0)\n",
        "    return df\n",
        "\n",
        "# example usage:\n",
        "# df = process_raw_list(my_raws, epoch_length=30.0, picks=['EEG'])  # or picks=None\n",
        "# quick plot of mean ratio by hour-of-day (binned)\n",
        "def plot_ratio_by_hour(df, ratio='alpha_beta', bins=24):\n",
        "    df['hour_bin'] = (df['hour_of_day']*(bins/24)).astype(int) % bins\n",
        "    agg = df.groupby('hour_bin')[ratio].agg(['mean','sem']).reset_index()\n",
        "    x = agg['hour_bin']*(24.0/bins)\n",
        "    plt.errorbar(x, agg['mean'], yerr=agg['sem'])\n",
        "    plt.xlabel('Hour of day'); plt.ylabel(ratio); plt.title(ratio+' by hour'); plt.show()\n",
        "\n",
        "# mixed-effects test: model ratio ~ circular hour + (1|session_index)\n",
        "# (session_index used as a random intercept representing each day/recording)\n",
        "def mixed_effects_test(df, outcome='alpha_beta'):\n",
        "    df2 = df.dropna(subset=[outcome,'hour_sin','hour_cos'])\n",
        "    model = smf.mixedlm(f\"{outcome} ~ hour_sin + hour_cos\", df2, groups=df2['session_index'])\n",
        "    res = model.fit(reml=False)\n",
        "    return res.summary()\n",
        "\n",
        "\n",
        "# end-to-end quick run:\n",
        "raw_list = sso.modalities['EEG'].datasets\n",
        "df = process_raw_list(raw_list)\n",
        "print(mixed_effects_test(df,'alpha_beta'))\n",
        "plot_ratio_by_hour(df,'alpha_beta',bins=24)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sso.modalities['WHISPER'].datasets\n"
      ]
    },
    {
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      },
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```         Mixed Linear Model Regression Results\n",
        "========================================================\n",
        "Model:            MixedLM Dependent Variable: alpha_beta\n",
        "No. Observations: 1099    Method:             ML        \n",
        "No. Groups:       9       Scale:              0.1015    \n",
        "Min. group size:  4       Log-Likelihood:     -315.6707 \n",
        "Max. group size:  294     Converged:          Yes       \n",
        "Mean group size:  122.1                                 \n",
        "---------------------------------------------------------\n",
        "            Coef.  Std.Err.    z    P>|z|  [0.025  0.975]\n",
        "---------------------------------------------------------\n",
        "Intercept   1.478     0.175  8.436  0.000   1.135   1.821\n",
        "hour_sin    0.716     0.182  3.937  0.000   0.359   1.072\n",
        "hour_cos    0.435     0.161  2.710  0.007   0.120   0.750\n",
        "Group Var   0.027     0.060                              \n",
        "========================================================\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "```\n",
        "          Mixed Linear Model Regression Results\n",
        "==========================================================\n",
        "Model:            MixedLM Dependent Variable: alpha_beta  \n",
        "No. Observations: 101840  Method:             ML          \n",
        "No. Groups:       494     Scale:              2.9224      \n",
        "Min. group size:  1       Log-Likelihood:     -199762.8521\n",
        "Max. group size:  1011    Converged:          Yes         \n",
        "Mean group size:  206.2                                   \n",
        "-----------------------------------------------------------\n",
        "             Coef.  Std.Err.    z     P>|z|  [0.025  0.975]\n",
        "-----------------------------------------------------------\n",
        "Intercept    1.058     0.028  37.672  0.000   1.003   1.113\n",
        "hour_sin     0.226     0.039   5.817  0.000   0.150   0.302\n",
        "hour_cos     0.135     0.037   3.689  0.000   0.063   0.207\n",
        "Group Var    0.292     0.016                               \n",
        "==========================================================\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Automatic Analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns                 # pip install seaborn\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "## INPUTS: df_feat\n",
        "\n",
        "## 1. Simple scatter + LOWESS smoother\n",
        "bands = [\"theta\", \"alpha\", \"beta\"]\n",
        "\n",
        "fig, axes = plt.subplots(len(bands), 1, figsize=(8, 3 * len(bands)))\n",
        "\n",
        "for ax, band in zip(axes, bands):\n",
        "    sns.scatterplot(\n",
        "        x=\"time_of_day\", y=band,\n",
        "        data=df_feat, s=10, alpha=.2, ax=ax)\n",
        "\n",
        "    # LOWESS (locally-weighted regression) trend line\n",
        "    sns.regplot(\n",
        "        x=\"time_of_day\", y=band,\n",
        "        data=df_feat, scatter=False, lowess=True,\n",
        "        line_kws={\"color\": \"red\", \"lw\": 2}, ax=ax\n",
        "    )\n",
        "    ax.set(title=f\"{band} power vs. time of day\",\n",
        "           xlabel=\"Clock time (decimal hours)\",\n",
        "           ylabel=\"Relative power\")\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## 2. Bin clock time and plot means Â± SEM\n",
        "## INPUTS: df_feat\n",
        "# 1-hour bins: [0-1),[1-2),â¦, [23-24)\n",
        "df_feat[\"hour_bin\"] = pd.cut(df_feat.time_of_day,\n",
        "                             bins=np.arange(0,25,1), right=False,\n",
        "                             labels=np.arange(24))\n",
        "\n",
        "for band in bands:\n",
        "    tmp = df_feat.groupby(\"hour_bin\")[band].agg([\"mean\", \"sem\"]).reset_index()\n",
        "    plt.errorbar(tmp.hour_bin.astype(int), tmp[\"mean\"],\n",
        "                 yerr=tmp[\"sem\"], marker=\"o\", label=band)\n",
        "\n",
        "plt.xticks(range(0,24,2))\n",
        "plt.xlabel(\"Clock hour\")\n",
        "plt.ylabel(\"Relative power (mean Â± SEM)\")\n",
        "plt.title(\"Hourly mean band power\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 3. Visualise per-day trajectories\n",
        "g = sns.relplot(\n",
        "    data=df_feat, kind=\"line\",\n",
        "    x=\"time_of_day\", y=\"alpha\", hue=\"day\",\n",
        "    estimator=None, units=\"day\",  # keep each day as its own line\n",
        "    height=4, aspect=1.5)\n",
        "g.set_axis_labels(\"Clock time (h)\", \"Alpha power\")\n",
        "g.figure.suptitle(\"Alpha trajectories per day\", y=1.03)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from statsmodels.tools import add_constant\n",
        "\n",
        "for band in bands:\n",
        "    mdf = models[band]                      # fitted MixedLMResults\n",
        "    beta0, beta1 = mdf.fe_params            # intercept & slope\n",
        "\n",
        "    # scatter of raw data\n",
        "    sns.scatterplot(x=\"time_of_day\", y=band,\n",
        "                    data=df_feat, alpha=.15, s=10)\n",
        "\n",
        "    # prediction line\n",
        "    x = np.linspace(0,24,200)\n",
        "    y = beta0 + beta1 * x\n",
        "    plt.plot(x, y, color=\"red\", lw=2,\n",
        "             label=f\"MixedLM slope = {beta1:.3f}\")\n",
        "    plt.title(f\"{band} â fixed-effect fit\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "df_feat[\"phi\"] = 2*np.pi*df_feat.time_of_day/24\n",
        "plt.subplot(111, projection='polar')\n",
        "plt.scatter(df_feat.phi, df_feat.alpha, s=3, alpha=.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Jaw Clench EEG Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_labeled_jaw_clinch_EEG = deepcopy(raw_eeg)\n",
        "# hand_labeled_jaw_clinch_EEG.save(fname='hand_labeled_jaw_clinch_EEG_2025-08-20.fif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_annotations = hand_labeled_jaw_clinch_EEG.annotations\n",
        "max_duration_reject_event: float = np.nanmax(user_annotations.to_data_frame()['duration'])\n",
        "max_duration_reject_event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_labeled_jaw_clinch_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# annotate_flat \n",
        "from mne.epochs import Epochs\n",
        "from mne.event import make_fixed_length_events\n",
        "\n",
        "\n",
        "reject_criteria = dict(\n",
        "    # mag=3000e-15,  # 3000 fT\n",
        "    # grad=3000e-13,  # 3000 fT/cm\n",
        "    eeg=1e-6,  # 100 ÂµV\n",
        "    # eog=200e-6,\n",
        ")  # 200 ÂµV\n",
        "\n",
        "flat_criteria = dict(mag=1e-15, grad=1e-13, eeg=1e-6)  # 1 fT  # 1 fT/cm  # 1 ÂµV\n",
        "\n",
        "#create equally spaced events in raw data, with the duration > the longest bad data segment (here, that was 4 s)\n",
        "events = make_fixed_length_events(hand_labeled_jaw_clinch_EEG, duration=(np.ceil(max_duration_reject_event)+1.0)) \n",
        "\n",
        "#convert the raw data to an epoch object using events and reject epochs where data is over a certain amplitude threshold (here, that was 0.6 mV or 6e-5 V)\n",
        "reject = dict(eeg=1e-6)\n",
        "epochs = Epochs(hand_labeled_jaw_clinch_EEG, events, reject=reject, preload=True)\n",
        "epochs.plot_drop_log()\n",
        "epochs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# annotations, bad_chans = mne.preprocessing.annotate_amplitude(raw=hand_labeled_jaw_clinch_EEG, peak=reject_criteria, min_duration=0.3)\n",
        "# annotations\n",
        "annotations, bad_chans = mne.preprocessing.annotate_amplitude(hand_labeled_jaw_clinch_EEG, peak=100e-3, picks = 'eeg', bad_percent=5, min_duration=0.005)\n",
        "annotations\n",
        "\n",
        "hand_labeled_jaw_clinch_EEG.set_annotations((hand_labeled_jaw_clinch_EEG.annotations + annotations))\n",
        "hand_labeled_jaw_clinch_EEG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reset 'hand_labeled_jaw_clinch_EEG'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_labeled_jaw_clinch_EEG = deepcopy(raw_eeg)\n",
        "hand_labeled_jaw_clinch_EEG.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "\n",
        "def detect_jaw_clench_annotations(raw, window_s=0.25, step_s=0.05, z_thresh=3.0, min_sync_channels=3, max_considered_channels=6, prob_thresh=0.5, merge_pad_s=0.1, picks=\"eeg\", return_probs=False):\n",
        "    \"\"\"\n",
        "    Compute sliding-window PTP per channel, z-score within-channel, and flag windows where >=min_sync_channels show high-amplitude transients.\n",
        "    Creates MNE Annotations labeled 'JAW_CLENCH[p=...]' over merged windows with probability>prob_thresh.\n",
        "    Args use seconds; thresholds are reasonable defaults for brief jaw-clench bursts.\n",
        "    \"\"\"\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "    w = max(1, int(round(window_s * sfreq)))\n",
        "    s = max(1, int(round(step_s * sfreq)))\n",
        "    pad = max(0, int(round(merge_pad_s * sfreq)))\n",
        "    picks_idx = mne.pick_types(raw.info, eeg=True) if picks == \"eeg\" else mne.pick_channels(raw.ch_names, picks)\n",
        "    if len(picks_idx) == 0:\n",
        "        return mne.Annotations(onset=[], duration=[], description=[])\n",
        "    data = raw.get_data(picks=picks_idx, reject_by_annotation=\"omit\")\n",
        "    n_ch, n_samp = data.shape\n",
        "    if n_samp < w:\n",
        "        return mne.Annotations(onset=[], duration=[], description=[])\n",
        "    # Sliding windows (center-aligned)\n",
        "    from numpy.lib.stride_tricks import sliding_window_view\n",
        "    sw = sliding_window_view(data, w, axis=1)  # shape: (n_ch, n_win_full, w)\n",
        "    win_idxs = np.arange(0, sw.shape[1] - 1, s)\n",
        "    sw = sw[:, win_idxs, :]\n",
        "    ptp = sw.max(axis=-1) - sw.min(axis=-1)  # (n_ch, n_win)\n",
        "    # Z-score per channel across time (robust to heavy tails)\n",
        "    ch_med = np.median(ptp, axis=1, keepdims=True)\n",
        "    ch_mad = np.median(np.abs(ptp - ch_med), axis=1, keepdims=True)\n",
        "    denom = np.maximum(ch_mad * 1.4826, 1e-12)\n",
        "    z = (ptp - ch_med) / denom\n",
        "    # Sync count per window and excess\n",
        "    over = z > z_thresh\n",
        "    k = over.sum(axis=0)  # number of channels exceeding threshold per window\n",
        "    k_cap = np.minimum(k, min(max_considered_channels, n_ch))\n",
        "    max_z = np.where(over, z, -np.inf).max(axis=0)\n",
        "    max_z[np.isneginf(max_z)] = 0.0\n",
        "    excess = np.clip(max_z - z_thresh, 0.0, None)\n",
        "    # Probability mapping: combines sync count and excess amplitude, capped by 3â6 channels\n",
        "    denom_k = float(min(max_considered_channels, n_ch))\n",
        "    p = np.clip((k_cap / denom_k) * np.tanh(excess / 3.0), 0.0, 1.0)\n",
        "    # Build intervals where p > prob_thresh, merge with padding\n",
        "    centers = (win_idxs + w // 2).astype(int)\n",
        "    mask = p > prob_thresh\n",
        "    if not np.any(mask):\n",
        "        return (mne.Annotations(onset=[], duration=[], description=[]), (centers / sfreq, p)) if return_probs else mne.Annotations(onset=[], duration=[], description=[])\n",
        "    idx = np.flatnonzero(mask)\n",
        "    groups = np.split(idx, np.where(np.diff(idx) > 1)[0] + 1)\n",
        "    onsets = []\n",
        "    durations = []\n",
        "    descs = []\n",
        "    for g in groups:\n",
        "        start = centers[g[0]] - pad\n",
        "        end = centers[g[-1]] + pad\n",
        "        start = int(np.clip(start, 0, n_samp - 1))\n",
        "        end = int(np.clip(end, start + 1, n_samp))\n",
        "        seg_p = float(np.mean(p[g]))\n",
        "        onsets.append(raw.first_time + start / sfreq)\n",
        "        durations.append((end - start) / sfreq)\n",
        "        descs.append(f\"JAW_CLENCH[p={seg_p:.2f}]\")\n",
        "    # return (ann, (centers / sfreq, p)) if return_probs else ann\n",
        "    ann = mne.Annotations(onset=onsets, duration=durations, description=descs, orig_time=raw.annotations.orig_time)\n",
        "    return (ann, (centers / sfreq, p)) if return_probs else ann\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "fn_bad_ann = detect_jaw_clench_annotations(hand_labeled_jaw_clinch_EEG)\n",
        "hand_labeled_jaw_clinch_EEG.set_annotations(hand_labeled_jaw_clinch_EEG.annotations + fn_bad_ann)\n",
        "hand_labeled_jaw_clinch_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.EegProcessing import annotate_jaw_clench\n",
        "\n",
        "\n",
        "hand_labeled_jaw_clinch_EEG = annotate_jaw_clench(hand_labeled_jaw_clinch_EEG, window_size=1.5, thresh_scale=1, min_channels=3)\n",
        "hand_labeled_jaw_clinch_EEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hand_labeled_jaw_clinch_EEG.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil, sqrt\n",
        "\n",
        "\n",
        "def plot_channel_amplitude_histogram(\n",
        "    raw,\n",
        "    *,\n",
        "    picks=\"eeg\",\n",
        "    bins=100,\n",
        "    log_y=False,\n",
        "    use_ptp=False,           # True â histogram of PTP values\n",
        "    window_size=1.0,         # seconds, only used if use_ptp=True\n",
        "    reject_by_annotation=True,\n",
        "    units=\"auto\",            # \"auto\" | \"V\" | \"ÂµV\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Plot one histogram per channel (sample amplitudes or PTP values).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    raw : mne.io.BaseRaw\n",
        "        Continuous EEG data.\n",
        "    picks : str | list\n",
        "        Which channels to include (default \"eeg\").\n",
        "    bins : int or sequence\n",
        "        Histogram bins passed to matplotlib/numpy.\n",
        "    log_y : bool\n",
        "        If True, use a log-scaled y-axis (helps when tails are long).\n",
        "    use_ptp : bool\n",
        "        False â histogram of every sample's amplitude  \n",
        "        True  â histogram of sliding-window peak-to-peak values.\n",
        "    window_size : float\n",
        "        Length (s) of the PTP window if `use_ptp=True`.\n",
        "    reject_by_annotation : bool\n",
        "        Whether to drop segments already annotated as bad.\n",
        "    units : str\n",
        "        \"V\" for Volts, \"ÂµV\" for microvolts, or \"auto\" (choose ÂµV if values\n",
        "        look <1e-2; otherwise V).\n",
        "    \"\"\"\n",
        "    if not isinstance(raw, mne.io.BaseRaw):\n",
        "        raise TypeError(\"`raw` must be an MNE Raw/RawArray object.\")\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # Pick data\n",
        "    # ------------------------------------------------------------\n",
        "    if isinstance(picks, (list, tuple)):\n",
        "        pick_idx = mne.pick_channels(raw.info[\"ch_names\"], include=picks)\n",
        "    else:\n",
        "        pick_idx = mne.pick_types(raw.info, meg=False, eeg=True)\n",
        "\n",
        "    ch_names = [raw.ch_names[i] for i in pick_idx]\n",
        "    data = raw.get_data(picks=pick_idx,\n",
        "                        reject_by_annotation=\"omit\" if reject_by_annotation else None)\n",
        "\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # Choose units\n",
        "    # ------------------------------------------------------------\n",
        "    if units == \"auto\":\n",
        "        # Heuristic: if median absolute amplitude < 1e-2 V, assume V units and\n",
        "        # convert to ÂµV for readability.\n",
        "        med_abs = np.median(np.abs(data))\n",
        "        if med_abs < 1e-2:\n",
        "            units = \"ÂµV\"\n",
        "        else:\n",
        "            units = \"V\"\n",
        "\n",
        "    scale = 1.0 if units == \"V\" else 1e6   # convert to ÂµV\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # Prepare the figure layout\n",
        "    # ------------------------------------------------------------\n",
        "    n_ch = len(ch_names)\n",
        "    n_cols = ceil(sqrt(n_ch))\n",
        "    n_rows = ceil(n_ch / n_cols)\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(3.2 * n_cols, 2.5 * n_rows), squeeze=False\n",
        "    )\n",
        "\n",
        "    # ------------------------------------------------------------\n",
        "    # Build histograms channel-by-channel\n",
        "    # ------------------------------------------------------------\n",
        "    if use_ptp:\n",
        "        win_samp = int(round(window_size * sfreq))\n",
        "        for ax, ch_idx, name in zip(axes.flatten(), pick_idx, ch_names):\n",
        "            sig = data[ch_names.index(name)]\n",
        "            ptp_vals = sig[\n",
        "                : len(sig) - win_samp + 1\n",
        "            ].reshape(-1, win_samp).ptp(axis=1)\n",
        "            ax.hist(ptp_vals * scale, bins=bins, color=\"steelblue\")\n",
        "            ax.set_title(name)\n",
        "            ax.set_xlabel(f\"PTP amplitude [{units}]\")\n",
        "            if log_y:\n",
        "                ax.set_yscale(\"log\")\n",
        "    else:\n",
        "        for ax, sig, name in zip(axes.flatten(), data, ch_names):\n",
        "            ax.hist(sig * scale, bins=bins, color=\"steelblue\")\n",
        "            ax.set_title(name)\n",
        "            ax.set_xlabel(f\"Amplitude [{units}]\")\n",
        "            if log_y:\n",
        "                ax.set_yscale(\"log\")\n",
        "\n",
        "    # Hide unused axes\n",
        "    for ax in axes.flatten()[n_ch:]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Example usage\n",
        "###############################################################################\n",
        "# raw = mne.io.read_raw_fif(\"subject_raw.fif\", preload=True)\n",
        "#\n",
        "# # 1) Quick look at sample-wise amplitudes\n",
        "# plot_channel_amplitude_histogram(hand_labeled_jaw_clinch_EEG, bins=100, log_y=False)\n",
        "\n",
        "plot_channel_amplitude_histogram(hand_labeled_jaw_clinch_EEG, bins=100, log_y=False, reject_by_annotation=False)\n",
        "\n",
        "\n",
        "\n",
        "#\n",
        "# # 2) Histogram of 1-second sliding PTP amplitudes (matches detector)\n",
        "# plot_channel_amplitude_histogram(\n",
        "#     hand_labeled_jaw_clinch_EEG,\n",
        "#     use_ptp=True,\n",
        "#     window_size=1.0,\n",
        "#     bins=100,\n",
        "#     log_y=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import mne\n",
        "import matplotlib.pyplot as plt\n",
        "from math import ceil, sqrt\n",
        "\n",
        "\n",
        "def _build_sample_mask(raw, keep_desc=None, drop_bad=False):\n",
        "    \"\"\"\n",
        "    Return a boolean vector (len = n_samples) marking samples to KEEP.\n",
        "\n",
        "    keep_desc : list | None\n",
        "        List of annotation descriptions to retain.  None â keep all.\n",
        "    drop_bad : bool\n",
        "        Additionally drop samples that fall in \"BAD\"-type annotations.\n",
        "    \"\"\"\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "    keep = np.zeros(len(raw.times), dtype=bool)\n",
        "\n",
        "    for on, dur, desc in zip(\n",
        "        raw.annotations.onset, raw.annotations.duration, raw.annotations.description\n",
        "    ):\n",
        "        start, stop = int(on * sfreq), int((on + dur) * sfreq)\n",
        "\n",
        "        if drop_bad and desc.upper().startswith(\"BAD\"):\n",
        "            continue  # we *drop* these later\n",
        "\n",
        "        if (keep_desc is None) or (desc in keep_desc):\n",
        "            keep[start:stop] = True\n",
        "\n",
        "    if drop_bad:\n",
        "        # Now knock out \"BAD\" samples from whatever we decided to keep\n",
        "        bad = np.zeros(len(raw.times), dtype=bool)\n",
        "        for on, dur, desc in zip(\n",
        "            raw.annotations.onset, raw.annotations.duration, raw.annotations.description\n",
        "        ):\n",
        "            if not desc.upper().startswith(\"BAD\"):\n",
        "                continue\n",
        "            start, stop = int(on * sfreq), int((on + dur) * sfreq)\n",
        "            bad[start:stop] = True\n",
        "        keep &= ~bad\n",
        "\n",
        "    return keep\n",
        "\n",
        "\n",
        "def plot_channel_amplitude_histogram(\n",
        "    raw,\n",
        "    *,\n",
        "    picks=\"eeg\",\n",
        "    bins=100,\n",
        "    log_y=False,\n",
        "    use_ptp=False,\n",
        "    window_size=1.0,\n",
        "    reject_by_annotation=True,\n",
        "    units=\"auto\",\n",
        "    only_annotated=False,\n",
        "    anno_descriptions=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Same interface as before but safe against boolean-index mismatches.\n",
        "    \"\"\"\n",
        "    if only_annotated:\n",
        "        keep_mask = _build_sample_mask(\n",
        "            raw,\n",
        "            keep_desc=anno_descriptions,\n",
        "            drop_bad=reject_by_annotation,\n",
        "        )\n",
        "    elif reject_by_annotation:\n",
        "        # keep everything except BAD segments\n",
        "        keep_mask = _build_sample_mask(raw, keep_desc=None, drop_bad=True)\n",
        "    else:\n",
        "        keep_mask = slice(None)  # keep everything\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Pick and scale data\n",
        "    # -----------------------------------------------------------------\n",
        "    if isinstance(picks, (list, tuple)):\n",
        "        pick_idx = mne.pick_channels(raw.info[\"ch_names\"], include=picks)\n",
        "    else:\n",
        "        pick_idx = mne.pick_types(raw.info, meg=False, eeg=True)\n",
        "\n",
        "    data = raw.get_data(picks=pick_idx, reject_by_annotation=None)[:, keep_mask]\n",
        "    ch_names = [raw.ch_names[i] for i in pick_idx]\n",
        "    sfreq = raw.info[\"sfreq\"]\n",
        "\n",
        "    # choose units\n",
        "    if units == \"auto\":\n",
        "        units = \"ÂµV\" if np.median(np.abs(data)) < 1e-2 else \"V\"\n",
        "    scale = 1.0 if units == \"V\" else 1e6\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Figure layout\n",
        "    # -----------------------------------------------------------------\n",
        "    n_ch = len(ch_names)\n",
        "    n_cols = ceil(sqrt(n_ch))\n",
        "    n_rows = ceil(n_ch / n_cols)\n",
        "    fig, axes = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(3.2 * n_cols, 2.5 * n_rows), squeeze=False\n",
        "    )\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # Plot\n",
        "    # -----------------------------------------------------------------\n",
        "    if use_ptp:\n",
        "        win_samp = int(round(window_size * sfreq))\n",
        "        for ax, sig, name in zip(axes.flatten(), data, ch_names):\n",
        "            if len(sig) < win_samp:\n",
        "                ax.axis(\"off\")\n",
        "                continue\n",
        "            ptp_vals = sig[: len(sig) - win_samp + 1].reshape(-1, win_samp).ptp(axis=1)\n",
        "            ax.hist(ptp_vals * scale, bins=bins, color=\"steelblue\")\n",
        "            ax.set_xlabel(f\"PTP [{units}]\")\n",
        "            ax.set_title(name)\n",
        "            if log_y:\n",
        "                ax.set_yscale(\"log\")\n",
        "    else:\n",
        "        for ax, sig, name in zip(axes.flatten(), data, ch_names):\n",
        "            ax.hist(sig * scale, bins=bins, color=\"steelblue\")\n",
        "            ax.set_xlabel(f\"Amp [{units}]\")\n",
        "            ax.set_title(name)\n",
        "            if log_y:\n",
        "                ax.set_yscale(\"log\")\n",
        "\n",
        "    # hide unused axes\n",
        "    for ax in axes.flatten()[n_ch:]:\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "# <Annotations | 86 segments: BAD_JAW (15), BAD_peak (71)>\n",
        "plot_channel_amplitude_histogram(\n",
        "    hand_labeled_jaw_clinch_EEG,\n",
        "    use_ptp=False,\n",
        "    only_annotated=True,          # keep only selected annotations\n",
        "    anno_descriptions=[\"BAD_JAW\"],\n",
        "    reject_by_annotation=False,   # <-- keep BAD samples!\n",
        "    bins=120,\n",
        "    log_y=False,\n",
        ")\n",
        "\n",
        "# # Histogram ONLY for samples labelled \"jaw_clench\", using 1-s PTP feature\n",
        "# plot_channel_amplitude_histogram(\n",
        "#     hand_labeled_jaw_clinch_EEG,\n",
        "#     use_ptp=False,\n",
        "#     window_size=1.0,\n",
        "#     only_annotated=False,\n",
        "#     anno_descriptions=[\"BAD_JAW\"],  # None â keep any annotation label\n",
        "#     bins=120,\n",
        "#     log_y=False,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_channel_amplitude_histogram(\n",
        "    hand_labeled_jaw_clinch_EEG,\n",
        "    use_ptp=False,\n",
        "    only_annotated=False,          # keep only selected annotations\n",
        "    anno_descriptions=[\"BAD_JAW\"],\n",
        "    reject_by_annotation=False,   # <-- keep BAD samples!\n",
        "    bins=120,\n",
        "    log_y=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_eeg = deepcopy(hand_labeled_jaw_clinch_EEG)\n",
        "\n",
        "# Apply band-pass filter (1-35Hz) and re-reference the raw signal\n",
        "eeg = raw_eeg.copy().filter(1, 35, verbose=False)\n",
        "eeg = nk.eeg_rereference(eeg, \"average\")\n",
        "\n",
        "nk.signal_plot(\n",
        "    [raw_eeg.get_data()[0, 0:500], eeg.get_data()[0, 0:500]],\n",
        "    labels=[\"Raw\", \"Preprocessed\"],\n",
        "    sampling_rate=eeg.info[\"sfreq\"],\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reject_criteria = dict(\n",
        "    mag=3000e-15,  # 3000 fT\n",
        "    grad=3000e-13,  # 3000 fT/cm\n",
        "    eeg=100e-6,  # 100 ÂµV\n",
        "    eog=200e-6,\n",
        ")  # 200 ÂµV\n",
        "\n",
        "flat_criteria = dict(mag=1e-15, grad=1e-13, eeg=1e-6)  # 1 fT  # 1 fT/cm  # 1 ÂµV\n",
        "epochs = mne.Epochs(\n",
        "    raw_eeg,\n",
        "    None,\n",
        "    tmin=-0.2,\n",
        "    tmax=0.5,\n",
        "    reject_tmax=0,\n",
        "    reject=reject_criteria,\n",
        "    flat=flat_criteria,\n",
        "    reject_by_annotation=False,\n",
        "    preload=True,\n",
        ")\n",
        "epochs.plot_drop_log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "## Try EOG -- fials due to lack of digitization points -- # RuntimeError: No digitization points found.\n",
        "eog_epochs = mne.preprocessing.create_eog_epochs(raw_eeg, ch_name=['AF3', 'AF4'], baseline=(-0.5, -0.2)) ## use the EEG channels closest to the eyes\n",
        "eog_epochs.plot_image(combine=\"mean\")\n",
        "eog_epochs.average().plot_joint()\n",
        "\n",
        "# The following electrodes have overlapping positions, which causes problems during visualization: T7, T8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.min(), df.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.channels.montage import DigMontage\n",
        "from phoofflineeeganalysis.analysis.anatomy_and_electrodes import ElectrodeHelper\n",
        "\n",
        "electrode_pos_parent_folder: Path = Path(\"E:/Dropbox (Personal)/Hardware/Consumer EEG Headsets/Emotiv Epoc EEG/ElectrodeLayouts\").resolve()\n",
        "electrode_positions_path: Path = electrode_pos_parent_folder.joinpath('ElectrodePositions_2025-08-14', 'brainstorm_electrode_positions_PhoHAle_eeg_subjectspacemm.tsv')\n",
        "\n",
        "active_electrode_man: ElectrodeHelper = ElectrodeHelper.init_EpocX_montage(electrode_positions_path=electrode_positions_path)\n",
        "emotiv_epocX_montage: DigMontage = active_electrode_man.active_montage\n",
        "\n",
        "# Just create montage from your electrode positions\n",
        "print(\"Montage created successfully!\")\n",
        "print(f\"Channel names: {emotiv_epocX_montage.ch_names}\")\n",
        "# Visualize the montage\n",
        "# ElectrodeHelper.visualize_montage(emotiv_epocX_montage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Single Calibration Motion recording to determine Quaternions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.historical_data import HistoricalData\n",
        "\n",
        "\n",
        "reference_EmotivEpocX_Headset_motion_file = Path(r\"E:\\Dropbox (Personal)\\Databases\\UnparsedData\\EmotivEpocX_EEGRecordings\\MOTION_RECORDINGS\\fif\\20250812-061030-Epoc X Motion-raw.fif\").resolve()\n",
        "assert reference_EmotivEpocX_Headset_motion_file.exists()\n",
        "\n",
        "constrain_channels = ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ']\n",
        "\n",
        "# adds the quaternion channels: ['qw', 'qx', 'qy', 'qz'] to the dataframe\n",
        "\n",
        "raw = read_raw(reference_EmotivEpocX_Headset_motion_file, preload=False)\n",
        "metadata_recording_start_datetime = raw.info.get('meas_date', None)\n",
        "if metadata_recording_start_datetime is None:\n",
        "    parsed_recording_start_datetime = HistoricalData.extract_datetime_from_filename(reference_EmotivEpocX_Headset_motion_file.name)\n",
        "    metadata_recording_start_datetime = parsed_recording_start_datetime\n",
        "    if True:                \n",
        "        # Make timezone-aware (UTC)\n",
        "        # dt = dt.rerawe(tzinfo=timezone.utc)\n",
        "        raw.info.set_meas_date(deepcopy(parsed_recording_start_datetime).replace(tzinfo=timezone.utc))\n",
        "        ## WAS UPDATED, probably need to re-save or something\n",
        "        meas_datetime = raw.info['meas_date']\n",
        "    else:\n",
        "        ## don't set it, but still use the parsed datetime\n",
        "        meas_datetime = deepcopy(parsed_recording_start_datetime).replace(tzinfo=timezone.utc) # raw.info['meas_date']  # This is an absolute datetime or tuple (timestamp, 0)\n",
        "\n",
        "else:                    \n",
        "    meas_datetime = raw.info['meas_date']  # This is an absolute datetime or tuple (timestamp, 0)\n",
        "\n",
        "\n",
        "raw_df: pd.DataFrame = raw.to_data_frame(picks=constrain_channels)\n",
        "data, times = raw.get_data(picks=constrain_channels, return_times=True)\n",
        "\n",
        "# Convert relative times to absolute timestamps (in seconds since epoch)\n",
        "start_time = meas_datetime.timestamp() if hasattr(meas_datetime, 'timestamp') else meas_datetime[0]\n",
        "abs_times = start_time + times\n",
        "\n",
        "raw_df['timestamp'] = abs_times\n",
        "raw_df['timestamp_dt'] = pd.to_datetime(raw_df['timestamp'], unit='s') ## add datetime column\n",
        "# Sort by column: 'timestamp_dt' (ascending)\n",
        "raw_df = raw_df.sort_values(['timestamp_dt'], na_position='first').reset_index(drop=True)\n",
        "\n",
        "## Estimate Quaternions from motion data:\n",
        "raw_df = MotionData.compute_quaternions(raw_df)\n",
        "raw_df\n",
        "\n",
        "# raw_df.to_csv('reference_pos_df_concatenated.csv', index=False)\n",
        "raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "raw.plot(duration=raw.times[-1], n_channels=len(raw.ch_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from phoofflineeeganalysis.analysis.motion_data import MotionData\n",
        "\n",
        "# Usage:\n",
        "MotionData.plot_3d_orientation(raw_df, step=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw_df.head().to_clipboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data.info['meas_date']\n",
        "# data.get_parsed_recording_start_datetime\n",
        "# data.annotations\n",
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Files in `WhisperVideoTranscripts_LSL_Converted` are transcripts generated by the `whisper-timestamped` repo's `transcript_to_lsl.py`, which translates spoken diction notes into LSL text events. \n",
        "# # Example filenames include:\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-07T095548.words.lsl.fif\",\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-06T185509.words.lsl.fif\",\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-06T153648.words.lsl.fif\",\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-06T151856.words.lsl.fif\",\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-07T185504.words.lsl.fif\",\n",
        "# \"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted/Debut_2025-08-07T150021.words.lsl.fif\",\n",
        "\n",
        "WhisperVideoTranscripts_LSL_Converted = Path(r\"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted\").resolve()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_recordings_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EEG Sonification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sounddevice as sd\n",
        "\n",
        "def play_sine(frequency, duration=1, sample_rate=44100):\n",
        "    t = np.linspace(0, duration, int(sample_rate * duration), endpoint=False)\n",
        "    sine_wave = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
        "    sd.play(sine_wave, sample_rate)\n",
        "    sd.wait()  # Wait until playback is finished\n",
        "\n",
        "# Example: Play a 440 Hz tone for 2 seconds\n",
        "play_sine(440, duration=2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from pysinewave import SineWave\n",
        "\n",
        "sinewave = SineWave(pitch=12, pitch_per_second=10)\n",
        "sinewave.play()\n",
        "time.sleep(2)\n",
        "sinewave.set_pitch(-5)\n",
        "time.sleep(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sinewave.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025-09-18 - LabRecorder XDF Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGData\n",
        "from phoofflineeeganalysis.analysis.MNE_helpers import DatasetDatetimeBoundsRenderingMixin, RawArrayExtended, RawExtended, up_convert_raw_objects, up_convert_raw_obj\n",
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import LabRecorderXDF, unwrap_single_element_listlike_if_needed\n",
        "\n",
        "\n",
        "lab_recorder_output_path = Path(r\"E:\\Dropbox (Personal)\\Databases\\UnparsedData\\LabRecorderStudies\\sub-P001\").resolve()\n",
        "assert lab_recorder_output_path.exists()\n",
        "\n",
        "labRecorder_PostProcessed_path: Path = sso.eeg_analyzed_parent_export_path.joinpath(f'LabRecorder_PostProcessed')\n",
        "labRecorder_PostProcessed_path.mkdir(exist_ok=True)\n",
        "\n",
        "should_write_final_merged_eeg_fif: bool = True\n",
        "# should_write_final_merged_eeg_fif: bool = False\n",
        "_out_eeg_raw, _out_xdf_stream_infos_df, lab_recorder_xdf_files = LabRecorderXDF.load_and_process_all(lab_recorder_output_path=lab_recorder_output_path, \n",
        "                                                                                                     labRecorder_PostProcessed_path=labRecorder_PostProcessed_path, should_write_final_merged_eeg_fif=should_write_final_merged_eeg_fif)\n",
        "xdf_dataset_indicies = np.unique(deepcopy(_out_xdf_stream_infos_df).reset_index(drop=False, inplace=False)['xdf_dataset_idx'].to_numpy())\n",
        "n_unique_xdf_datasets: int = len(xdf_dataset_indicies)\n",
        "print(f'n_unique_xdf_datasets: {n_unique_xdf_datasets}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(_out_xdf_stream_infos_df)\n",
        "_out_xdf_stream_infos_df.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_eeg_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Batch Computations on `_out_eeg_raw`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_sessions: int = len(_out_eeg_raw)\n",
        "num_sessions\n",
        "\n",
        "\n",
        "results_dict = {}\n",
        "\n",
        "for an_xdf_dataset_idx in np.arange(num_sessions):\n",
        "    a_raw = deepcopy(_out_eeg_raw[an_xdf_dataset_idx])\n",
        "    a_raw.down_convert_to_base_type()\n",
        "    results_dict[an_xdf_dataset_idx] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw = deepcopy(_out_eeg_raw[-3])\n",
        "a_raw.down_convert_to_base_type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUT: fixed_len_epochs\n",
        "freqs = np.arange(5., 40., 1.0)\n",
        "# Define frequencies and number of cycles\n",
        "# freqs = np.logspace(*np.log10([2, 40]), num=20)\n",
        "n_cycles = freqs / 2.0 # A common approach is to use a fixed number of cycles or a value that increases with frequency.\n",
        "\n",
        "freqs\n",
        "n_cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixed Length Epoch-based:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import RawExtended, RawArrayExtended\n",
        "\n",
        "## INPUT: a_raw\n",
        "fixed_len_epochs = mne.make_fixed_length_epochs(a_raw, duration=8, preload=True, reject_by_annotation=False)\n",
        "fixed_len_epochs\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUT: fixed_len_epochs\n",
        "freqs = np.arange(5., 40., 1.0)\n",
        "\n",
        "# Define frequencies and number of cycles\n",
        "# freqs = np.logspace(*np.log10([2, 40]), num=20)\n",
        "n_cycles = freqs / 2.0 # A common approach is to use a fixed number of cycles or a value that increases with frequency.\n",
        "\n",
        "# n_cycles = np.linspace(3,10,len(freqs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Compute time-frequency power using Morlet wavelets -- NO EPOCHS, RAW DATA\n",
        "# power = a_raw.compute_tfr(method=\"morlet\",\n",
        "power = mne.time_frequency.tfr_morlet(\n",
        "    fixed_len_epochs,\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    return_itc=False,\n",
        "    average=False, # Compute TFR on continuous data without averaging\n",
        "    # average=True, # Compute TFR on continuous data without averaging\n",
        "    decim=3, # Decimate the output for faster computation and smaller file size\n",
        "    n_jobs=-1 # Use 1 core for this example\n",
        ")\n",
        "power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Array version:\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "# get raw data (channels Ã time)\n",
        "data = fixed_len_epochs.get_data() # shape: (n_epochs, n_channels, n_times)\n",
        "print(f'np.shape(data): {np.shape(data)}')\n",
        "# run morlet on continuous data\n",
        "power: NDArray = tfr_array_morlet(\n",
        "    data=data,\n",
        "    sfreq=fixed_len_epochs.info['sfreq'],\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output='power',\n",
        "    decim=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# power shape: (n_epochs=1, n_channels, n_freqs, n_times)\n",
        "power = np.squeeze(power) ## remove epoch\n",
        "print(f'np.shape(power): {np.shape(power)}')\n",
        "\n",
        "# np.shape(data): (593, 14, 1024)\n",
        "# np.shape(power): (593, 14, 20, 342)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "repo_data_folder_path = Path(r\"C:\\Users\\pho\\repos\\EmotivEpoc\\PhoLabStreamingReceiver\\data\").resolve()\n",
        "\n",
        "h5_file_path = repo_data_folder_path.joinpath(f'2025-09-21_exported_sess.h5').resolve()\n",
        "\n",
        "print(f'writing to \"{h5_file_path.as_posix()}\"')\n",
        "\n",
        "# raw_data = a_raw.get_data() # (14, 607857)\n",
        "# np.shape(raw_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with h5py.File(h5_file_path, \"w\") as f:\n",
        "    \n",
        "    d0 = f.create_dataset(\"raw\", data=raw_data)\n",
        "    d0.attrs[\"dim_labels\"] = [\"channels\", \"time\"]\n",
        "\n",
        "    d1 = f.create_dataset(\"epoched\", data=data)\n",
        "    d1.attrs[\"dim_labels\"] = [\"epochs\", \"channels\", \"time\"]\n",
        "\n",
        "    d2 = f.create_dataset(\"power\", data=power)\n",
        "    d2.attrs[\"dim_labels\"] = [\"epochs\", \"channels\", \"time\", \"frequency\"]\n",
        "\n",
        "\n",
        "print(f'done! wrote to \"{h5_file_path.as_uri()}\"')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_epochs, n_channels, n_freqs, n_t_steps = np.shape(power)\n",
        "\n",
        "ch_idx: int = 0\n",
        "single_channel_power_data: NDArray = np.squeeze(power[:, ch_idx, :, :]) # (n_epochs, n_freqs, n_t_steps)\n",
        "\n",
        "\n",
        "# epoch_idx: int = 1\n",
        "# single_channel_power_data: NDArray = np.squeeze(power[epoch_idx, ch_idx, :, :]) # (n_epochs, n_freqs, n_t_steps)\n",
        "\n",
        "# n_epochs, n_freqs, n_t_steps = np.shape(single_channel_power_data)\n",
        "\n",
        "np.shape(single_channel_power_data) # (n_epochs, n_freqs, n_t_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## time subset\n",
        "active_epoch_idx: int = 1\n",
        "# single_channel_single_time_range_power_data = single_channel_power_data[:, start_t:(start_t + active_n_t_steps)]\n",
        "# n_freqs, active_n_t_steps = np.shape(single_channel_single_time_range_power_data)\n",
        "# print(f'active_n_t_steps: {active_n_t_steps}')\n",
        "\n",
        "single_channel_single_epoch_power_data = single_channel_power_data[active_epoch_idx, :, :]\n",
        "n_freqs, active_n_t_steps = np.shape(single_channel_single_epoch_power_data)\n",
        "print(f'single_channel_single_epoch_power_data: {np.shape(single_channel_single_epoch_power_data)}')\n",
        "\n",
        "n_freqs\n",
        "active_n_t_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "single_channel_single_epoch_power_data\n",
        "\n",
        "np.ptp(single_channel_single_epoch_power_data)\n",
        "\n",
        "np.nanmin(single_channel_single_epoch_power_data), np.nanmax(single_channel_single_epoch_power_data)\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# data = np.random.randn(10000) * 2\n",
        "# data[::1000] = 1000  # inject outliers\n",
        "\n",
        "arr = deepcopy(single_channel_single_epoch_power_data)\n",
        "\n",
        "vmin, vmax = np.percentile(arr, [1, 99])  # clip extremes\n",
        "plt.hist(np.clip(arr, vmin, vmax), bins=100)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import napari\n",
        "# Example 4D array: (time, z, y, x)\n",
        "\n",
        "viewer = napari.Viewer()\n",
        "# viewer.add_image(raw_data, name='raw_data', channel_axis=None)  # no channel axis\n",
        "# viewer.add_image(data, name='epoched', channel_axis=None)  # no channel axis\n",
        "# viewer.add_image(power, name='power', channel_axis=None)  # no channel axis\n",
        "\n",
        "viewer.add_image(single_channel_single_epoch_power_data, name='single_channel_single_epoch_power_data', channel_axis=None)  # no channel axis\n",
        "\n",
        "napari.run()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 5), sharey=True, layout=\"constrained\", num='Continuous Wavelet: single_channel_single_time_range_power_data')\n",
        "# ax.imshow(single_channel_single_epoch_power_data, cmap='viridis')\n",
        "# ax.matshow(single_channel_single_epoch_power_data, vmin=0, vmax=1, cmap='viridis')\n",
        "# min_max_kwargs = dict(vmin=np.nanmin(single_channel_single_epoch_power_data), vmax=np.nanmax(single_channel_single_epoch_power_data))\n",
        "min_max_kwargs = dict(vmin=np.nanmin(single_channel_single_epoch_power_data), vmax=3.0)\n",
        "\n",
        "ax.matshow(single_channel_single_epoch_power_data, **min_max_kwargs, cmap='viridis')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyqtgraph as pg\n",
        "\n",
        "app = pg.mkQApp('WaveletPlotter- Epochs Based')\n",
        "win = pg.GraphicsLayoutWidget()\n",
        "plot = win.addPlot()\n",
        "plot.showGrid(x=True, y=True)\n",
        "\n",
        "# disable vertical panning/zooming\n",
        "plot.setMouseEnabled(x=True, y=False)\n",
        "\n",
        "# apply viridis colormap\n",
        "cmap = pg.colormap.get('viridis')\n",
        "img = pg.ImageItem(single_channel_single_epoch_power_data.T)\n",
        "img.setLookupTable(cmap.getLookupTable(0.0, 1.0, 256))\n",
        "\n",
        "plot.addItem(img)\n",
        "\n",
        "# auto-range to image\n",
        "plot.setAspectLocked(False)\n",
        "plot.getViewBox().autoRange()\n",
        "\n",
        "# fix y-range to image bounds\n",
        "y_min, y_max = 0, data.shape[0]\n",
        "plot.setYRange(y_min, y_max, padding=0)\n",
        "plot.vb.setLimits(yMin=y_min, yMax=y_max)\n",
        "\n",
        "win.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg.image(single_channel_single_epoch_power_data.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg.image(single_channel_power_data.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Raw (NO EPOCHS, RAW DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from mne.time_frequency import psd_welch\n",
        "from mne.time_frequency import psd_array_welch, psd_array_multitaper, Spectrum\n",
        "## INPUT: a_raw\n",
        "\n",
        "# define conventional EEG bands\n",
        "bands = {'delta': (1,4), 'theta': (4,8), 'alpha': (8,13), 'beta': (13,30), 'gamma': (30,40)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.time_frequency import psd_array_welch\n",
        "import numpy as np\n",
        "\n",
        "raw_data = a_raw.get_data()  # (n_channels, n_times)\n",
        "sfreq = a_raw.info['sfreq']\n",
        "\n",
        "# Welch PSD\n",
        "psds, freqs = psd_array_welch(raw_data, sfreq=sfreq, fmin=1, fmax=40,\n",
        "                              n_fft=2048, n_overlap=1024, n_per_seg=2048, verbose=True)\n",
        "\n",
        "# Conventional EEG bands\n",
        "bands = {'delta': (1,4), 'theta': (4,8), 'alpha': (8,13), 'beta': (13,30), 'gamma': (30,40)}\n",
        "band_power = {b: psds[:, (freqs>=fmin)&(freqs<fmax)].mean(axis=1) for b,(fmin,fmax) in bands.items()}\n",
        "\n",
        "# band_power[b] is (n_channels,) array\n",
        "\n",
        "band_power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute PSD for all channels (continuous)\n",
        "psds, freqs = psd_welch(a_raw, fmin=1, fmax=40, n_fft=2048, n_overlap=1024, n_per_seg=2048, average='mean')\n",
        "\n",
        "# compute average band power\n",
        "band_power = {b: psds[:, (freqs>=fmin)&(freqs<fmax)].mean(axis=1) for b,(fmin,fmax) in bands.items()}\n",
        "\n",
        "## OUTPUTS: band_power\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a_spectrum: Spectrum = a_raw.compute_psd(method=\"welch\", fmin=1, fmax=40) # , picks=\"eeg\", tmin=10, tmax=200\n",
        "a_spectrum: Spectrum  = a_raw.compute_psd(method='welch', fmin=1, fmax=40, n_fft=2048, n_overlap=1024, n_per_seg=2048, average='mean')\n",
        "psds = a_spectrum.get_data()  # shape: (n_channels, n_freqs)\n",
        "freqs = a_spectrum.freqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "band_power = {b: psds[:, (freqs>=fmin)&(freqs<fmax)].mean(axis=1) for b,(fmin,fmax) in bands.items()}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import RawExtended, RawArrayExtended\n",
        "\n",
        "## INPUT: a_raw\n",
        "print(\"Pre-computing all CWTs... (this may take a moment)\")\n",
        "\n",
        "# Compute time-frequency power using Morlet wavelets -- NO EPOCHS, RAW DATA\n",
        "# power = a_raw.compute_tfr(method=\"morlet\",\n",
        "power = mne.time_frequency.tfr_morlet(\n",
        "    a_raw,\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    return_itc=False,\n",
        "    # average=False, # Compute TFR on continuous data without averaging\n",
        "    # average=True, # Compute TFR on continuous data without averaging\n",
        "    decim=3, # Decimate the output for faster computation and smaller file size\n",
        "    n_jobs=-1 # Use 1 core for this example\n",
        ")\n",
        "power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "# get raw data (channels Ã time)\n",
        "data = a_raw.get_data()[np.newaxis, ...]  # shape: (n_epochs=1, n_channels, n_times)\n",
        "\n",
        "# run morlet on continuous data\n",
        "power: NDArray = tfr_array_morlet(\n",
        "    data=data,\n",
        "    sfreq=a_raw.info['sfreq'],\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output='power',\n",
        "    decim=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# power shape: (n_epochs=1, n_channels, n_freqs, n_times)\n",
        "power = np.squeeze(power) ## remove epoch\n",
        "print(f'np.shape(power): {np.shape(power)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_channels, n_freqs, n_t_steps = np.shape(power)\n",
        "\n",
        "ch_idx: int = 0\n",
        "single_channel_power_data: NDArray = np.squeeze(power[ch_idx, :, :]) # (n_freqs, n_t_steps)\n",
        "\n",
        "# n_freqs, n_t_steps = np.shape(single_channel_power_data)\n",
        "\n",
        "np.shape(single_channel_power_data) # (n_freqs, n_t_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## time subset\n",
        "active_n_t_steps: int = 1000\n",
        "start_t = 2000\n",
        "single_channel_single_time_range_power_data = single_channel_power_data[:, start_t:(start_t + active_n_t_steps)]\n",
        "\n",
        "n_freqs, active_n_t_steps = np.shape(single_channel_single_time_range_power_data)\n",
        "\n",
        "print(f'active_n_t_steps: {active_n_t_steps}')\n",
        "n_t_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 5), sharey=True, layout=\"constrained\", num='Continuous Wavelet: single_channel_single_time_range_power_data')\n",
        "ax.imshow(single_channel_single_time_range_power_data)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyqtgraph as pg\n",
        "\n",
        "app = pg.mkQApp('WaveletPlotter')\n",
        "win = pg.GraphicsLayoutWidget()\n",
        "plot = win.addPlot()\n",
        "plot.showGrid(x=True, y=True)\n",
        "\n",
        "# disable vertical panning/zooming\n",
        "plot.setMouseEnabled(x=True, y=False)\n",
        "\n",
        "# apply viridis colormap\n",
        "cmap = pg.colormap.get('viridis')\n",
        "img = pg.ImageItem(single_channel_power_data.T)\n",
        "img.setLookupTable(cmap.getLookupTable(0.0, 1.0, 256))\n",
        "\n",
        "plot.addItem(img)\n",
        "\n",
        "# auto-range to image\n",
        "plot.setAspectLocked(False)\n",
        "plot.getViewBox().autoRange()\n",
        "\n",
        "# fix y-range to image bounds\n",
        "y_min, y_max = 0, data.shape[0]\n",
        "plot.setYRange(y_min, y_max, padding=0)\n",
        "plot.vb.setLimits(yMin=y_min, yMax=y_max)\n",
        "\n",
        "win.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg.image(single_channel_single_time_range_power_data.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pg.image(single_channel_power_data.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n",
        "# out = BasicBinnedImageRenderingWindow(single_channel_power_data.T, np.arange(n_t_steps), freqs, name='single_channel_power', title=\"single_channel_power (n_channels, n_freqs, n_t_steps)\", variable_label='single_channel_power')\n",
        "out = BasicBinnedImageRenderingWindow(single_channel_single_time_range_power_data.T, np.arange(active_n_t_steps), freqs, name='single_channel_power', title=\"single_channel_power (n_channels, n_freqs, n_t_steps)\", variable_label='single_channel_power')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the average power across all EEG channels\n",
        "power.plot_topo(\n",
        "    baseline=(None, 0), # Optional: Set baseline for normalization\n",
        "    mode='logratio',\n",
        "    title='Average power',\n",
        "    cmap='viridis',\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# Plot the power for a specific channel\n",
        "channel_name = a_raw.ch_names[0]\n",
        "power.plot(\n",
        "    picks=[channel_name],\n",
        "    combine='mean',\n",
        "    baseline=(None, 0),\n",
        "    mode='logratio',\n",
        "    title=f'Power for channel {channel_name}',\n",
        ")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPT 2025-09-21 4:22pm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "# continuous data: (n_channels, n_times)\n",
        "raw_data = a_raw.get_data()\n",
        "print(f'raw_data.shape: {raw_data.shape}')\n",
        "\n",
        "freqs = np.logspace(*np.log10([2, 40]), num=20)\n",
        "n_cycles = np.linspace(3, 10, len(freqs))  # better low-freq resolution\n",
        "\n",
        "power = tfr_array_morlet(\n",
        "    data=raw_data[np.newaxis, ...],  # add epoch axis -> (1, n_channels, n_times)\n",
        "    sfreq=a_raw.info['sfreq'],\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output='power',\n",
        "    decim=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# shape: (1, n_channels, n_freqs, n_times)\n",
        "power = np.squeeze(power)  # (n_channels, n_freqs, n_times)\n",
        "print(f'power.shape: {power.shape}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example: first channel\n",
        "ch_idx = 0\n",
        "channel_power = power[ch_idx]  # (n_freqs, n_times)\n",
        "\n",
        "plt.figure(num='WAVELET_dBPower', clear=True)\n",
        "plt.imshow(channel_power, aspect='auto', origin='lower',\n",
        "           extent=[0, channel_power.shape[1]/(a_raw.info['sfreq']/3), freqs[0], freqs[-1]])\n",
        "plt.yscale('log')\n",
        "plt.colorbar(label='Power')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUTS: a_raw, freqs, \n",
        "a_raw_hp = a_raw.copy().filter(l_freq=1.0, h_freq=None)\n",
        "power = tfr_array_morlet(data=a_raw_hp.get_data()[np.newaxis,...], sfreq=a_raw.info['sfreq'], freqs=freqs, n_cycles=np.linspace(3,10,len(freqs)), output='power', decim=1, n_jobs=1)\n",
        "power = np.squeeze(power); power_db = 10*np.log10(power+1e-12)\n",
        "np.shape(power)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example: first channel\n",
        "ch_idx = 0\n",
        "channel_power = power[ch_idx]  # (n_freqs, n_times)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(num='highpass_WAVELET_dBPower', clear=True)\n",
        "plt.imshow(channel_power, aspect='auto', origin='lower',\n",
        "           extent=[0, channel_power.shape[1]/(a_raw.info['sfreq']/3), freqs[0], freqs[-1]])\n",
        "plt.yscale('log')\n",
        "plt.colorbar(label='Power')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### @@@@@@ THIS SPECTOGRAM LOOKS GREAT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.signal import spectrogram\n",
        "\n",
        "## INPUTS: ch_idx: int\n",
        "\n",
        "f, t, Sxx = spectrogram(a_raw.get_data()[ch_idx], fs=a_raw.info['sfreq'], nperseg=1024, noverlap=512)\n",
        "\n",
        "plt.figure(num='spectrogram', clear=True)\n",
        "plt.pcolormesh(t, f, 10*np.log10(Sxx+1e-12), shading='auto'); plt.ylim([1,40])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw = deepcopy(_out_eeg_raw[-3])\n",
        "ch_names = deepcopy(a_raw.info.ch_names)\n",
        "# a_raw.down_convert_to_base_type()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACTIVE: refine the active raws and compute them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "from phoofflineeeganalysis.PendingNotebookCode import batch_compute_all_eeg_datasets, render_all_spectograms_to_high_quality_pdfs, plot_all_spectograms\n",
        "from phoofflineeeganalysis.PendingNotebookCode import plot_session_spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(_out_eeg_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUTS: _out_eeg_raw\n",
        "# Process only the last 5 datasets using 4 workers:\n",
        "active_only_out_eeg_raws, results = batch_compute_all_eeg_datasets(eeg_raws=_out_eeg_raw, limit_num_items=150, max_workers = 4)\n",
        "\n",
        "## OUTPUTS: active_only_out_eeg_raws, results\n",
        "# 1m 19.8s for 25 sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import XDFDataStreamAccessor\n",
        "\n",
        "num_sessions: int = len(results)\n",
        "num_sessions\n",
        "\n",
        "# xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df, active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df, active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "# xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df[_out_xdf_stream_infos_df['name'] == 'Epoc X'], active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "xdf_stream_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flat_annotations = []\n",
        "\n",
        "for an_xdf_dataset_idx in np.arange(num_sessions):\n",
        "    a_raw = active_only_out_eeg_raws[an_xdf_dataset_idx]\n",
        "    a_meas_date = a_raw.info.get('meas_date')\n",
        "    a_result = results[an_xdf_dataset_idx]\n",
        "    a_stream_info = deepcopy(_out_xdf_stream_infos_df).loc[an_xdf_dataset_idx]    \n",
        "    # print(f'i: {i}, a_meas_date: {a_meas_date}, a_stream_info: {a_stream_info}\\n\\n')\n",
        "    print(f'i: {an_xdf_dataset_idx}, a_meas_date: {a_meas_date}')\n",
        "    a_df = a_raw.annotations.to_data_frame(time_format='datetime')\n",
        "    a_df = a_df[a_df['description'] != 'BAD_motion']\n",
        "    a_df['xdf_dataset_idx'] = an_xdf_dataset_idx\n",
        "    flat_annotations.append(a_df)\n",
        "\n",
        "flat_annotations = pd.concat(flat_annotations, ignore_index=True)\n",
        "flat_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flat_annotations['onset_str'] = flat_annotations['onset'].dt.strftime(\"%Y-%m-%d_%I:%M:%S.%f %p\")\n",
        "flat_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
        "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import ScatterItemData\n",
        "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## It's passed a specific computation_result which has a .sess attribute that's used to determine which spikes are displayed or not.\n",
        "spike_raster_window: Spike3DRasterWindowWidget = Spike3DRasterWindowWidget(spikes_df, type_of_3d_plotter=type_of_3d_plotter, application_name=f'Spike Raster Window - {active_display_fn_identifying_ctx_string}', neuron_colors=neuron_colors, neuron_sort_order=neuron_sort_order,\n",
        "                                                                            params_kwargs=dict(use_docked_pyqtgraph_plots=use_docked_pyqtgraph_plots),\n",
        "                                                                            ) ## surprisingly only needs spikes_df !!?!\n",
        "# Set Window Title Options:\n",
        "a_file_prefix = str(computation_result.sess.filePrefix.resolve())\n",
        "spike_raster_window.setWindowFilePath(a_file_prefix)\n",
        "spike_raster_window.setWindowTitle(f'Spike Raster Window - {active_config_name} - {a_file_prefix}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "from phoofflineeeganalysis.UI.CustomCalendarWidget import CalendarDatasource\n",
        "from phoofflineeeganalysis.UI.CustomCalendarWidget import CustomDataDisplayingCalendar\n",
        "\n",
        "a_ds: CalendarDatasource = CalendarDatasource(xdf_stream_infos_df=xdf_stream_infos_df)\n",
        "ex = CustomDataDisplayingCalendar()\n",
        "ex.show()\n",
        "ex.set_datasource(data_source=a_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "found_df = a_ds.get_records_for_day(day_date=datetime(2025, 9, 10, tzinfo=timezone.utc))\n",
        "found_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "day_date = datetime(2025, 9, 11, tzinfo=timezone.utc)\n",
        "day_date = day_date.replace(hour=0, minute=0, second=0, microsecond=0).astimezone(tz=timezone.utc)\n",
        "day_date\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphocorehelpers.plotting.image_plotting_helpers import IMShowHelpers\n",
        "\n",
        "\n",
        "\n",
        "def plot_matrix(xbin_edges, ybin_edges, matrix, ax, **kwargs):\n",
        "\n",
        "    def setup_stable_axes_limits(xbins_edges, ybin_edges, ax):\n",
        "        \" manually sets the axis data limits to disable autoscaling given the xbins_edges/ybin_edges \"\n",
        "        # x == horizontal orientation:\n",
        "        ax.set_xlim(left=xbins_edges[0], right=xbins_edges[-1])\n",
        "        ax.set_ylim(bottom=ybin_edges[0], top=ybin_edges[-1])\n",
        "\n",
        "\n",
        "    variable_value = matrix\n",
        "\n",
        "    xmin, xmax, ymin, ymax = (xbin_edges[0], xbin_edges[-1], ybin_edges[0], ybin_edges[-1]) # the same for both orientations\n",
        "    x_first_extent = (xmin, xmax, ymin, ymax) # traditional order of the extant axes\n",
        "    # y_first_extent = (ymin, ymax, xmin, xmax) # swapped the order of the extent axes.\n",
        "    main_plot_kwargs = {\n",
        "        'cmap': 'viridis',\n",
        "        'origin':'lower',\n",
        "        'extent':x_first_extent,\n",
        "        'aspect': 'auto',        \n",
        "    }\n",
        "\n",
        "    \"\"\"\n",
        "    Note that changing the origin while keeping everything else the same doesn't flip the direction of the yaxis labels despite flipping the yaxis of the data.\n",
        "    \"\"\"\n",
        "    im_out = ax.imshow(variable_value, **main_plot_kwargs)\n",
        "    setup_stable_axes_limits(xbin_edges, ybin_edges, ax)\n",
        "    return im_out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(layout=\"constrained\", figsize=[9, 9], dpi=220, clear=True) # figsize=[Width, height] in inches.\n",
        "long_width_ratio = 1\n",
        "ax_dict = fig.subplot_mosaic(\n",
        "    [\n",
        "        [\"ax_dumb\", \"ax_dumb_avg\"],\n",
        "        [\"ax_good\", \"ax_good_avg\"],\n",
        "\t\t\n",
        "        # [\"ax_dumb_avg\"],\n",
        "        # [\"ax_good_avg\"],\n",
        "    ],\n",
        "    # set the height ratios between the rows\n",
        "    # set the width ratios between the columns\n",
        "    width_ratios=[10, 1],\n",
        "    sharey=True,\n",
        "    gridspec_kw=dict(wspace=0, hspace=0.0) # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import napari\n",
        "\n",
        "viewer = napari.Viewer(ndisplay=3)\n",
        "\n",
        "# viewer.add_points(mov_avg_filtered, size=2, face_color='red', name='mov_avg_filtered')\n",
        "# viewer.add_points(data2, size=2, face_color='green', name='dataset2')\n",
        "# viewer.add_points(data3, size=2, face_color='blue', name='dataset3')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "named_sessions_dict = {'dumb': -2,\n",
        "\t\t\t\t\t   'good': -6,\n",
        "}\n",
        "\n",
        "# dumb_session_idx: int = -2\n",
        "\n",
        "_out_layers = {}\n",
        "\n",
        "for a_name, an_xdf_dataset_idx in named_sessions_dict.items():\n",
        "    a_raw = active_only_out_eeg_raws[an_xdf_dataset_idx]\n",
        "\n",
        "    a_df = a_raw.annotations.to_data_frame(time_format='datetime')\n",
        "    a_df = a_df[a_df['description'] != 'BAD_motion']\n",
        "\n",
        "    a_result = results[an_xdf_dataset_idx]\n",
        "    # a_stream_info = deepcopy(_out_xdf_stream_infos_df).loc[an_xdf_dataset_idx]    \n",
        "    Sxx = a_result['spectogram']['Sxx']\n",
        "    Sxx_avg = a_result['spectogram']['Sxx_avg']\n",
        "    # t = a_result['spectogram']['t']\n",
        "    # freqs = a_result['spectogram']['freqs']\n",
        "    # fs = a_result['spectogram']['fs']\n",
        "    freqs, t, _ = a_result['spectogram']['spectogram_result_dict']['AF3']\n",
        "    \n",
        "    Sxx_avg_across_channel_avg = np.atleast_2d(Sxx_avg.mean(dim='channels', skipna=True))\n",
        "    # Sxx_avg_across_channel_avg\n",
        "    \n",
        "    Sxx_across_channel_avg = Sxx.mean(dim='channels', skipna=True)\n",
        "    # Sxx_across_channel_avg\n",
        "\n",
        "    ax_label = f\"ax_{a_name}\"\n",
        "    ax_label_avg = f\"ax_{a_name}_avg\"\n",
        "    # np.shape(Sxx_across_channel_avg)\n",
        "    \n",
        "    # xbin = deepcopy(t)\n",
        "    # ybin = deepcopy(freqs)\n",
        "    # xmin, xmax, ymin, ymax = (xbin[0], xbin[-1], ybin[0], ybin[-1])\n",
        "    # # xmin, xmax, ymin, ymax = (active_one_step_decoder.ybin[0], active_one_step_decoder.ybin[-1], active_one_step_decoder.xbin[0], active_one_step_decoder.xbin[-1]) # Reversed x and y axes, seems not good.\n",
        "    # extent = (xmin, xmax, ymin, ymax)\n",
        "    \n",
        "    # ax_dict[ax_label].imshow(Sxx_avg_across_channel_avg.T)\n",
        "    # ax_dict[ax_label_avg].imshow(Sxx_across_channel_avg, extent=extent, origin='lower')\n",
        "    \n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_vertical_plot_imshow(xbin_edges=np.arange(1), ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    # ax_dict[ax_label].autoscale(False)\n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_horizontal_plot_imshow(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    im_out = plot_matrix(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    ax_dict[ax_label].set_ylabel(a_name)\n",
        "    # ax_dict[ax_label].autoscale(False)\n",
        "    \n",
        "\n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_vertical_plot_imshow(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label_avg])\n",
        "    avg_im_out = plot_matrix(xbin_edges=[0.0, 1.0], ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label_avg])\n",
        "    # ax_dict[ax_label_avg].set_ylabel(f\"{a_name}_avg\")\n",
        "\n",
        "    \n",
        "    napari_img_layer_kwargs = dict(\n",
        "        # channel_axis=None,\n",
        "        # rgb=None,\n",
        "        colormap='yellow', #'bop_blue',\n",
        "        # contrast_limits=None,\n",
        "        gamma=0.02,\n",
        "        interpolation2d='nearest',\n",
        "        interpolation3d='linear',\n",
        "        rendering='mip',\n",
        "        depiction='volume',\n",
        "        # iso_threshold=None,\n",
        "        # attenuation=0.05,\n",
        "        # name=None,\n",
        "        # metadata=None,\n",
        "        # scale=None,\n",
        "        translate=None,\n",
        "        # rotate=None,\n",
        "        # shear=None,\n",
        "        # affine=None,\n",
        "        # opacity=1,\n",
        "        blending='additive',\n",
        "        visible=True,\n",
        "        # multiscale=None,\n",
        "        # cache=True,\n",
        "        # plane=None,\n",
        "        # experimental_clipping_planes=None,\n",
        "        # custom_interpolation_kernel_2d=None,\n",
        "    )\n",
        "    \n",
        "\n",
        "    curr_name = f\"{a_name}\"\n",
        "    _out_layers[curr_name] = viewer.add_image(Sxx, name=curr_name, **napari_img_layer_kwargs)\n",
        "    viewer.dims.axis_labels = Sxx.dims # ('channels', 'freqs', 'times')\n",
        "\n",
        "    curr_name = f\"{a_name}_avg\"\n",
        "    # napari_img_layer_kwargs['translate'] = [0.0, 1852.0]\n",
        "    napari_img_layer_kwargs['translate'] = [0.0, -2.0]\n",
        "    _out_avg_layer = viewer.add_image(Sxx_avg.T, name=curr_name, **napari_img_layer_kwargs) #  colormap='bop_blue', gamma=0.20, rendering='additive'\n",
        "    _out_layers[curr_name] = _out_avg_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# _out_avg_layer.translate([512.0, 0.0])\n",
        "# _out_avg_layer.set_translation([512.0, 0.0])\n",
        "# _out_avg_layer.translate = [0.0, 513.0]\n",
        "_out_avg_layer.translate = [0.0, 1852.0]\n",
        "_out_avg_layer.translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_avg_layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "viewer.dims\n",
        "viewer.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_extract_layers_info\n",
        "\n",
        "# @function_attributes(short_name=None, tags=['napari', 'config'], input_requires=[], output_provides=[], uses=[], used_by=['napari_extract_layers_info'], creation_date='2024-08-12 08:54', related_items=[])\n",
        "def extract_layer_info(a_layer):\n",
        "    \"\"\" Extracts info as a dict from a single Napari layer. \n",
        "    by default Napari layers print like: `<Shapes layer 'Shapes' at 0x1635a1e8460>`: without any properties that can be easily referenced.\n",
        "    This function extracts a dict of properties.\n",
        "\n",
        "    from pyphoplacecellanalysis.GUI.Napari.napari_helpers import extract_layer_info\n",
        "\n",
        "    \"\"\"\n",
        "    out_properties_dict = {}\n",
        "    positioning = ['scale', 'translate', 'rotate', 'shear', 'affine', 'corner_pixels']\n",
        "    visual = ['opacity', 'blending', 'visible', 'z_index', 'contrast_limits_range', '_colormap_name', 'gamma']\n",
        "    # positioning = ['scale', 'translate', 'rotate', 'shear', 'affine']\n",
        "    out_properties_dict['positioning'] = {}\n",
        "\n",
        "    for a_property_name in positioning:\n",
        "        out_properties_dict['positioning'][a_property_name] = getattr(a_layer, a_property_name)\n",
        "\n",
        "    out_properties_dict['visual'] = {}\n",
        "    for a_property_name in visual:\n",
        "        try:\n",
        "            out_properties_dict['visual'][a_property_name] = getattr(a_layer, a_property_name)\n",
        "        except (AttributeError, ValueError, KeyError, TypeError) as e:\n",
        "            print(f'failed to get property: \"{a_property_name}\" with error {e}')\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "    return out_properties_dict\n",
        "\n",
        "\n",
        "# @function_attributes(short_name=None, tags=['napari', 'config'], input_requires=[], output_provides=[], uses=['extract_layer_info'], used_by=[], creation_date='2024-08-12 08:54', related_items=[])\n",
        "def napari_extract_layers_info(layers):\n",
        "\t\"\"\"extracts info dict from each layer as well.\n",
        "\tUsage:\n",
        "        from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_extract_layers_info\n",
        "\t\tlayers = directional_viewer.layers # [<Shapes layer 'Shapes' at 0x1635a1e8460>, <Shapes layer 'Shapes [1]' at 0x164d5402e50>]\n",
        "\t\tout_layers_info_dict = debug_print_layers_info(layers)\n",
        "\n",
        "\t\"\"\"\n",
        "\tout_layers_info_dict = {}\n",
        "\tfor a_layer in layers:\n",
        "\t\ta_name: str = str(a_layer.name)\n",
        "\t\tout_properties_dict = extract_layer_info(a_layer)\n",
        "\t\tout_layers_info_dict[a_name] = out_properties_dict\n",
        "\t\t# if isinstance(a_layer, Shapes):\n",
        "\t\t# \tprint(f'shapes layer: {a_layer}')\n",
        "\t\t# \ta_shapes_layer: Shapes = a_layer\n",
        "\t\t# \t# print(f'a_shapes_layer.properties: {a_shapes_layer.properties}')\n",
        "\t\t# \tout_properties_dict = extract_layer_info(a_layer)\n",
        "\t\t# \tprint(f'\\tout_properties_dict: {out_properties_dict}')\n",
        "\t\t# \tout_layers_info_dict[a_name] = out_properties_dict\n",
        "\t\t# else:\n",
        "\t\t# \tprint(f'unknown layer: {a_layer}')\t\n",
        "\treturn out_layers_info_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "layers = viewer.layers # [<Shapes layer 'Shapes' at 0x1635a1e8460>, <Shapes layer 'Shapes [1]' at 0x164d5402e50>]\n",
        "out_layers_info_dict = napari_extract_layers_info(layers)\n",
        "out_layers_info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_layers_info_dict['dumb']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from napari.layers.image.image import Image\n",
        "\n",
        "property_names = ['metadata', 'blending', 'opacity', 'rendering', 'scale', 'gamma', 'contrast_limits_range', 'colormap']\n",
        "for a_layer in layers:\n",
        "    an_img_layer: Image = a_layer\n",
        "    \n",
        "#    type(a_layer)\n",
        "\n",
        "# an_img_layer.blending\n",
        "an_img_layer.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "napari.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xbin\n",
        "ybin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_stream_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "\n",
        "## INPUTS: a_result\n",
        "a_spectogram_result: Dict = a_result['spectogram'] \n",
        "\n",
        "ch_names = a_spectogram_result['ch_names']\n",
        "fs = a_spectogram_result['fs']\n",
        "a_spectogram_result_dict = a_spectogram_result['spectogram_result_dict'] # Dict[channel: Tuple]\n",
        "Sxx = a_spectogram_result['Sxx']\n",
        "Sxx_avg = a_spectogram_result['Sxx_avg']\n",
        "\n",
        "Sxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Sxx_avg_list = [] \n",
        "\n",
        "# ch_names = a_raw.info.ch_names\n",
        "\n",
        "for a_ch, a_tuple in a_spectogram_result_dict.items():\n",
        "    f, t, Sxx = a_tuple ## unpack the tuple\n",
        "    # np.shape(Sxx) # (513, 1116) - (n_freqs, n_times)\n",
        "    n_freqs = np.shape(f)\n",
        "    n_times = np.shape(t) \n",
        "    Sxx_avg = np.nanmean(Sxx, axis=-1) ## average over all time to get one per session\n",
        "    Sxx_avg_list.append(Sxx_avg)\n",
        "    \n",
        "Sxx_avg_list = np.stack(Sxx_avg_list) # (14, 513) - (n_channels, n_freqs)\n",
        "Sxx_avg_list = xr.DataArray(Sxx_avg_list, dims=(\"channels\", \"freqs\"), coords={\"channels\": ch_names, \"freqs\": f})\n",
        "np.shape(Sxx_avg_list)\n",
        "Sxx_avg_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ch_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Sxx_avg = np.nanmean(Sxx, axis=-1) ## average over all time to get one per session\n",
        "Sxx_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_spectograms(active_only_out_eeg_raws, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"axes.titlesize\"] = 8\n",
        "plt.rcParams[\"axes.labelsize\"] = 8\n",
        "plt.rcParams[\"xtick.labelsize\"] = 6\n",
        "plt.rcParams[\"ytick.labelsize\"] = 6\n",
        "plt.rcParams[\"legend.fontsize\"] = 6\n",
        "plt.rcParams[\"figure.titlesize\"] = 8\n",
        "plt.rcParams[\"axes.titlepad\"] = 0\n",
        "plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "plt.rcParams[\"figure.constrained_layout.h_pad\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.w_pad\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.hspace\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.hspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.hspace\"] = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Plot a synchronized EEG Raw data and Spectogram Figure together:\n",
        "active_eeg_idx: int = -4\n",
        "mne_raw_fig = active_only_out_eeg_raws[active_eeg_idx].plot(time_format='datetime', scalings='auto') # MNEBrowseFigure\n",
        "fig, axs = plot_session_spectogram(active_only_out_eeg_raws[active_eeg_idx], results[active_eeg_idx], sync_to_mne_raw_fig=mne_raw_fig)\n",
        "# plt.subplots_adjust(wspace=0, hspace=0)  # remove spacing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "active_eeg_idx: int = -6\n",
        "mne_raw_fig2 = active_only_out_eeg_raws[active_eeg_idx].plot(time_format='datetime', scalings='auto') # MNEBrowseFigure\n",
        "fig2, axs2 = plot_session_spectogram(active_only_out_eeg_raws[active_eeg_idx], results[active_eeg_idx], sync_to_mne_raw_fig=mne_raw_fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and show all spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute results first\n",
        "active_only_out_eeg_raws, results = batch_compute_all_eeg_datasets(eeg_raws=_out_eeg_raw, limit_num_items=3)\n",
        "\n",
        "# Render to PDFs (paged)\n",
        "from pathlib import Path\n",
        "out_paths = render_all_spectograms_to_high_quality_pdfs(\n",
        "    active_only_out_eeg_raws,\n",
        "    results,\n",
        "    output_parent_folder=Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/exports\"),\n",
        "    mode=\"paged\",\n",
        "    seconds_per_page=180.0,\n",
        "    freq_min_hz=1.0,\n",
        "    freq_max_hz=40.0,\n",
        "    dpi=300\n",
        ")\n",
        "print(f\"Wrote {len(out_paths)} PDF(s)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(num='spectrogram_all_channels', clear=True)\n",
        "\n",
        "fig, axs = plt.subplots(nrows=len(a_raw.info.ch_names), ncols=1, num='spectrogram_all_channels')\n",
        "\n",
        "spectogram_result_dict =a_raw_outputs['spectogram']['spectogram_result_dict']\n",
        "fs = a_raw_outputs['spectogram']['fs']\n",
        "\n",
        "for ch_idx, (a_ch, a_ch_spect_result_tuple) in enumerate(spectogram_result_dict.items()):\n",
        "    f, t, Sxx = a_ch_spect_result_tuple\n",
        "    an_ax = axs[ch_idx]\n",
        "    an_ax.pcolormesh(t, f, 10*np.log10(Sxx+1e-12), shading='auto')\n",
        "    # plt.pcolormesh(t, f, 10*np.log10(Sxx+1e-12), shading='auto', ax=an_ax)\n",
        "    plt.ylim([1, 40])\n",
        "    # plt.ylabel(f\"{a_ch}[{ch_idx}]\", ax=an_ax)\n",
        "    an_ax.set_ylabel(f\"{a_ch}[{ch_idx}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### THIS WAVLET LOOKS OKAY TOO!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.time_frequency import tfr_array_morlet\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# continuous data: (n_channels, n_times)\n",
        "raw_data = a_raw.get_data()\n",
        "sfreq = a_raw.info['sfreq']\n",
        "\n",
        "# frequencies and cycles\n",
        "freqs = np.logspace(np.log10(2), np.log10(40), num=20)\n",
        "n_cycles = np.linspace(6, 12, len(freqs))  # longer cycles at low freq â more energy captured\n",
        "\n",
        "# compute Morlet TFR\n",
        "power = tfr_array_morlet(\n",
        "    data=raw_data[np.newaxis, ...],  # add epoch axis\n",
        "    sfreq=sfreq,\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output='power',\n",
        "    decim=1,  # preserve temporal resolution\n",
        "    n_jobs=-1\n",
        ")\n",
        "power = np.squeeze(power)  # (n_channels, n_freqs, n_times)\n",
        "\n",
        "# convert to dB\n",
        "power_db = 10 * np.log10(power + 1e-12)\n",
        "\n",
        "# example: plot first channel\n",
        "ch_idx = 0\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.imshow(power_db[ch_idx], aspect='auto', origin='lower',\n",
        "           extent=[0, raw_data.shape[1]/sfreq, freqs[0], freqs[-1]])\n",
        "plt.yscale('log')\n",
        "plt.colorbar(label='Power (dB)')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('Frequency (Hz)')\n",
        "plt.title('Morlet TFR (dB)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## OLD (pre 2025-09-21) Epochs approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ==================================================================================================================================================================================================================================================================================== #\n",
        "# Epochs Approach                                                                                                                                                                                                                                                                      #\n",
        "# ==================================================================================================================================================================================================================================================================================== #\n",
        "## INPUT: a_raw\n",
        "duration = 8\n",
        "overlap = 2.5\n",
        "\n",
        "epochs = mne.make_fixed_length_epochs(a_raw, duration=duration, overlap=overlap, preload=True)\n",
        "\n",
        "# This single line computes the CWT for ALL epochs and ALL channels\n",
        "all_cwt = tfr_morlet(epochs, freqs=freqs, n_cycles=n_cycles, use_fft=True, return_itc=False, average=False, n_jobs=-1)\n",
        "print(f'done.')\n",
        "\n",
        "\n",
        "chan_idx: int = 0\n",
        "curr_chan_cwt = all_cwt.pick(chan_idx)\n",
        "# curr_chan_cwt = all_cwt[:, chan_idx, :, :]\n",
        "curr_chan_cwt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_cwt.shape\n",
        "curr_chan_cwt.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# all_cwt.plot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mne.time_frequency import tfr_array_morlet, tfr_array_multitaper, AverageTFRArray\n",
        "\n",
        "# freqs = np.arange(5.0, 100.0, 3.0)\n",
        "\n",
        "## INPUTS: epochs, freqs\n",
        "freqs\n",
        "vmin, vmax = -3.0, 3.0  # Define our color limits.\n",
        "\n",
        "power = tfr_array_morlet(\n",
        "    epochs.get_data(),\n",
        "    sfreq=epochs.info[\"sfreq\"],\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output=\"avg_power\",\n",
        "    zero_mean=False,\n",
        ")\n",
        "# Put it into a TFR container for easy plotting\n",
        "tfr = AverageTFRArray(\n",
        "    info=epochs.info, data=power, times=epochs.times, freqs=freqs, nave=len(epochs)\n",
        ")\n",
        "tfr.plot(\n",
        "    # baseline=(0.0, 0.1),\n",
        "    # picks=[0, 1],\n",
        "    mode=\"mean\",\n",
        "    # vlim=(vmin, vmax),\n",
        "    title=\"TFR calculated on a NumPy array\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from autoreject import AutoReject\n",
        "from autoreject import get_rejection_threshold\n",
        "\n",
        "fixed_len_epochs_list = []\n",
        "reject_params = []\n",
        "\n",
        "for a_raw in _out_eeg_raw:\n",
        "    fixed_len_epochs: mne.Epochs = mne.make_fixed_length_epochs(a_raw, duration=3, overlap=0.5, preload=True)\n",
        "    fixed_len_epochs_list.append(fixed_len_epochs)\n",
        "    reject = get_rejection_threshold(fixed_len_epochs)  \n",
        "    reject_params.append(reject)\n",
        "\n",
        "reject_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reject_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "reject_criteria = dict(\n",
        "    mag=4000e-15,  # 4000 fT\n",
        "    grad=4000e-13,  # 4000 fT/cm\n",
        "    eeg=150e-6,  # 150 ÂµV\n",
        "    eog=250e-6,\n",
        ")  # 250 ÂµV\n",
        "\n",
        "epochs = mne.Epochs(\n",
        "    a_raw,\n",
        "    events,\n",
        "    event_id=event_dict,\n",
        "    tmin=-0.2,\n",
        "    tmax=0.5,\n",
        "    reject=reject_criteria,\n",
        "    preload=True,\n",
        ")\n",
        "\n",
        "\n",
        "ar = AutoReject()\n",
        "epochs_clean = ar.fit_transform(fixed_len_epochs)\n",
        "epochs_clean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "reject = get_rejection_threshold(fixed_len_epochs)  \n",
        "reject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_eeg_raw[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_eeg_raw[-2]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "converted_out_eeg_raw = up_convert_raw_obj(_out_eeg_raw[0])\n",
        "# converted_out_eeg_raw.raw_timerange()\n",
        "converted_out_eeg_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# stream_infos\n",
        "# _out_xdf_stream_infos_df = pd.concat(_out_xdf_stream_infos_df)\n",
        "_out_xdf_stream_infos_df = _out_xdf_stream_infos_df.set_index('xdf_dataset_idx')\n",
        "_out_xdf_stream_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for a_raw in _out_eeg_raw:\n",
        "    # a_raw\n",
        "    # a_raw.filenames[0]\n",
        "\t# a_raw.save_to_fieldtrip_mat(\n",
        "    str(a_raw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Old \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# vmin, vmax = -3.0, 3.0  # Define our color limits.\n",
        "\n",
        "\n",
        "vmin, vmax = -10.0, 10.0  # Define our color limits.\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, layout=\"constrained\", num='Continuous Wavelet')\n",
        "ax = axs[0]\n",
        "\n",
        "single_channel_power.plot(\n",
        "    [0],\n",
        "    # baseline=(0.0, 0.1),\n",
        "    mode=\"mean\",\n",
        "    vlim=(vmin, vmax),\n",
        "    axes=ax,\n",
        "    show=False,\n",
        "    colorbar=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANalysis for Fatigue/Bandpowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.computations.fatigue_analysis import compare_multiple_recordings, compute_fatigue_metrics, analyze_fatigue_trends, print_analysis_report, visualize_fatigue_comparison\n",
        "\n",
        "raw_objects = deepcopy(_out_eeg_raw)\n",
        "raw_obj_labels = [str(a_raw) for a_raw in raw_objects]\n",
        "if len(raw_objects) >= 2:\n",
        "    results = compare_multiple_recordings(raw_objects, raw_obj_labels[:len(raw_objects)])\n",
        "    print_analysis_report(results)\n",
        "    visualize_fatigue_comparison(results)\n",
        "\n",
        "    results\n",
        "else:\n",
        "    print(\"Not enough data files found for comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sliding wavlet analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyqtgraph as pg\n",
        "from phoofflineeeganalysis.timeflux.nodes.wavelet_cwt import EEGViewer\n",
        "\n",
        "app = pg.mkQApp('EEGWaveletViewer')\n",
        "\n",
        "raw = _out_eeg_raw[-1]\n",
        "\n",
        "# Launch the Qt Application\n",
        "viewer = EEGViewer(raw_data=raw)\n",
        "viewer.show()\n",
        "# sys.exit(app.exec_())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fixed_len_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.timeflux.nodes.wavelet_cwt import plot_raw_with_cwt\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# First, create a sample 14-channel Raw object for demonstration\n",
        "# sfreq = 250\n",
        "# ch_names = [f'EEG {i+1:02d}' for i in range(14)]\n",
        "# info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
        "# n_seconds = 120\n",
        "# data = np.random.randn(len(ch_names), sfreq * n_seconds)\n",
        "# raw = mne.io.RawArray(data, info)\n",
        "\n",
        "raw = _out_eeg_raw[-1]\n",
        "\n",
        "# Now, use the function to plot the first channel (index 0) from 10 to 20 seconds\n",
        "plot_raw_with_cwt(raw, start_seconds=10.0, end_seconds=20.0, channel_index=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# You can easily inspect another channel or time window\n",
        "plot_raw_with_cwt(raw, start_seconds=35.5, end_seconds=45.5, channel_index=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exploration/Manual XDF stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_raws = raws_dict[DataModalityType.EEG.value]\n",
        "if len(eeg_raws) == 1:\n",
        "    eeg_raws = eeg_raws[0]\n",
        "\n",
        "eeg_raws\n",
        "\n",
        "# eeg_raws.filenames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# meas_date.strftime(\"%Y-%m-%dT%H-%M-%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## When done processing the entire LabRecorder.xdf, save only the EEG data (with all annotations and such added) to a new file\n",
        "labRecorder_PostProcessed_path: Path = sso.eeg_analyzed_parent_export_path.joinpath(f'LabRecorder_PostProcessed').resolve()\n",
        "labRecorder_PostProcessed_path.mkdir(exist_ok=True)\n",
        "\n",
        "a_lab_recorder_filename: str = a_xdf_file.stem\n",
        "a_lab_recorder_filename_parts = a_lab_recorder_filename.split('_')\n",
        "\n",
        "## replace with the eeg meas date\n",
        "meas_date = eeg_raws.info.get('meas_date')\n",
        "a_lab_recorder_filename_parts[-2] = meas_date.strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "\n",
        "a_lab_recorder_filename: str = '_'.join(a_lab_recorder_filename_parts[:-1]) ## drop only the last part\n",
        "a_lab_recorder_filename\n",
        "\n",
        "a_lab_recorder_filepath = labRecorder_PostProcessed_path.joinpath(a_lab_recorder_filename)\n",
        "# a_lab_recorder_filepath.with_suffix('.fif')\n",
        "\n",
        "a_lab_recorder_filepath = a_lab_recorder_filepath.with_suffix('.fif')\n",
        "print(f'saving finalized EEG data out to \"{a_lab_recorder_filepath.as_posix()}\"')\n",
        "eeg_raws.save(a_lab_recorder_filepath, overwrite=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "{a_xdf_file}'\n",
        "\n",
        "eeg_raws.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_raws.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.motion_data import MotionData\n",
        "\n",
        "motion_raw = raws_dict[DataModalityType.MOTION.value]\n",
        "if len(motion_raw) == 1:\n",
        "    motion_raw = motion_raw[0]\n",
        "\n",
        "motion_annots: mne.Annotations = MotionData.find_high_accel_periods(motion_raw, should_set_bad_period_annotations=True)\n",
        "motion_annots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_annotation_alrignment(motion_annots: mne.Annotations):\n",
        "    annot_df: pd.DataFrame = motion_annots.to_data_frame(time_format='datetime')\n",
        "    \n",
        "    motion_annots.orig_time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "annot_orig_time = motion_annots.orig_time\n",
        "annot_orig_time\n",
        "# annot_df: pd.DataFrame = motion_annots.to_data_frame(time_format='datetime')\n",
        "annot_df: pd.DataFrame = motion_annots.to_data_frame(time_format='timedelta')\n",
        "# annot_df: pd.DataFrame = motion_annots.to_data_frame(time_format='ms')\n",
        "# annot_df: pd.DataFrame = motion_annots.to_data_frame(time_format=None)\n",
        "annot_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_annot: mne.Annotations = mne.Annotations(onset=annot_df['onset'].to_numpy(), duration=annot_df['duration'].to_numpy(), description=annot_df['description'].to_numpy(), orig_time=annot_orig_time)\n",
        "# new_annot.to_data_frame(time_format='datetime')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# eeg_min, eeg_max = (eeg_raws.times[0], eeg_raws.times[-1])\n",
        "# motion_min, motion_max = (motion_raw.times[0], motion_raw.times[-1])\n",
        "\n",
        "eeg_raw_df = eeg_raws.to_data_frame(time_format='datetime')\n",
        "motion_raw_df = motion_raw.to_data_frame(time_format='datetime')\n",
        "eeg_raw_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_col_name: str = 'time'\n",
        "eeg_min, eeg_max = (eeg_raw_df[time_col_name].min(), eeg_raw_df[time_col_name].max())\n",
        "motion_min, motion_max = (motion_raw_df[time_col_name].min(), motion_raw_df[time_col_name].max())\n",
        "\n",
        "eeg_min, eeg_max\n",
        "motion_min, motion_max\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MNEHelpers.merge_annotations(raw=eeg_raws, new_annots=motion_annots, align_to_Raw_meas_time=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MNEHelpers.merge_annotations(raw=eeg_raws, new_annots=motion_annots, align_to_Raw_meas_time=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_raws.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a_df[accel_col_names].plot()\n",
        "a_df[accel_col_names + accel_diff_col_names].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "motion_raw.plot()\n",
        "# motion_raw.select_channels(['AccX', 'AccY', 'AccZ'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_df = pd.concat([v.to_data_frame(time_format='datetime') for v in raws_dict[DataModalityType.EEG.value]]).drop_duplicates(subset=['time'], inplace=False, keep='first')\n",
        "eeg_df\n",
        "\n",
        "eeg_df = pd.concat([v.to_data_frame(time_format='datetime') for v in raws_dict[DataModalityType.EEG.value]]).drop_duplicates(subset=['time'], inplace=False, keep='first')\n",
        "eeg_df\n",
        "\n",
        "eeg_df = pd.concat([v.to_data_frame(time_format='datetime') for v in raws_dict[DataModalityType.PHO_LOG_TO_LSL.value]]).drop_duplicates(subset=['time'], inplace=False, keep='first')\n",
        "eeg_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers\n",
        "\n",
        "\n",
        "for an_annotation_ds in raws_dict[DataModalityType.PHO_LOG_TO_LSL.value]:\n",
        "    # raws_dict[DataModalityType.EEG.value] = [an_eeg_ds.set_annotations(an_annotation_ds) for an_eeg_ds in raws_dict[DataModalityType.EEG.value]]\n",
        "    for an_eeg_ds in raws_dict[DataModalityType.EEG.value]:\n",
        "        # an_eeg_ds.annotations\n",
        "        meas_date = deepcopy(an_eeg_ds.info.get('meas_date'))\n",
        "        # meas_date\n",
        "        # an_annotation_ds.orig_time\n",
        "        # an_annotation_ds.to_data_frame()\n",
        "        # MNEHelpers.merge_annotations(raw=an_eeg_ds, new_annots=an_annotation_ds, align_to_Raw_meas_time=True)\n",
        "        MNEHelpers.merge_annotations(raw=an_eeg_ds, new_annots=an_annotation_ds, align_to_Raw_meas_time=False)        \n",
        "        # if (an_eeg_ds.annotations is None) or (len(an_eeg_ds.annotations) < 1):\n",
        "        #     # an_eeg_ds.annotations = an_annotation_ds\n",
        "        #     an_eeg_ds.set_annotations(an_annotation_ds)\n",
        "        # else:\n",
        "        #     # a_raw: mne.io.Raw = mne.io.Raw(an_eeg_ds)\n",
        "        #     an_eeg_ds.set_annotations(an_annotation_ds)\n",
        "        #     # an_eeg_ds.set_annotation(an_annotation_ds)\n",
        "        an_eeg_ds.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw = raws_dict[DataModalityType.EEG.value][0]\n",
        "a_raw.set_annotations(an_annotation_ds)\n",
        "a_raw.annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "annotations = raws_dict[DataModalityType.EEG.value][0].annotations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stream_infos['name'].to_list()\n",
        "\n",
        "\n",
        "stream_name_to_modality_dict = {'Epoc X': 'EEG', 'Epoc X Motion':'MOTION', 'Epoc X eQuality':None}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_streams = stream_infos[stream_infos['type'] == 'EEG']['stream_id'].to_numpy()\n",
        "eeg_streams\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "eeg_raws = [raws[stream_idx-1] for stream_idx in eeg_streams]\n",
        "eeg_raws"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
