{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "tags": [
          "run_group_main",
          "run_unpickle_pipeline",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "%config IPCompleter.use_jedi = False\n",
        "%pdb off\n",
        "%load_ext autoreload\n",
        "%autoreload 3\n",
        "# %matplotlib inline\n",
        "%matplotlib qt5\n",
        "import mne\n",
        "mne.viz.set_browser_backend(\"qt\")  # or \"matplotlib\"\n",
        "mne.set_config(\"MNE_BROWSER_BACKEND\", \"qt\")  # or \"matplotlib\"\n",
        "%gui qt\n",
        "\n",
        "import IPython\n",
        "\n",
        "# Jupyter-lab enable printing for any line on its own (instead of just the last one in the cell)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Use MNE to load and analyze saved EEG and Motion recordings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "tags": [
          "run_group_main",
          "run_unpickle_pipeline",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import re\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import uuid\n",
        "from copy import deepcopy\n",
        "from typing import Dict, List, Tuple, Optional, Callable, Union, Any\n",
        "from nptyping import NDArray\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "import mne\n",
        "from mne import set_log_level\n",
        "from copy import deepcopy\n",
        "import mne\n",
        "\n",
        "from mne.io import read_raw\n",
        "\n",
        "datasets = []\n",
        "# mne.viz.set_browser_backend(\"Matplotlib\")\n",
        "mne.viz.set_browser_backend(\"qt\")\n",
        "\n",
        "from mne_lsl.player import PlayerLSL as Player\n",
        "from mne_lsl.stream import StreamLSL as Stream\n",
        "\n",
        "from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers\n",
        "from phoofflineeeganalysis.analysis.historical_data import HistoricalData\n",
        "from phoofflineeeganalysis.analysis.motion_data import MotionData\n",
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "from phoofflineeeganalysis.analysis.anatomy_and_electrodes import ElectrodeHelper\n",
        "# from ..EegProcessing import bandpower\n",
        "# from phoofflineeeganalysis.EegProcessing import analyze_eeg_trends\n",
        "from phoofflineeeganalysis.EegVisualization import VisHelpers\n",
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import SavedSessionsProcessor, SessionModality, DataModalityType\n",
        "\n",
        "set_log_level(\"WARNING\")\n",
        "\n",
        "# eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()\n",
        "# headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()\n",
        "\n",
        "# assert eeg_recordings_file_path.exists()\n",
        "# assert headset_motion_recordings_file_path.exists()\n",
        "\n",
        "eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()\n",
        "flutter_eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings').resolve()\n",
        "flutter_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/MOTION_RECORDINGS').resolve()\n",
        "flutter_GENERIC_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/GENERIC_RECORDINGS').resolve()\n",
        "\n",
        "headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()\n",
        "WhisperVideoTranscripts_LSL_Converted = Path(r\"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted\").resolve()\n",
        "pho_log_to_LSL_recordings_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/PhoLogToLabStreamingLayer_logs').resolve()\n",
        "## These contain little LSL .fif files with names like: '20250808_062814_log.fif', \n",
        "\n",
        "eeg_analyzed_parent_export_path = Path(\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed\").resolve()\n",
        "pickled_data_path = Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/PICKLED_COLLECTION\").resolve()\n",
        "assert pickled_data_path.exists()\n",
        "\n",
        "# n_most_recent_sessions_to_preprocess: int = None # None means all sessions\n",
        "# n_most_recent_sessions_to_preprocess: int = 35\n",
        "# n_most_recent_sessions_to_preprocess: int = 5\n",
        "n_most_recent_sessions_to_preprocess = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SavedSessionProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_main",
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "\n",
        "sso: SavedSessionsProcessor = SavedSessionsProcessor(eeg_recordings_file_path=eeg_recordings_file_path,\n",
        "                                                     headset_motion_recordings_file_path=headset_motion_recordings_file_path, WhisperVideoTranscripts_LSL_Converted_file_path=WhisperVideoTranscripts_LSL_Converted, pho_log_to_LSL_recordings_path=pho_log_to_LSL_recordings_path,\n",
        "                                                    eeg_analyzed_parent_export_path=eeg_analyzed_parent_export_path, \n",
        "                                                     n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess, \n",
        "                                                    should_load_data=True, should_load_preprocessed=False,\n",
        "                                                    #  should_load_data=True, should_load_preprocessed=True,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025-09-18 - LabRecorder XDF Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGData\n",
        "from phoofflineeeganalysis.analysis.MNE_helpers import DatasetDatetimeBoundsRenderingMixin, RawArrayExtended, RawExtended, up_convert_raw_objects, up_convert_raw_obj\n",
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import LabRecorderXDF, unwrap_single_element_listlike_if_needed\n",
        "\n",
        "\n",
        "lab_recorder_output_path = Path(r\"E:\\Dropbox (Personal)\\Databases\\UnparsedData\\LabRecorderStudies\\sub-P001\").resolve()\n",
        "assert lab_recorder_output_path.exists()\n",
        "\n",
        "labRecorder_PostProcessed_path: Path = sso.eeg_analyzed_parent_export_path.joinpath(f'LabRecorder_PostProcessed')\n",
        "labRecorder_PostProcessed_path.mkdir(exist_ok=True)\n",
        "\n",
        "should_write_final_merged_eeg_fif: bool = True\n",
        "# should_write_final_merged_eeg_fif: bool = False\n",
        "_out_eeg_raw, _out_xdf_stream_infos_df, lab_recorder_xdf_files = LabRecorderXDF.load_and_process_all(lab_recorder_output_path=lab_recorder_output_path, \n",
        "                                                                                                     labRecorder_PostProcessed_path=labRecorder_PostProcessed_path, should_write_final_merged_eeg_fif=should_write_final_merged_eeg_fif)\n",
        "xdf_dataset_indicies = np.unique(deepcopy(_out_xdf_stream_infos_df).reset_index(drop=False, inplace=False)['xdf_dataset_idx'].to_numpy())\n",
        "n_unique_xdf_datasets: int = len(xdf_dataset_indicies)\n",
        "print(f'n_unique_xdf_datasets: {n_unique_xdf_datasets}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import XDFDataStreamAccessor\n",
        "\n",
        "\n",
        "_out_xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df, active_only_out_eeg_raws=_out_eeg_raw) # [_out_xdf_stream_infos_df['name'] == 'Epoc X']\n",
        "_out_xdf_stream_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster, SynchronizedPlotMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_xdf_stream_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(_out_xdf_stream_infos_df)\n",
        "_out_xdf_stream_infos_df.loc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_eeg_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Batch Computations on `_out_eeg_raw`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_sessions: int = len(_out_eeg_raw)\n",
        "num_sessions\n",
        "\n",
        "\n",
        "results_dict = {}\n",
        "\n",
        "for an_xdf_dataset_idx in np.arange(num_sessions):\n",
        "    a_raw = deepcopy(_out_eeg_raw[an_xdf_dataset_idx])\n",
        "    a_raw.down_convert_to_base_type()\n",
        "    results_dict[an_xdf_dataset_idx] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw = deepcopy(_out_eeg_raw[-3])\n",
        "a_raw.down_convert_to_base_type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUT: fixed_len_epochs\n",
        "freqs = np.arange(5., 40., 1.0)\n",
        "# Define frequencies and number of cycles\n",
        "# freqs = np.logspace(*np.log10([2, 40]), num=20)\n",
        "n_cycles = freqs / 2.0 # A common approach is to use a fixed number of cycles or a value that increases with frequency.\n",
        "\n",
        "freqs\n",
        "n_cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphoplacecellanalysis.GUI.PyQtPlot.BinnedImageRenderingWindow import BasicBinnedImageRenderingWindow, LayoutScrollability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fixed Length Epoch-based:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.MNE_helpers import RawExtended, RawArrayExtended\n",
        "\n",
        "## INPUT: a_raw\n",
        "fixed_len_epochs = mne.make_fixed_length_epochs(a_raw, duration=8, preload=True, reject_by_annotation=False)\n",
        "fixed_len_epochs\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## INPUT: fixed_len_epochs\n",
        "freqs = np.arange(5., 40., 1.0)\n",
        "\n",
        "# Define frequencies and number of cycles\n",
        "# freqs = np.logspace(*np.log10([2, 40]), num=20)\n",
        "n_cycles = freqs / 2.0 # A common approach is to use a fixed number of cycles or a value that increases with frequency.\n",
        "\n",
        "# n_cycles = np.linspace(3,10,len(freqs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Compute time-frequency power using Morlet wavelets -- NO EPOCHS, RAW DATA\n",
        "# power = a_raw.compute_tfr(method=\"morlet\",\n",
        "power = mne.time_frequency.tfr_morlet(\n",
        "    fixed_len_epochs,\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    return_itc=False,\n",
        "    average=False, # Compute TFR on continuous data without averaging\n",
        "    # average=True, # Compute TFR on continuous data without averaging\n",
        "    decim=3, # Decimate the output for faster computation and smaller file size\n",
        "    n_jobs=-1 # Use 1 core for this example\n",
        ")\n",
        "power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Array version:\n",
        "from mne.time_frequency import tfr_array_morlet\n",
        "\n",
        "# get raw data (channels × time)\n",
        "data = fixed_len_epochs.get_data() # shape: (n_epochs, n_channels, n_times)\n",
        "print(f'np.shape(data): {np.shape(data)}')\n",
        "# run morlet on continuous data\n",
        "power: NDArray = tfr_array_morlet(\n",
        "    data=data,\n",
        "    sfreq=fixed_len_epochs.info['sfreq'],\n",
        "    freqs=freqs,\n",
        "    n_cycles=n_cycles,\n",
        "    output='power',\n",
        "    decim=3,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# power shape: (n_epochs=1, n_channels, n_freqs, n_times)\n",
        "power = np.squeeze(power) ## remove epoch\n",
        "print(f'np.shape(power): {np.shape(power)}')\n",
        "\n",
        "# np.shape(data): (593, 14, 1024)\n",
        "# np.shape(power): (593, 14, 20, 342)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "repo_data_folder_path = Path(r\"C:\\Users\\pho\\repos\\EmotivEpoc\\PhoLabStreamingReceiver\\data\").resolve()\n",
        "\n",
        "h5_file_path = repo_data_folder_path.joinpath(f'2025-09-21_exported_sess.h5').resolve()\n",
        "\n",
        "print(f'writing to \"{h5_file_path.as_posix()}\"')\n",
        "\n",
        "# raw_data = a_raw.get_data() # (14, 607857)\n",
        "# np.shape(raw_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with h5py.File(h5_file_path, \"w\") as f:\n",
        "    \n",
        "    d0 = f.create_dataset(\"raw\", data=raw_data)\n",
        "    d0.attrs[\"dim_labels\"] = [\"channels\", \"time\"]\n",
        "\n",
        "    d1 = f.create_dataset(\"epoched\", data=data)\n",
        "    d1.attrs[\"dim_labels\"] = [\"epochs\", \"channels\", \"time\"]\n",
        "\n",
        "    d2 = f.create_dataset(\"power\", data=power)\n",
        "    d2.attrs[\"dim_labels\"] = [\"epochs\", \"channels\", \"time\", \"frequency\"]\n",
        "\n",
        "\n",
        "print(f'done! wrote to \"{h5_file_path.as_uri()}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ACTIVE: refine the active raws and compute them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "from phoofflineeeganalysis.PendingNotebookCode import batch_compute_all_eeg_datasets, render_all_spectograms_to_high_quality_pdfs, plot_all_spectograms\n",
        "from phoofflineeeganalysis.PendingNotebookCode import plot_session_spectogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "run_group_xdf_only"
        ]
      },
      "outputs": [],
      "source": [
        "## INPUTS: _out_eeg_raw\n",
        "# Process only the last 5 datasets using 4 workers:\n",
        "active_only_out_eeg_raws, results = batch_compute_all_eeg_datasets(eeg_raws=_out_eeg_raw, limit_num_items=150, max_workers = 4)\n",
        "\n",
        "## OUTPUTS: active_only_out_eeg_raws, results\n",
        "# 1m 19.8s for 25 sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import XDFDataStreamAccessor\n",
        "\n",
        "num_sessions: int = len(results)\n",
        "num_sessions\n",
        "\n",
        "# xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df, active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df, active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "# xdf_stream_infos_df: pd.DataFrame = XDFDataStreamAccessor.init_from_results(_out_xdf_stream_infos_df=_out_xdf_stream_infos_df[_out_xdf_stream_infos_df['name'] == 'Epoc X'], active_only_out_eeg_raws=active_only_out_eeg_raws)\n",
        "xdf_stream_infos_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations\n",
        "from phoofflineeeganalysis.analysis.SavedSessionsProcessor import LabRecorderXDF\n",
        "\n",
        "hdf5_out_path: Path = Path('E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed').joinpath('2025-09-29_all_HDF.h5').resolve()\n",
        "\n",
        "hdf5_out_path.unlink(missing_ok=True)\n",
        "\n",
        "LabRecorderXDF.to_hdf(active_only_out_eeg_raws=active_only_out_eeg_raws, results=results, xdf_stream_infos_df=xdf_stream_infos_df, file_path=hdf5_out_path, root_key='/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations, EEGData\n",
        "\n",
        "write_mode = 'r+'\n",
        "if (not hdf5_out_path.exists()):\n",
        "    write_mode = 'w'\n",
        "    \n",
        "write_mode = 'w'\n",
        "\n",
        "with h5py.File(hdf5_out_path, write_mode) as f:\n",
        "    for an_xdf_dataset_idx in np.arange(num_sessions):\n",
        "        if (an_xdf_dataset_idx < 5):\n",
        "            a_raw = active_only_out_eeg_raws[an_xdf_dataset_idx]\n",
        "            a_meas_date = a_raw.info.get('meas_date')\n",
        "            a_raw_key: str = a_meas_date.strftime(\"%Y-%m-%d/%H-%M-%S\") # '2025-09-22/21-35-47'\n",
        "            a_result = results[an_xdf_dataset_idx]\n",
        "            # EEGComputations.to_hdf(a_result=a_result, file_path=hdf5_out_path, root_key=f'/result/{a_raw_key}')\n",
        "            # EEGComputations.to_hdf(a_result=a_result, file_path=f, root_key=f'/result/{a_raw_key}')\n",
        "            EEGComputations.perform_write_to_hdf(a_result=a_result, f=f, root_key=f'/result/{a_raw_key}')\n",
        "            # a_stream_info = deepcopy(_out_xdf_stream_infos_df).loc[an_xdf_dataset_idx]    \n",
        "            # print(f'i: {i}, a_meas_date: {a_meas_date}, a_stream_info: {a_stream_info}\\n\\n')\n",
        "            # print(f'i: {an_xdf_dataset_idx}, a_meas_date: {a_meas_date}')\n",
        "            # a_df = a_raw.annotations.to_data_frame(time_format='datetime')\n",
        "            # a_df = a_df[a_df['description'] != 'BAD_motion']\n",
        "            # a_df['xdf_dataset_idx'] = an_xdf_dataset_idx\n",
        "            # flat_annotations.append(a_df)\n",
        "\n",
        "# flat_annotations = pd.concat(flat_annotations, ignore_index=True)\n",
        "# flat_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flat_annotations['onset_str'] = flat_annotations['onset'].dt.strftime(\"%Y-%m-%d_%I:%M:%S.%f %p\")\n",
        "flat_annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Out to HDF5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "from phoofflineeeganalysis.analysis.EEG_data import EEGComputations\n",
        "\n",
        "hdf5_out_path: Path = Path('E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed').joinpath('2025-09-29_all_HDF.h5').resolve()\n",
        "hdf5_out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import h5py\n",
        "\n",
        "_loaded_dict = {'xdf_stream_infos_df': None, 'flat_annotations': None, }\n",
        "\n",
        "_loaded_dict['xdf_stream_infos_df'] = pd.read_hdf(hdf5_out_path, mode='r', key='/xdf_stream_infos_df')\n",
        "_loaded_dict['flat_annotations'] = pd.read_hdf(hdf5_out_path, mode='r', key='/flat_annotations')\n",
        "_loaded_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with h5py.File(hdf5_out_path, 'r') as f:\n",
        "    # f.attrs\n",
        "    # f.name\n",
        "    # f.visititems()\n",
        "    _loaded_dict['xdf_stream_infos_df'] = pd.read_hdf(f, key='/xdf_stream_infos_df')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xdf_stream_infos_df.to_hdf(hdf5_out_path, key='/xdf_stream_infos_df', append=False)\n",
        "flat_annotations.to_hdf(hdf5_out_path, key='/flat_annotations_df', append=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for an_xdf_dataset_idx in np.arange(num_sessions):\n",
        "    a_raw = active_only_out_eeg_raws[an_xdf_dataset_idx]\n",
        "    a_meas_date: datetime = a_raw.info.get('meas_date')\n",
        "    a_result = results[an_xdf_dataset_idx]\n",
        "    # a_stream_info = deepcopy(_out_xdf_stream_infos_df).loc[an_xdf_dataset_idx]    \n",
        "    # print(f'i: {i}, a_meas_date: {a_meas_date}, a_stream_info: {a_stream_info}\\n\\n')\n",
        "    # print(f'i: {an_xdf_dataset_idx}, a_meas_date: {a_meas_date}')\n",
        "    # a_df = a_raw.to_data_frame(time_format='datetime')\n",
        "\n",
        "    a_raw_key: str = a_meas_date.strftime(\"%Y-%m-%d/%H-%M-%S\") # '2025-09-22/21-35-47'\n",
        "    a_raw.to_data_frame(time_format='datetime').to_hdf(hdf5_out_path, key=f'/raw/{a_raw_key}/df', append=True)\n",
        "    EEGComputations.to_hdf(a_result=a_result, file_path=hdf5_out_path, root_key=f'/result/{a_raw_key}')\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# a_raw_key: str = a_meas_date.strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "a_raw_key: str = a_meas_date.strftime(\"%Y-%m-%d/%H-%M-%S\") # '2025-09-22/21-35-47'\n",
        "a_raw.to_data_frame(time_format='datetime').to_hdf(hdf5_out_path, key=f'/raw/{a_raw_key}/df', append=True)\n",
        "\n",
        "EEGComputations.to_hdf(a_result=a_result, file_path=hdf5_out_path, root_key=f'/result/{a_raw_key}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "all_WHISPER_df.drop(columns=['filepath']).to_hdf(hdf5_out_path, key='modalities/WHISPER/df', append=True)\n",
        "all_pho_log_to_lsl_df.drop(columns=['filepath']).to_hdf(hdf5_out_path, key='modalities/PHO_LOG_TO_LSL/df', append=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spike3DRasterWindowWidget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.SpikeRasterWidgets.Spike2DRaster import Spike2DRaster\n",
        "# from pyphoplacecellanalysis.SpecificResults.PendingNotebookCode import _setup_spike_raster_window_for_debugging\n",
        "# from pyphoplacecellanalysis.GUI.PyQtPlot.Widgets.Mixins.Render2DScrollWindowPlot import ScatterItemData\n",
        "from pyphoplacecellanalysis.GUI.Qt.SpikeRasterWindows.Spike3DRasterWindowWidget import Spike3DRasterWindowWidget\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## It's passed a specific computation_result which has a .sess attribute that's used to determine which spikes are displayed or not.\n",
        "spike_raster_window: Spike3DRasterWindowWidget = Spike3DRasterWindowWidget(spikes_df, type_of_3d_plotter=type_of_3d_plotter, application_name=f'Spike Raster Window - {active_display_fn_identifying_ctx_string}', neuron_colors=neuron_colors, neuron_sort_order=neuron_sort_order,\n",
        "                                                                            params_kwargs=dict(use_docked_pyqtgraph_plots=use_docked_pyqtgraph_plots),\n",
        "                                                                            ) ## surprisingly only needs spikes_df !!?!\n",
        "# Set Window Title Options:\n",
        "a_file_prefix = str(computation_result.sess.filePrefix.resolve())\n",
        "spike_raster_window.setWindowFilePath(a_file_prefix)\n",
        "spike_raster_window.setWindowTitle(f'Spike Raster Window - {active_config_name} - {a_file_prefix}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Gets the existing SpikeRasterWindow or creates a new one if one doesn't already exist:\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, main_graphics_layout_widget, main_plot_widget, background_static_scroll_plot_widget) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "# spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=False, allow_replace_hardcoded_main_plots_with_tracks=True)\n",
        "spike_raster_window, (active_2d_plot, active_3d_plot, *_all_outputs_dict) = Spike3DRasterWindowWidget.find_or_create_if_needed(curr_active_pipeline, force_create_new=True, allow_replace_hardcoded_main_plots_with_tracks=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CustomCalendarWidget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime, timezone\n",
        "from phoofflineeeganalysis.UI.CustomCalendarWidget import CalendarDatasource\n",
        "from phoofflineeeganalysis.UI.CustomCalendarWidget import CustomDataDisplayingCalendar\n",
        "\n",
        "a_ds: CalendarDatasource = CalendarDatasource(xdf_stream_infos_df=xdf_stream_infos_df)\n",
        "ex = CustomDataDisplayingCalendar()\n",
        "ex.show()\n",
        "ex.set_datasource(data_source=a_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "found_df = a_ds.get_records_for_day(day_date=datetime(2025, 9, 10, tzinfo=timezone.utc))\n",
        "found_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "day_date = datetime(2025, 9, 11, tzinfo=timezone.utc)\n",
        "day_date = day_date.replace(hour=0, minute=0, second=0, microsecond=0).astimezone(tz=timezone.utc)\n",
        "day_date\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Direct Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphocorehelpers.plotting.image_plotting_helpers import IMShowHelpers\n",
        "\n",
        "\n",
        "\n",
        "def plot_matrix(xbin_edges, ybin_edges, matrix, ax, **kwargs):\n",
        "\n",
        "    def setup_stable_axes_limits(xbins_edges, ybin_edges, ax):\n",
        "        \" manually sets the axis data limits to disable autoscaling given the xbins_edges/ybin_edges \"\n",
        "        # x == horizontal orientation:\n",
        "        ax.set_xlim(left=xbins_edges[0], right=xbins_edges[-1])\n",
        "        ax.set_ylim(bottom=ybin_edges[0], top=ybin_edges[-1])\n",
        "\n",
        "\n",
        "    variable_value = matrix\n",
        "\n",
        "    xmin, xmax, ymin, ymax = (xbin_edges[0], xbin_edges[-1], ybin_edges[0], ybin_edges[-1]) # the same for both orientations\n",
        "    x_first_extent = (xmin, xmax, ymin, ymax) # traditional order of the extant axes\n",
        "    # y_first_extent = (ymin, ymax, xmin, xmax) # swapped the order of the extent axes.\n",
        "    main_plot_kwargs = {\n",
        "        'cmap': 'viridis',\n",
        "        'origin':'lower',\n",
        "        'extent':x_first_extent,\n",
        "        'aspect': 'auto',        \n",
        "    }\n",
        "\n",
        "    \"\"\"\n",
        "    Note that changing the origin while keeping everything else the same doesn't flip the direction of the yaxis labels despite flipping the yaxis of the data.\n",
        "    \"\"\"\n",
        "    im_out = ax.imshow(variable_value, **main_plot_kwargs)\n",
        "    setup_stable_axes_limits(xbin_edges, ybin_edges, ax)\n",
        "    return im_out\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(layout=\"constrained\", figsize=[9, 9], dpi=220, clear=True) # figsize=[Width, height] in inches.\n",
        "long_width_ratio = 1\n",
        "ax_dict = fig.subplot_mosaic(\n",
        "    [\n",
        "        [\"ax_dumb\", \"ax_dumb_avg\"],\n",
        "        [\"ax_good\", \"ax_good_avg\"],\n",
        "\t\t\n",
        "        # [\"ax_dumb_avg\"],\n",
        "        # [\"ax_good_avg\"],\n",
        "    ],\n",
        "    # set the height ratios between the rows\n",
        "    # set the width ratios between the columns\n",
        "    width_ratios=[10, 1],\n",
        "    sharey=True,\n",
        "    gridspec_kw=dict(wspace=0, hspace=0.0) # `wspace=0`` is responsible for sticking the pf and the activity axes together with no spacing\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import napari\n",
        "\n",
        "viewer = napari.Viewer(ndisplay=3)\n",
        "\n",
        "# viewer.add_points(mov_avg_filtered, size=2, face_color='red', name='mov_avg_filtered')\n",
        "# viewer.add_points(data2, size=2, face_color='green', name='dataset2')\n",
        "# viewer.add_points(data3, size=2, face_color='blue', name='dataset3')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "named_sessions_dict = {'dumb': -2,\n",
        "\t\t\t\t\t   'good': -6,\n",
        "}\n",
        "\n",
        "# dumb_session_idx: int = -2\n",
        "\n",
        "_out_layers = {}\n",
        "\n",
        "for a_name, an_xdf_dataset_idx in named_sessions_dict.items():\n",
        "    a_raw = active_only_out_eeg_raws[an_xdf_dataset_idx]\n",
        "\n",
        "    a_df = a_raw.annotations.to_data_frame(time_format='datetime')\n",
        "    a_df = a_df[a_df['description'] != 'BAD_motion']\n",
        "\n",
        "    a_result = results[an_xdf_dataset_idx]\n",
        "    # a_stream_info = deepcopy(_out_xdf_stream_infos_df).loc[an_xdf_dataset_idx]    \n",
        "    Sxx = a_result['spectogram']['Sxx']\n",
        "    Sxx_avg = a_result['spectogram']['Sxx_avg']\n",
        "    # t = a_result['spectogram']['t']\n",
        "    # freqs = a_result['spectogram']['freqs']\n",
        "    # fs = a_result['spectogram']['fs']\n",
        "    freqs, t, _ = a_result['spectogram']['spectogram_result_dict']['AF3']\n",
        "    \n",
        "    Sxx_avg_across_channel_avg = np.atleast_2d(Sxx_avg.mean(dim='channels', skipna=True))\n",
        "    # Sxx_avg_across_channel_avg\n",
        "    \n",
        "    Sxx_across_channel_avg = Sxx.mean(dim='channels', skipna=True)\n",
        "    # Sxx_across_channel_avg\n",
        "\n",
        "    ax_label = f\"ax_{a_name}\"\n",
        "    ax_label_avg = f\"ax_{a_name}_avg\"\n",
        "    # np.shape(Sxx_across_channel_avg)\n",
        "    \n",
        "    # xbin = deepcopy(t)\n",
        "    # ybin = deepcopy(freqs)\n",
        "    # xmin, xmax, ymin, ymax = (xbin[0], xbin[-1], ybin[0], ybin[-1])\n",
        "    # # xmin, xmax, ymin, ymax = (active_one_step_decoder.ybin[0], active_one_step_decoder.ybin[-1], active_one_step_decoder.xbin[0], active_one_step_decoder.xbin[-1]) # Reversed x and y axes, seems not good.\n",
        "    # extent = (xmin, xmax, ymin, ymax)\n",
        "    \n",
        "    # ax_dict[ax_label].imshow(Sxx_avg_across_channel_avg.T)\n",
        "    # ax_dict[ax_label_avg].imshow(Sxx_across_channel_avg, extent=extent, origin='lower')\n",
        "    \n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_vertical_plot_imshow(xbin_edges=np.arange(1), ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    # ax_dict[ax_label].autoscale(False)\n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_horizontal_plot_imshow(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    im_out = plot_matrix(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_across_channel_avg, ax=ax_dict[ax_label])\n",
        "    ax_dict[ax_label].set_ylabel(a_name)\n",
        "    # ax_dict[ax_label].autoscale(False)\n",
        "    \n",
        "\n",
        "    # fig, axs, plot_im_out = IMShowHelpers.final_x_vertical_plot_imshow(xbin_edges=t, ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label_avg])\n",
        "    avg_im_out = plot_matrix(xbin_edges=[0.0, 1.0], ybin_edges=freqs, matrix=Sxx_avg_across_channel_avg, ax=ax_dict[ax_label_avg])\n",
        "    # ax_dict[ax_label_avg].set_ylabel(f\"{a_name}_avg\")\n",
        "\n",
        "    \n",
        "    napari_img_layer_kwargs = dict(\n",
        "        # channel_axis=None,\n",
        "        # rgb=None,\n",
        "        colormap='yellow', #'bop_blue',\n",
        "        # contrast_limits=None,\n",
        "        gamma=0.02,\n",
        "        interpolation2d='nearest',\n",
        "        interpolation3d='linear',\n",
        "        rendering='mip',\n",
        "        depiction='volume',\n",
        "        # iso_threshold=None,\n",
        "        # attenuation=0.05,\n",
        "        # name=None,\n",
        "        # metadata=None,\n",
        "        # scale=None,\n",
        "        translate=None,\n",
        "        # rotate=None,\n",
        "        # shear=None,\n",
        "        # affine=None,\n",
        "        # opacity=1,\n",
        "        blending='additive',\n",
        "        visible=True,\n",
        "        # multiscale=None,\n",
        "        # cache=True,\n",
        "        # plane=None,\n",
        "        # experimental_clipping_planes=None,\n",
        "        # custom_interpolation_kernel_2d=None,\n",
        "    )\n",
        "    \n",
        "\n",
        "    curr_name = f\"{a_name}\"\n",
        "    _out_layers[curr_name] = viewer.add_image(Sxx, name=curr_name, **napari_img_layer_kwargs)\n",
        "    viewer.dims.axis_labels = Sxx.dims # ('channels', 'freqs', 'times')\n",
        "\n",
        "    curr_name = f\"{a_name}_avg\"\n",
        "    # napari_img_layer_kwargs['translate'] = [0.0, 1852.0]\n",
        "    napari_img_layer_kwargs['translate'] = [0.0, -2.0]\n",
        "    _out_avg_layer = viewer.add_image(Sxx_avg.T, name=curr_name, **napari_img_layer_kwargs) #  colormap='bop_blue', gamma=0.20, rendering='additive'\n",
        "    _out_layers[curr_name] = _out_avg_layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# _out_avg_layer.translate([512.0, 0.0])\n",
        "# _out_avg_layer.set_translation([512.0, 0.0])\n",
        "# _out_avg_layer.translate = [0.0, 513.0]\n",
        "_out_avg_layer.translate = [0.0, 1852.0]\n",
        "_out_avg_layer.translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "_out_avg_layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "viewer.dims\n",
        "viewer.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_extract_layers_info\n",
        "\n",
        "# @function_attributes(short_name=None, tags=['napari', 'config'], input_requires=[], output_provides=[], uses=[], used_by=['napari_extract_layers_info'], creation_date='2024-08-12 08:54', related_items=[])\n",
        "def extract_layer_info(a_layer):\n",
        "    \"\"\" Extracts info as a dict from a single Napari layer. \n",
        "    by default Napari layers print like: `<Shapes layer 'Shapes' at 0x1635a1e8460>`: without any properties that can be easily referenced.\n",
        "    This function extracts a dict of properties.\n",
        "\n",
        "    from pyphoplacecellanalysis.GUI.Napari.napari_helpers import extract_layer_info\n",
        "\n",
        "    \"\"\"\n",
        "    out_properties_dict = {}\n",
        "    positioning = ['scale', 'translate', 'rotate', 'shear', 'affine', 'corner_pixels']\n",
        "    visual = ['opacity', 'blending', 'visible', 'z_index', 'contrast_limits_range', '_colormap_name', 'gamma']\n",
        "    # positioning = ['scale', 'translate', 'rotate', 'shear', 'affine']\n",
        "    out_properties_dict['positioning'] = {}\n",
        "\n",
        "    for a_property_name in positioning:\n",
        "        out_properties_dict['positioning'][a_property_name] = getattr(a_layer, a_property_name)\n",
        "\n",
        "    out_properties_dict['visual'] = {}\n",
        "    for a_property_name in visual:\n",
        "        try:\n",
        "            out_properties_dict['visual'][a_property_name] = getattr(a_layer, a_property_name)\n",
        "        except (AttributeError, ValueError, KeyError, TypeError) as e:\n",
        "            print(f'failed to get property: \"{a_property_name}\" with error {e}')\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "    return out_properties_dict\n",
        "\n",
        "\n",
        "# @function_attributes(short_name=None, tags=['napari', 'config'], input_requires=[], output_provides=[], uses=['extract_layer_info'], used_by=[], creation_date='2024-08-12 08:54', related_items=[])\n",
        "def napari_extract_layers_info(layers):\n",
        "\t\"\"\"extracts info dict from each layer as well.\n",
        "\tUsage:\n",
        "        from pyphoplacecellanalysis.GUI.Napari.napari_helpers import napari_extract_layers_info\n",
        "\t\tlayers = directional_viewer.layers # [<Shapes layer 'Shapes' at 0x1635a1e8460>, <Shapes layer 'Shapes [1]' at 0x164d5402e50>]\n",
        "\t\tout_layers_info_dict = debug_print_layers_info(layers)\n",
        "\n",
        "\t\"\"\"\n",
        "\tout_layers_info_dict = {}\n",
        "\tfor a_layer in layers:\n",
        "\t\ta_name: str = str(a_layer.name)\n",
        "\t\tout_properties_dict = extract_layer_info(a_layer)\n",
        "\t\tout_layers_info_dict[a_name] = out_properties_dict\n",
        "\t\t# if isinstance(a_layer, Shapes):\n",
        "\t\t# \tprint(f'shapes layer: {a_layer}')\n",
        "\t\t# \ta_shapes_layer: Shapes = a_layer\n",
        "\t\t# \t# print(f'a_shapes_layer.properties: {a_shapes_layer.properties}')\n",
        "\t\t# \tout_properties_dict = extract_layer_info(a_layer)\n",
        "\t\t# \tprint(f'\\tout_properties_dict: {out_properties_dict}')\n",
        "\t\t# \tout_layers_info_dict[a_name] = out_properties_dict\n",
        "\t\t# else:\n",
        "\t\t# \tprint(f'unknown layer: {a_layer}')\t\n",
        "\treturn out_layers_info_dict\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "layers = viewer.layers # [<Shapes layer 'Shapes' at 0x1635a1e8460>, <Shapes layer 'Shapes [1]' at 0x164d5402e50>]\n",
        "out_layers_info_dict = napari_extract_layers_info(layers)\n",
        "out_layers_info_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out_layers_info_dict['dumb']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from napari.layers.image.image import Image\n",
        "\n",
        "property_names = ['metadata', 'blending', 'opacity', 'rendering', 'scale', 'gamma', 'contrast_limits_range', 'colormap']\n",
        "for a_layer in layers:\n",
        "    an_img_layer: Image = a_layer\n",
        "    \n",
        "#    type(a_layer)\n",
        "\n",
        "# an_img_layer.blending\n",
        "an_img_layer.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "napari.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xbin\n",
        "ybin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_stream_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "\n",
        "## INPUTS: a_result\n",
        "a_spectogram_result: Dict = a_result['spectogram'] \n",
        "\n",
        "ch_names = a_spectogram_result['ch_names']\n",
        "fs = a_spectogram_result['fs']\n",
        "a_spectogram_result_dict = a_spectogram_result['spectogram_result_dict'] # Dict[channel: Tuple]\n",
        "Sxx = a_spectogram_result['Sxx']\n",
        "Sxx_avg = a_spectogram_result['Sxx_avg']\n",
        "\n",
        "Sxx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Sxx_avg_list = [] \n",
        "\n",
        "# ch_names = a_raw.info.ch_names\n",
        "\n",
        "for a_ch, a_tuple in a_spectogram_result_dict.items():\n",
        "    f, t, Sxx = a_tuple ## unpack the tuple\n",
        "    # np.shape(Sxx) # (513, 1116) - (n_freqs, n_times)\n",
        "    n_freqs = np.shape(f)\n",
        "    n_times = np.shape(t) \n",
        "    Sxx_avg = np.nanmean(Sxx, axis=-1) ## average over all time to get one per session\n",
        "    Sxx_avg_list.append(Sxx_avg)\n",
        "    \n",
        "Sxx_avg_list = np.stack(Sxx_avg_list) # (14, 513) - (n_channels, n_freqs)\n",
        "Sxx_avg_list = xr.DataArray(Sxx_avg_list, dims=(\"channels\", \"freqs\"), coords={\"channels\": ch_names, \"freqs\": f})\n",
        "np.shape(Sxx_avg_list)\n",
        "Sxx_avg_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ch_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a_raw.annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Sxx_avg = np.nanmean(Sxx, axis=-1) ## average over all time to get one per session\n",
        "Sxx_avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.close('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_all_spectograms(active_only_out_eeg_raws, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams[\"axes.titlesize\"] = 8\n",
        "plt.rcParams[\"axes.labelsize\"] = 8\n",
        "plt.rcParams[\"xtick.labelsize\"] = 6\n",
        "plt.rcParams[\"ytick.labelsize\"] = 6\n",
        "plt.rcParams[\"legend.fontsize\"] = 6\n",
        "plt.rcParams[\"figure.titlesize\"] = 8\n",
        "plt.rcParams[\"axes.titlepad\"] = 0\n",
        "plt.rcParams[\"figure.constrained_layout.use\"] = True\n",
        "plt.rcParams[\"figure.constrained_layout.h_pad\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.w_pad\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.hspace\"] = 0.0\n",
        "plt.rcParams[\"figure.constrained_layout.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.hspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.wspace\"] = 0.0\n",
        "plt.rcParams[\"figure.subplot.hspace\"] = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Plot a synchronized EEG Raw data and Spectogram Figure together:\n",
        "active_eeg_idx: int = -4\n",
        "mne_raw_fig = active_only_out_eeg_raws[active_eeg_idx].plot(time_format='datetime', scalings='auto') # MNEBrowseFigure\n",
        "fig, axs = plot_session_spectogram(active_only_out_eeg_raws[active_eeg_idx], results[active_eeg_idx], sync_to_mne_raw_fig=mne_raw_fig)\n",
        "# plt.subplots_adjust(wspace=0, hspace=0)  # remove spacing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "active_eeg_idx: int = -6\n",
        "mne_raw_fig2 = active_only_out_eeg_raws[active_eeg_idx].plot(time_format='datetime', scalings='auto') # MNEBrowseFigure\n",
        "fig2, axs2 = plot_session_spectogram(active_only_out_eeg_raws[active_eeg_idx], results[active_eeg_idx], sync_to_mne_raw_fig=mne_raw_fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute and show all spectograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute results first\n",
        "active_only_out_eeg_raws, results = batch_compute_all_eeg_datasets(eeg_raws=_out_eeg_raw, limit_num_items=3)\n",
        "\n",
        "# Render to PDFs (paged)\n",
        "from pathlib import Path\n",
        "out_paths = render_all_spectograms_to_high_quality_pdfs(\n",
        "    active_only_out_eeg_raws,\n",
        "    results,\n",
        "    output_parent_folder=Path(r\"E:/Dropbox (Personal)/Databases/AnalysisData/MNE_preprocessed/exports\"),\n",
        "    mode=\"paged\",\n",
        "    seconds_per_page=180.0,\n",
        "    freq_min_hz=1.0,\n",
        "    freq_max_hz=40.0,\n",
        "    dpi=300\n",
        ")\n",
        "print(f\"Wrote {len(out_paths)} PDF(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANalysis for Fatigue/Bandpowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.analysis.computations.fatigue_analysis import compare_multiple_recordings, compute_fatigue_metrics, analyze_fatigue_trends, print_analysis_report, visualize_fatigue_comparison\n",
        "\n",
        "raw_objects = deepcopy(_out_eeg_raw)\n",
        "raw_obj_labels = [str(a_raw) for a_raw in raw_objects]\n",
        "if len(raw_objects) >= 2:\n",
        "    results = compare_multiple_recordings(raw_objects, raw_obj_labels[:len(raw_objects)])\n",
        "    print_analysis_report(results)\n",
        "    visualize_fatigue_comparison(results)\n",
        "\n",
        "    results\n",
        "else:\n",
        "    print(\"Not enough data files found for comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sliding wavlet analyses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyqtgraph as pg\n",
        "from phoofflineeeganalysis.timeflux.nodes.wavelet_cwt import EEGViewer\n",
        "\n",
        "app = pg.mkQApp('EEGWaveletViewer')\n",
        "\n",
        "raw = _out_eeg_raw[-1]\n",
        "\n",
        "# Launch the Qt Application\n",
        "viewer = EEGViewer(raw_data=raw)\n",
        "viewer.show()\n",
        "# sys.exit(app.exec_())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fixed_len_epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from phoofflineeeganalysis.timeflux.nodes.wavelet_cwt import plot_raw_with_cwt\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "# First, create a sample 14-channel Raw object for demonstration\n",
        "# sfreq = 250\n",
        "# ch_names = [f'EEG {i+1:02d}' for i in range(14)]\n",
        "# info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='eeg')\n",
        "# n_seconds = 120\n",
        "# data = np.random.randn(len(ch_names), sfreq * n_seconds)\n",
        "# raw = mne.io.RawArray(data, info)\n",
        "\n",
        "raw = _out_eeg_raw[-1]\n",
        "\n",
        "# Now, use the function to plot the first channel (index 0) from 10 to 20 seconds\n",
        "plot_raw_with_cwt(raw, start_seconds=10.0, end_seconds=20.0, channel_index=0)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
