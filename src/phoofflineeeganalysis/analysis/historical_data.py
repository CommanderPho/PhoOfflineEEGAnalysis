import time
import re
from datetime import datetime, timezone

import uuid
from copy import deepcopy
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from nptyping import NDArray
from matplotlib import pyplot as plt

from pathlib import Path
import numpy as np
import pandas as pd

import mne
from mne import set_log_level
from copy import deepcopy
import mne

from mne.io import read_raw
import pyedflib

from phoofflineeeganalysis.analysis.motion_data import MotionData ## for creating single EDF+ files containing channel with different sampling rates (e.g. EEG and MOTION data)

mne.viz.set_browser_backend("Matplotlib")

from mne_lsl.player import PlayerLSL as Player
from mne_lsl.stream import StreamLSL as Stream

# from phoofflineeeganalysis.EegProcessing import bandpower
from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers
# from ..EegProcessing import bandpower
from numpy.typing import NDArray
# from nptyping import NDArray

set_log_level("WARNING")

## Decode the GENERIC with `from emotiv-lsl.emotiv_lsl.emotiv_epoc_x import EmotivEpocX`
import os, sys
# os.environ["PYTHONPATH"] = "C:/Users/pho/repos/EmotivEpoc/emotiv-lsl:" + os.environ.get("PYTHONPATH", "")
sys.path.append("C:/Users/pho/repos/EmotivEpoc/emotiv-lsl")

from phoofflineeeganalysis.helpers.indexing_helpers import reorder_columns_relative
from emotiv_lsl.emotiv_epoc_x import EmotivBase, EmotivEpocX



class GenericRawDebugFileProcessor:
    """ processes files generated by Flutter after 2025-09-10 which contain the raw (non-decoded/decrypted) data spat out from the BLE Flutter app.
    
    #TODO 2025-09-11 23:08: - [ ] EEFICIENCY MAJOR - hold an instance to `emotiv_epoc_x` instead of making fresh each time we want to decode (each file)

    CAPTURES: emotiv_epoc_x

    from phoofflineeeganalysis.analysis.historical_data import GenericRawDebugFileProcessor

    """
    # line_length_to_modality_dict = {65: 'MOTION', 85: 'EEG'}
    modality_to_line_length_dict = {'MOTION':65, 'EEG':85}    
    modality_to_num_valid_columns_dict = {'MOTION': 7, 'EEG': 31}
    emotiv_epoc_x = None
    
    @classmethod
    def decode_raw_generic_eeg_packets(cls, eeg_line_data):

        def _subfn_clean_packet(pkt: bytes) -> bytes:
            # Drop trailing 0xFFs
            pkt = pkt.rstrip(b'\xff')
            # Keep only full 16-byte blocks
            valid_len = (len(pkt) // 16) * 16
            return pkt[:valid_len]


        # skip timestamp (row[0])
        timestamps: NDArray = np.array([int(row[0].strip('\n')) for row in eeg_line_data])
        raw_bytes_list = [bytes(int(x.strip('\n')) for x in row[1:]) for row in eeg_line_data]

        ## len should be the same len(timestamps) == len(raw_bytes_list)


        raw_bytes_list_length = np.array([len(v) for v in raw_bytes_list])
        raw_bytes_list_length

        cleaned_raw_bytes_list = [_subfn_clean_packet(p) for p in raw_bytes_list]
        cleaned_raw_bytes_list_length = np.array([len(v) for v in cleaned_raw_bytes_list])
        cleaned_raw_bytes_list_length
        # OUTPUTS: timestamps, cleaned_raw_bytes_list

        # Clean packet using `emotiv_lsl`'s `emotiv_epoc_x.EmotivEpocX` class: _______________________________________________________________________________________________________________________________________________________________________________________________________________ #
        if cls.emotiv_epoc_x is None:
            print(f"Performing one-time init of `cls.emotiv_epoc_x`...")
            crypto_key = bytearray(b'6566565666756557') # emotiv_epoc_x.get_crypto_key()
            print(f'crypto_key: {crypto_key}')
            cls.emotiv_epoc_x = EmotivEpocX.init_with_serial(serial_number=None, cryptokey=crypto_key, enable_electrode_quality_stream=False)
            cls.emotiv_epoc_x.enable_electrode_quality_stream = False
            

        # decoded_packets_list = [emotiv_epoc_x.decode_data(a_packet) for a_packet in raw_bytes_list]
        decoded_packets_list = [np.array(cls.emotiv_epoc_x.decode_data(a_packet)[0]) for a_packet in cleaned_raw_bytes_list] ## only extract the packet data (packet_data, eeg_quality_data)

        # Loaded variable 'decoded_packets_list' from kernel state
        decoded_packets_list_df: pd.DataFrame = pd.DataFrame(decoded_packets_list)
        decoded_packets_list_df['timestamp'] = timestamps
        decoded_packets_list_df = decoded_packets_list_df.drop_duplicates() # Drop duplicate rows across all columns (importantly including 'timestamp')
        decoded_packets_list_df = decoded_packets_list_df.sort_values(['timestamp']) # Sort by column: 'timestamp' (ascending)
        decoded_packets_list_df

        ## Find rows with any missing elmeents:
        df_is_na = decoded_packets_list_df.isna()
        # num_missing_elements = df_is_na.sum(axis='columns')
        num_valid_columns = np.logical_not(df_is_na).sum(axis='columns')

        
        num_valid_columns.value_counts()

        valid_eeg_indicies = (num_valid_columns == cls.modality_to_num_valid_columns_dict['EEG'])
        # valid_eeg_indicies
        ## Just the EEG rows with the right number of columns:
        valid_eeg_decoded_df: pd.DataFrame = decoded_packets_list_df[valid_eeg_indicies]
        valid_eeg_decoded_df


        ## valid_eeg_decoded_df - comes in with 30 columns (way more than the num of EEG channels, so we need to get only the EEG data (hope it's right):
        ## INPUTS: valid_eeg_decoded_df, emotiv_epoc_x
        # channel_names = deepcopy(cls.emotiv_epoc_x.eeg_channel_names)
        channel_names = deepcopy(HistoricalData.modality_channels_dict['EEG'])
        
        # eeg_channel_names
        # len(eeg_channel_names)
        # final_df_col_names = ['timestamp'] + eeg_channel_names
        final_eeg_decoded_df: pd.DataFrame = valid_eeg_decoded_df.iloc[:, :len(channel_names)] ## get the first data columns
        final_eeg_decoded_df.columns = channel_names ## set the EEG data column names
        final_eeg_decoded_df['timestamp'] = deepcopy(valid_eeg_decoded_df['timestamp']) # copy the timestamp column 
        _remaineder_decoded_df: pd.DataFrame = valid_eeg_decoded_df.iloc[:, len(channel_names):-2]
        _remaineder_decoded_df['timestamp'] = deepcopy(valid_eeg_decoded_df['timestamp']) # copy the timestamp column to the _remaineder_decoded_df too, just in case

        ## Move timestamp column to the end:
        final_eeg_decoded_df = reorder_columns_relative(final_eeg_decoded_df, column_names={'timestamp': 0}, relative_mode='start')
        _remaineder_decoded_df = reorder_columns_relative(_remaineder_decoded_df, column_names={'timestamp': 0}, relative_mode='start')

        ## OUTPUTS: final_eeg_decoded_df, _remaineder_decoded_df
        return (final_eeg_decoded_df, _remaineder_decoded_df)

    @classmethod
    def decode_raw_generic_MOTION_packets(cls, line_data):
        """ 2025-09-12 - Not throuroughly tested. Is decryption even needed for motion data? Or just decoding?
        """

        modality_name: str = 'MOTION'
        
        def _subfn_clean_packet(pkt: bytes) -> bytes:
            # Drop trailing 0xFFs
            pkt = pkt.rstrip(b'\xff')
            # Keep only full 16-byte blocks
            valid_len = (len(pkt) // 16) * 16
            return pkt[:valid_len]


        # skip timestamp (row[0])
        timestamps: NDArray = np.array([int(row[0].strip('\n')) for row in line_data])
        line_data: List[NDArray] = [np.array([int(x.strip('\n')) for x in row[1:]]) for row in line_data] ## lists of integers
        
        
        # raw_bytes_list = [bytes(int(x.strip('\n')) for x in row[1:]) for row in line_data]
        raw_bytes_list = [bytes(row) for row in line_data] ## arrays to bytes

        # _debug_out_file_path = Path(r'C:\Users\pho\repos\EmotivEpoc\PhoLabStreamingReceiver\data\raw_bytes_list.txt').resolve()
        # with open(_debug_out_file_path, "w") as f:
        #     f.writelines([str(v) for v in raw_bytes_list])
        #     # f.write(str(raw_bytes_list))
            
        # ## len should be the same len(timestamps) == len(raw_bytes_list)


        raw_bytes_list_length = np.array([len(v) for v in raw_bytes_list])
        
        cleaned_raw_bytes_list = [_subfn_clean_packet(p) for p in raw_bytes_list]
        cleaned_raw_bytes_list_length = np.array([len(v) for v in cleaned_raw_bytes_list])
        cleaned_raw_bytes_list_length
        # OUTPUTS: timestamps, cleaned_raw_bytes_list

        # Clean packet using `emotiv_lsl`'s `emotiv_epoc_x.EmotivEpocX` class: _______________________________________________________________________________________________________________________________________________________________________________________________________________ #
        if cls.emotiv_epoc_x is None:
            print(f"Performing one-time init of `cls.emotiv_epoc_x`...")
            crypto_key = bytearray(b'6566565666756557') # emotiv_epoc_x.get_crypto_key()
            print(f'crypto_key: {crypto_key}')
            cls.emotiv_epoc_x = EmotivEpocX.init_with_serial(serial_number=None, cryptokey=crypto_key, enable_electrode_quality_stream=False)
            cls.emotiv_epoc_x.enable_electrode_quality_stream = False


        # decoded_packets_list = [emotiv_epoc_x.decode_data(a_packet) for a_packet in raw_bytes_list]
        decoded_packets_list = [np.array(cls.emotiv_epoc_x.decode_data(a_packet)[0]) for a_packet in cleaned_raw_bytes_list] ## .decode_data(...) IS CORRECT (not .decode_motion_data(...) 
        
        # Loaded variable 'decoded_packets_list' from kernel state
        decoded_packets_list_df: pd.DataFrame = pd.DataFrame(decoded_packets_list)
        decoded_packets_list_df['timestamp'] = timestamps
        decoded_packets_list_df = decoded_packets_list_df.drop_duplicates() # Drop duplicate rows across all columns (importantly including 'timestamp')
        decoded_packets_list_df = decoded_packets_list_df.sort_values(['timestamp']) # Sort by column: 'timestamp' (ascending)
        decoded_packets_list_df

        ## Find rows with any missing elmeents:
        df_is_na = decoded_packets_list_df.isna()
        # num_missing_elements = df_is_na.sum(axis='columns')
        num_valid_columns = np.logical_not(df_is_na).sum(axis='columns')
        # num_valid_columns.value_counts()

        valid_eeg_indicies = (num_valid_columns == cls.modality_to_num_valid_columns_dict[modality_name])
        # valid_eeg_indicies
        ## Just the EEG rows with the right number of columns:
        valid_decoded_df: pd.DataFrame = decoded_packets_list_df[valid_eeg_indicies]


        ## valid_eeg_decoded_df - comes in with 30 columns (way more than the num of EEG channels, so we need to get only the EEG data (hope it's right):
        ## INPUTS: valid_eeg_decoded_df, emotiv_epoc_x
                
        channel_names = deepcopy(HistoricalData.modality_channels_dict[modality_name])
        # eeg_channel_names
        # len(eeg_channel_names)
        # final_df_col_names = ['timestamp'] + eeg_channel_names
        final_motion_decoded_df: pd.DataFrame = valid_decoded_df.iloc[:, :len(channel_names)] ## get the first data columns
        final_motion_decoded_df.columns = channel_names ## set the EEG data column names
        final_motion_decoded_df['timestamp'] = deepcopy(valid_decoded_df['timestamp']) # copy the timestamp column 
        _remaineder_decoded_df: pd.DataFrame = valid_decoded_df.iloc[:, len(channel_names):-2]
        _remaineder_decoded_df['timestamp'] = deepcopy(valid_decoded_df['timestamp']) # copy the timestamp column to the _remaineder_decoded_df too, just in case

        ## Move timestamp column to the end:
        final_motion_decoded_df = reorder_columns_relative(final_motion_decoded_df, column_names={'timestamp': 0}, relative_mode='start')
        _remaineder_decoded_df = reorder_columns_relative(_remaineder_decoded_df, column_names={'timestamp': 0}, relative_mode='start')

        ## OUTPUTS: final_eeg_decoded_df, _remaineder_decoded_df
        return (final_motion_decoded_df, _remaineder_decoded_df)




    @classmethod
    def main_perform_decode(cls, a_recording_file: Path):
        """ #TODO 2025-09-11 22:51: - [ ] this was converted from an intermediate output data format, so parsing to data packet can be made direct

            final_eeg_decoded_df, _remaineder_decoded_df = GenericRawDebugFileProcessor.main_perform_decode(a_recording_file=a_recording_file)
        """
        print(f'GENERIC FILE PROCESSING')
        ## GENERIC TYPE:
        ## Process line-by-line to handle the variable line lengths
        line_parts_by_length: Dict[int, List[str]] = {}
        # _subfn_load_eeg_bytes(a_recording_file)

        with open(a_recording_file, 'r', newline='\n') as f:
            lines = f.readlines()
            for line in lines:
                line = line.strip('\n').strip('\r')
                line_parts = line.split(',')
                num_entries = len(line_parts)
                if num_entries not in line_parts_by_length:
                    line_parts_by_length[num_entries] = []
                line_parts_by_length[num_entries].append(line_parts)


        ## END with open ...
        # full_data = np.concatenate(all_data, axis=1)  # concatenate over time
        # full_times = np.concatenate(all_times)
        # num_records_per_length = {k:len(v) for k, v in line_parts_by_length.items()}        
        modality_to_processing_fn_dict = {'MOTION': cls.decode_raw_generic_MOTION_packets, 'EEG': cls.decode_raw_generic_eeg_packets}
        modality_out_linedata_df_dict = {'MOTION': None, 'EEG': None}   
        modality_out_dfs_dict = {'MOTION': None, 'EEG': None}
        for a_modality, a_length in cls.modality_to_line_length_dict.items():
            modality_line_data: List = line_parts_by_length.get(a_length, [])
            # modality_out_linedata_dict[a_modality] = modality_line_data
            modality_out_linedata_df_dict[a_modality] = pd.DataFrame(modality_line_data)
            modality_out_dfs_dict[a_modality] = modality_to_processing_fn_dict[a_modality](modality_line_data) ## uses `cls.decode_raw_generic_eeg_packets(...)`

        ## UNPACK LIKE:
        # decoded_df, _remaineder_decoded_df = modality_out_dfs_dict['MOTION']
        
        # eeg_line_data: List = line_parts_by_length.get(cls.modality_to_line_length_dict['EEG'], [])
        ## OUTPUTS: eeg_line_data
        # final_eeg_decoded_df, _remaineder_decoded_df = cls.decode_raw_generic_eeg_packets(eeg_line_data=eeg_line_data) ## uses `cls.decode_raw_generic_eeg_packets(...)`

        # return final_eeg_decoded_df, _remaineder_decoded_df
        return modality_out_dfs_dict, modality_out_linedata_df_dict


class FlutterExportedData:
    """ Uses `GenericRawDebugFileProcessor` as one of the three .csv file formats
    
    """
    @classmethod
    def try_load_flutter_recordings(cls, 
                                    flutter_eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings').resolve(),
                                    flutter_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/MOTION_RECORDINGS').resolve(),
                                    flutter_GENERIC_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/GENERIC_RECORDINGS').resolve(),
                                    should_load_data = True,
                                    should_process: bool=False,
                                    n_most_recent_sessions_to_preprocess: int = 5,
                                    ):
        # filenames are like: "eeg_data_2025-08-12T02-56-32.509841.csv" for EEG and "motion_data_2025-08-12T02-56-32.537110.csv" for motion All files are in .csv format
        # "timestamp	AF3	F7	F3	FC5	T7	P7	O1	O2	P8	T8	FC6	F4	F8	AF4"

        found_recording_file_modality_dict = {} ## for finding input files
        flat_data_modality_dict = {} ## For collecting outputs 

        if flutter_eeg_recordings_file_path is not None:
            assert flutter_eeg_recordings_file_path.exists()
            found_flutter_EEG_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_eeg_recordings_file_path, recordings_extensions=[".csv"])
            found_recording_file_modality_dict.update({'EEG': found_flutter_EEG_recording_files})

        # assert flutter_motion_recordings_file_path.exists()
        # found_MOTION_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_motion_recordings_file_path, recordings_extensions=[".csv"])
        # found_recording_file_modality_dict.update({'MOTION': found_MOTION_recording_files})

        if flutter_GENERIC_recordings_file_path is not None:
            assert flutter_GENERIC_recordings_file_path.exists()
            found_GENERIC_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_GENERIC_recordings_file_path, recordings_extensions=[".csv"])
            found_recording_file_modality_dict.update({'GENERIC': found_GENERIC_recording_files})


        # a_recording_file = found_flutter_EEG_recording_files[-1]
        # raw_df: pd.DataFrame = pd.read_csv(a_recording_file, low_memory=False, skiprows=0, header=0, parse_dates=['timestamp'])
        # # ch_names = raw_df.columns.to_list()[1:]
        # ch_names = raw_df.columns.to_list() ## Includes timestamp
        # sfreq = modality_sfreq_dict['EEG']
        # info = mne.create_info(ch_names = ch_names, sfreq = sfreq)
        # raw = mne.io.RawArray(raw_df.T, info)
        # raw
        # raw_df.columns
        if should_process:
            ## Load and process the found recording file types:
            for k, a_found_recording_files in found_recording_file_modality_dict.items():
                constrain_channels = HistoricalData.modality_channels_dict.get(k, None)
                # constrain_channels = None
                # flat_data_modality_dict[k] = HistoricalData.read_exported_csv_files(found_recording_files=a_found_recording_files, constrain_channels=constrain_channels, file_type=k, should_load_data=should_load_data,
                #                                                                      debug_n_max_files_to_load=n_most_recent_sessions_to_preprocess)

                flat_data_modality_dict[k] = HistoricalData.read_exported_csv_files(found_recording_files=a_found_recording_files, constrain_channels=constrain_channels, file_type=k, should_load_data=should_load_data,
                                                                                    debug_n_max_files_to_load=n_most_recent_sessions_to_preprocess)

                # flat_data_modality_dict[k] = modality_out_dfs_dict
                # (all_data, all_times), datasets, df = flat_data_modality_dict[k] ## Unpacking
                # modality_datasets, modality_df = flat_data_modality_dict[k] ## Unpacking

        return flat_data_modality_dict, found_recording_file_modality_dict






class HistoricalData:
    """ Methods related to retrospective processing of recorded data
        
    from phoofflineeeganalysis.analysis.historical_data import HistoricalData
        
    """
    modality_channels_dict = {'EEG': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'],
                            'MOTION': ['AccX', 'AccY', 'AccZ', 'GyroX', 'GyroY', 'GyroZ'],
                            'GENERIC': ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'],
    }
    
    modality_sfreq_dict = {'EEG': 128, 'MOTION': 16,
                           'GENERIC': 128, 
    }


    # modality_to_num_valid_columns_dict = {'MOTION': 7, 'EEG': 31}
    

    @classmethod
    def get_recording_files(cls, recordings_dir: Path, recordings_extensions = ['.fif']):
        found_recording_files = []
        for ext in recordings_extensions:
            found_recording_files.extend(recordings_dir.glob(f"*{ext}"))
            # found_recording_files.extend(recordings_dir.glob(f"*{ext.upper()}"))
        try:
            found_recording_files.sort(key=lambda f: (-(f.stat().st_mtime), f.name.lower()[::-1]))
        except Exception as e:
            raise e
        
        return found_recording_files


    @classmethod
    def extract_datetime_from_filename(cls, filename) -> datetime:
        """Extract recording start datetime from filename by searching for any valid datetime substring.
        Examples: 
            '20250730-195857-Epoc X Motion-raw.fif',
            '20250730-200710-Epoc X-raw.fif',
            '20250618-185519-Epoc X-raw.fif',
            '20250618-185519-Epoc X-raw.fif', # '20250618-185519'
            'eeg_data_2025-08-12T02-56-32.509841.csv',
        """
        candidates = re.findall(r'\d{4}[-_]?\d{2}[-_]?\d{2}[ T_-]?\d{2}[:\-]?\d{2}[:\-]?\d{2}', filename)
        for cand in candidates:
            # normalized = cand.replace("_", "-").replace(" ", "T").replace("-", "T")
            normalized = cand.replace("_", "T").replace(" ", "T") # .replace("-", "T")
            for fmt in [
                "%Y-%m-%dT%H-%M-%S",
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%dT%H%M%S",
                "%Y%m%dT%H%M%S",
                "%Y%m%d_%H%M%S",
                "%Y%m%d-%H%M%S",
                "%Y%m%d%H%M%S"
            ]:
                try:
                    return datetime.strptime(normalized, fmt)
                except ValueError:
                    continue
        raise ValueError(f"Filename '{filename}' does not contain a recognized datetime.")




    @classmethod
    def get_or_parse_datetime_from_raw(cls, raw, override_filepath: Optional[Path]=None, allow_setting_meas_date_from_filename:bool=True) -> datetime:
        """ Get the recording start datetime from the raw.info['meas_date'] or parse from filename if not present
        if allow_setting_meas_date_from_filename is True, it will set the raw.info['meas_date'] if it was None
        """        
        metadata_recording_start_datetime = raw.info.get('meas_date', None)
        if metadata_recording_start_datetime is None:
            if override_filepath is None:
                override_filepath = Path(raw.filenames[0])
            parsed_recording_start_datetime = cls.extract_datetime_from_filename(override_filepath.name)
            metadata_recording_start_datetime = parsed_recording_start_datetime
            if allow_setting_meas_date_from_filename:                
                # Make timezone-aware (UTC)
                # dt = dt.rerawe(tzinfo=timezone.utc)
                raw.info.set_meas_date(deepcopy(parsed_recording_start_datetime).replace(tzinfo=timezone.utc))
                ## WAS UPDATED, probably need to re-save or something
                meas_datetime = raw.info['meas_date']
            else:
                ## don't set it, but still use the parsed datetime
                meas_datetime = deepcopy(parsed_recording_start_datetime).replace(tzinfo=timezone.utc) # raw.info['meas_date']  # This is an absolute datetime or tuple (timestamp, 0)

        else:                    
            meas_datetime = raw.info['meas_date']  # This is an absolute datetime or tuple (timestamp, 0)
            
        return meas_datetime
    
    
    @classmethod
    def MAIN_process_recording_files(cls, eeg_recordings_file_path: Optional[Path] = None, headset_motion_recordings_file_path: Optional[Path] = None, WhisperVideoTranscripts_LSL_Converted: Optional[Path]=None, pho_log_to_LSL_recordings_path: Optional[Path]=None, should_load_data: bool=False):
                  
        """        
            eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()
            headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()
            WhisperVideoTranscripts_LSL_Converted = Path(r"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted").resolve()
            
            flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(eeg_recordings_file_path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve(),
                            headset_motion_recordings_file_path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve(),
                            WhisperVideoTranscripts_LSL_Converted = Path(r"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted").resolve(),
            )
            flat_data_modality_dict
        """
        found_recording_file_modality_dict = {} ## for finding input files

        if eeg_recordings_file_path is not None:
            assert eeg_recordings_file_path.exists()
            found_EEG_recording_files = cls.get_recording_files(recordings_dir=eeg_recordings_file_path)
            found_recording_file_modality_dict.update({'EEG': found_EEG_recording_files})
        
        if headset_motion_recordings_file_path is not None:
            assert headset_motion_recordings_file_path.exists()
            found_MOTION_recording_files = cls.get_recording_files(recordings_dir=headset_motion_recordings_file_path)
            found_recording_file_modality_dict.update({'MOTION': found_MOTION_recording_files})
            
        if WhisperVideoTranscripts_LSL_Converted is not None:
            assert WhisperVideoTranscripts_LSL_Converted.exists()
            found_WHISPER_recording_files = cls.get_recording_files(recordings_dir=WhisperVideoTranscripts_LSL_Converted)
            found_recording_file_modality_dict.update({'WHISPER': found_WHISPER_recording_files})

        if pho_log_to_LSL_recordings_path is not None:
            assert pho_log_to_LSL_recordings_path.exists()
            found_PHO_LOG_TO_LSL_recordings_files = cls.get_recording_files(recordings_dir=pho_log_to_LSL_recordings_path)
            found_recording_file_modality_dict.update({'PHO_LOG_TO_LSL': found_PHO_LOG_TO_LSL_recordings_files})

        ## Load and process the found recording file types:
        flat_data_modality_dict = cls.read_recording_files(found_recording_file_modality_dict=found_recording_file_modality_dict, should_load_data=should_load_data)

        return flat_data_modality_dict, found_recording_file_modality_dict


    @classmethod
    def read_exported_fif_files(cls, found_recording_files: List[Path], allow_setting_meas_date_from_filename: bool=True, constrain_channels: Optional[List[str]]=None, include_src_file_column: bool=True, file_type: str='MISC', should_load_data: bool=False):
        """
        # found_recording_files = deepcopy(found_EEG_recording_files)
                found_recording_files = deepcopy(found_MOTION_recording_files)
          HistoricalData.read_exported_fif_files(found_recording_files=found_recording_files)

        """
        all_data = []
        all_times = []
        datasets = []
        n_records = []
        src_file = []
        expected_channels = cls.modality_channels_dict.get(file_type, None)
        
        for a_recording_file in found_recording_files:
            assert a_recording_file.exists()
            try:            
                raw = read_raw(a_recording_file, preload=False)
                meas_datetime = cls.get_or_parse_datetime_from_raw(raw, allow_setting_meas_date_from_filename=allow_setting_meas_date_from_filename)
                if should_load_data:
                    # raw.info.ch_names
                    # raw.to_data_frame()
                    data, times = raw.get_data(picks=constrain_channels, return_times=True)

                    # Convert relative times to absolute timestamps (in seconds since epoch)
                    start_time = meas_datetime.timestamp() if hasattr(meas_datetime, 'timestamp') else meas_datetime[0]
                    abs_times = start_time + times
                    a_n_records: int = len(times) ## n_records per src file
                    all_data.append(data)
                    all_times.append(abs_times)
                    n_records.append(a_n_records) ## n_records per src file
                    if include_src_file_column:
                        src_file.extend(([a_recording_file.stem] * a_n_records))            
                # END if should_load_data
                ## Append to datasets
                datasets.append(raw)
                                    
            except (ValueError, AttributeError, TypeError) as e:
                print(f'Encountered error: {e} while trying to read CSV file {a_recording_file}. Skipping')
                # datasets.append(None)
                pass
            except Exception as e:
                raise
        
        ## END `for a_recording_file in found_recording_files`
        
        if should_load_data:
            all_data_shapes = [np.shape(v) for v in all_data]
            all_data_ch_names = [raw.info.ch_names for raw in datasets]
            full_data = np.concatenate(all_data, axis=1)  # concatenate over time
            full_times = np.concatenate(all_times)

            if constrain_channels is None:
                constrain_channels = all_data_ch_names[0] ## take the first list of channel names (all of them) as the channels for the data. They better be the same!
            ## Hopefully same size as number of channels!
            df = pd.DataFrame(full_data.T, columns=constrain_channels)
            df['timestamp'] = full_times
            df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s') ## add datetime column
            if include_src_file_column:
                assert len(src_file) == len(df), f"len(src_file): {len(src_file)}, len(df): {len(df)}"
                df['src_file'] = src_file
            # Sort by column: 'timestamp_dt' (ascending)
            df = df.sort_values(['timestamp_dt'], na_position='first').reset_index(drop=True)
            
            if file_type == 'MOTION':
                ## Estimate Quaternions from motion data:
                df = MotionData.compute_quaternions(df)
            elif file_type == 'EEG':
                pass
            else:
                pass            

            # df.to_csv('eeg_concatenated.csv', index=False)
        else:
            df = None
        
        return (all_data, all_times), datasets, df
    

    @classmethod
    def read_exported_csv_files(cls, found_recording_files: List[Path], allow_setting_meas_date_from_filename: bool=True, constrain_channels: Optional[List[str]]=None, include_src_file_column: bool=True, file_type: str='MISC',
             should_load_data: bool=False, debug_n_max_files_to_load: int = 5):
        """ Reads .csv exported by the Flutter BLE iOS/Android app in the ~2025-09-09 format
        
        # found_recording_files = deepcopy(found_EEG_recording_files)
                found_recording_files = deepcopy(found_MOTION_recording_files)
            HistoricalData.read_exported_fif_files(found_recording_files=found_recording_files)

        """
        from phoofflineeeganalysis.analysis.MNE_helpers import DatasetDatetimeBoundsRenderingMixin, RawArrayExtended, RawExtended, up_convert_raw_objects, up_convert_raw_obj
        
        # import csv
        
        # def _subfn_load_eeg_bytes(path):
        #     with open(path, newline="") as f:
        #         reader = csv.reader(f)
        #         for row in reader:
        #             if not row: 
        #                 continue
        #             # skip timestamp (row[0])
        #             timestamp = int(row[0])
        #             raw_bytes = bytes(int(x.strip('\n')) for x in row[1:])
        #             yield (timestamp, raw_bytes)


        # all_data = []
        # all_times = []
        # datasets = []
        # n_records = []
        # src_file = []
        
        all_data = {}
        all_times = {}
        datasets = {}
        n_records = {}
        src_file = {}

        # expected_channels = cls.modality_channels_dict.get(file_type, None)
        


        # def _subfn_build_RawEEG_from_df(raw_df: pd.DataFrame, a_recording_file: Path, a_file_type='EEG'):
        def _subfn_build_RawFile_from_df(raw_df: pd.DataFrame, a_recording_file: Path, a_file_type='EEG'):
            """ 
            captures: cls, should_load_data, include_src_file_column
            captures and updates: all_data, all_times, datasets, n_records, src_file, expected_channels

            (data, times), raw, meas_datetime = _subfn_build_RawEEG_from_df(raw_df=raw_df, a_recording_file=a_recording_file)
            
            a_recording_file: USED FOR PARSING DATETIME FROM FILENAME AND A FEW OTHER THINGS!
            
            """
            curr_expected_channels = cls.modality_channels_dict.get(a_file_type, None)

            # ch_names = raw_df.columns.to_list()[1:]
            ch_names = raw_df.columns.to_list() ## Includes timestamp
            # if np.isin(ch_names, expected_channels).all():
            found_good_channels = np.array(curr_expected_channels)[np.isin(curr_expected_channels, ch_names)]
            if len(found_good_channels) == 0:
                raise ValueError(f'loaded CSV has the wrong channel names: ch_names: {ch_names}, curr_expected_channels: {curr_expected_channels}')
            sfreq = cls.modality_sfreq_dict['EEG']
            info = mne.create_info(ch_names = ch_names, sfreq = sfreq, ch_types=(['misc'] + ['eeg'] * (len(ch_names)-1)))
            # info.set_channel_types(mapping={ch: 'eeg' for ch in ch_names if ch != 'timestamp'})

            raw = mne.io.RawArray(raw_df.T, info)
            # raw.set_filename(a_recording_file.as_posix())
            meas_datetime = cls.get_or_parse_datetime_from_raw(raw, override_filepath=a_recording_file, allow_setting_meas_date_from_filename=allow_setting_meas_date_from_filename)

            if should_load_data:
                # raw.info.ch_names
                # raw.to_data_frame()
                data, times = raw.get_data(picks=constrain_channels, return_times=True)

                # Convert relative times to absolute timestamps (in seconds since epoch)
                start_time = meas_datetime.timestamp() if hasattr(meas_datetime, 'timestamp') else meas_datetime[0]
                abs_times = start_time + times
                a_n_records: int = len(times) ## n_records per src file
                
                if a_file_type not in all_data:
                    all_data[a_file_type] = []
                    
                if a_file_type not in all_times:
                    all_times[a_file_type] = []

                if a_file_type not in n_records:
                    n_records[a_file_type] = []

                all_data[a_file_type].append(data)
                all_times[a_file_type].append(abs_times)
                n_records[a_file_type].append(a_n_records) ## n_records per src file
                if include_src_file_column:
                    if a_file_type not in src_file:
                        src_file[a_file_type] = []
                    src_file[a_file_type].extend(([a_recording_file.stem] * a_n_records))
            else:
                # data = raw_df[[c for c in raw_df.columns if c not in ('timestamp')]].to_numpy()
                # times = raw_df['timestamp'].to_numpy()                
                data = None
                times = None
                
            # END if should_load_data
            
            ## Append to datasets
            if a_file_type not in datasets:
                datasets[a_file_type] = []
                
            if len(raw.times) == 0:
                raise ValueError(f"len(raw.times) == 0!! {len(raw.times) == 0}")
            else:
                up_convert_raw_obj(raw)
                
                datasets[a_file_type].append(raw)

            return (data, times), raw, meas_datetime


        # ==================================================================================================================================================================================================================================================================================== #
        # BEGIN FUNCTION BODY                                                                                                                                                                                                                                                                  #
        # ==================================================================================================================================================================================================================================================================================== #


        ## the two lengths are (65, 85)
        if (file_type == 'GENERIC'):
            print(f'GENERIC FILE PROCESSING -- found {len(found_recording_files)} files, loading only {(debug_n_max_files_to_load or len(found_recording_files))}: ')

        else:
            ## standard type (exported/decoded EEG or MOTION file output)
            print(f'file_type: {file_type} PROCESSING -- found {len(found_recording_files)} files, loading only {(debug_n_max_files_to_load or len(found_recording_files))}: ')
            
            assert file_type in ['EEG', 'MOTION'], f"file_type: {file_type} was no in the expected list!"
            data_channel_names = deepcopy(cls.modality_channels_dict[file_type])
            all_channel_names = ['timestamp'] + data_channel_names
            required_num_columns: int = len(all_channel_names) # len(channel_names) + 1 # +1 for the timestamp column
            required_channel_index_map = {ch_name:i for i, ch_name in enumerate(all_channel_names)} ## like {'timestamp': 0}

            ## These above properties are only used in the standard type mode



        for a_file_idx, a_recording_file in enumerate(found_recording_files):
            if (debug_n_max_files_to_load is not None) and ((a_file_idx+1) > debug_n_max_files_to_load):
                continue ## stop processing the rest

            if a_recording_file.exists():
                print(f'FILE[{a_file_idx}]: {a_recording_file.stem} | file_type: "{file_type}" PROCESSING:')
                try:
                    ## the two lengths are (65, 85)
                    if (file_type == 'GENERIC'):
                        # ## GENERIC TYPE:
                        # ## Process line-by-line to handle the variable line lengths
                        modality_out_dfs_dict, modality_out_linedata_df_dict = GenericRawDebugFileProcessor.main_perform_decode(a_recording_file=a_recording_file)
                        for a_modality_name, (final_modality_decoded_df, _remaineder_decoded_df) in modality_out_dfs_dict.items():
                            # final_eeg_decoded_df, _remaineder_decoded_df = modality_out_dfs_dict['EEG']
                            (data, times), raw, meas_datetime = _subfn_build_RawFile_from_df(raw_df=final_modality_decoded_df, a_recording_file=a_recording_file, a_file_type=a_modality_name)
                        
                        ## #TODO 2025-09-18 10:27: - [ ] we would add MOTION packets here

                    else:
                        ## standard type (exported/decoded EEG or MOTION file output)
                        raw_df: pd.DataFrame = pd.read_csv(a_recording_file, low_memory=False, skiprows=0, header=0) # , parse_dates=['timestamp']

                        # Loaded variable 'decoded_packets_list' from kernel state
                        raw_df['timestamp'] = pd.to_numeric(raw_df['timestamp'], errors='coerce') # .astype('int64') # Use non-nullable integer type
                        
                        for col in data_channel_names:
                            raw_df[col] = pd.to_numeric(raw_df[col], errors='coerce')

                        raw_df = raw_df.drop_duplicates() # Drop duplicate rows across all columns (importantly including 'timestamp')
                        raw_df = raw_df.sort_values(['timestamp']) # Sort by column: 'timestamp' (ascending)

                        ## Find rows with any missing elmeents:
                        df_is_na = raw_df.isna()
                        # num_missing_elements = df_is_na.sum(axis='columns')
                        num_valid_columns = np.logical_not(df_is_na).sum(axis='columns')

                        valid_eeg_row_record_indicies = (num_valid_columns == required_num_columns)
                        ## Just the EEG rows with the right number of columns:
                        final_eeg_decoded_df: pd.DataFrame = raw_df[valid_eeg_row_record_indicies]

                        if not np.all(list(final_eeg_decoded_df.columns) != all_channel_names):
                            print(f'WARN: reordering columns!')
                            ## Move timestamp column to the end:
                            final_eeg_decoded_df = reorder_columns_relative(final_eeg_decoded_df, column_names=required_channel_index_map, relative_mode='start')


                        (data, times), raw, meas_datetime = _subfn_build_RawFile_from_df(raw_df=final_eeg_decoded_df, a_recording_file=a_recording_file, a_file_type=file_type)
                        
                except (ValueError, AttributeError, TypeError) as e:
                    print(f'\tEncountered error: {e} while trying to read CSV file {a_recording_file}. Skipping')
                    # datasets.append(None)
                    pass
                except Exception as e:
                    raise
            ## END if a_recording_file.exists()
            
        ## END `for a_file_idx, a_recording_file in enu...`

        modality_dfs = {}

        if should_load_data:            
            for a_modality_name, a_datasets in datasets.items():
                if len(a_datasets) == 0:
                    df = None
                else:
                    # all_data_shapes = [np.shape(v) for v in all_data]
                    all_data_ch_names = [raw.info.ch_names for raw in a_datasets]

                    full_data = np.concatenate(all_data.get(a_modality_name, []), axis=1)  # concatenate over time
                    full_times = np.concatenate(all_times.get(a_modality_name, []))

                    if constrain_channels is None:
                        constrain_channels = all_data_ch_names[0] ## take the first list of channel names (all of them) as the channels for the data. They better be the same!
                    ## Hopefully same size as number of channels!
                    df = pd.DataFrame(full_data.T, columns=constrain_channels)
                    df['timestamp'] = full_times
                    df['timestamp_dt'] = pd.to_datetime(df['timestamp'], unit='s') ## add datetime column
                    if include_src_file_column:
                        assert len(src_file.get(a_modality_name, [])) == len(df), f"len(src_file): {len(src_file.get(a_modality_name, []))}, len(df): {len(df)}"
                        df['src_file'] = src_file.get(a_modality_name, [])
                    # Sort by column: 'timestamp_dt' (ascending)
                    df = df.sort_values(['timestamp_dt'], na_position='first').reset_index(drop=True)
                    # if file_type == 'MOTION':
                    #     ## Estimate Quaternions from motion data:
                    #     df = MotionData.compute_quaternions(df)
                    # elif file_type == 'EEG':
                    #     pass
                    # else:
                    #     pass            

                modality_dfs[a_modality_name] = df
            # df.to_csv('eeg_concatenated.csv', index=False)
        else:
            df = None


        return datasets, modality_dfs

        # return (all_data, all_times), datasets, df
        

    @classmethod
    def read_recording_files(cls, found_recording_file_modality_dict: Dict[str, List[Path]], should_load_data: bool=False):
        """ Load and process the found recording file types
            eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve()
            headset_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve()
            WhisperVideoTranscripts_LSL_Converted = Path(r"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted").resolve()

            flat_data_modality_dict, found_recording_file_modality_dict = HistoricalData.MAIN_process_recording_files(eeg_recordings_file_path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/fif').resolve(),
                            headset_motion_recordings_file_path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEpocX_EEGRecordings/MOTION_RECORDINGS/fif').resolve(),
                            WhisperVideoTranscripts_LSL_Converted = Path(r"E:/Dropbox (Personal)/Databases/UnparsedData/WhisperVideoTranscripts_LSL_Converted").resolve(),
            )
            flat_data_modality_dict
        """
        flat_data_modality_dict = {} ## For collecting outputs 
        ## Load and process the found recording file types:
        for k, a_found_recording_files in found_recording_file_modality_dict.items():
            flat_data_modality_dict[k] = cls.read_exported_fif_files(found_recording_files=a_found_recording_files, constrain_channels=cls.modality_channels_dict.get(k, None), file_type=k, should_load_data=should_load_data)
            # (all_data, all_times), datasets, df = flat_data_modality_dict[k] ## Unpacking
        return flat_data_modality_dict


    @classmethod
    def add_bad_periods_from_MOTION_data(cls,
            active_EEG_IDXs, datasets_EEG,
            active_motion_IDXs, datasets_MOTION, analysis_results_MOTION,
            preprocessed_EEG_save_path: Optional[Path]=None, debug_print: bool = False,
        ):
        """ Find periods that overlap the motion data
        
        from phoofflineeeganalysis.analysis.EEG_data import EEGData
        from phoofflineeeganalysis.analysis.motion_data import MotionData
        from phoofflineeeganalysis.analysis.historical_data import HistoricalData
        
        n_most_recent_sessions_to_preprocess: int = 10
        
        ## BEGIN ANALYSIS of EEG Data
        preprocessed_EEG_save_path: Path = eeg_analyzed_parent_export_path.joinpath('preprocessed_EEG').resolve()
        preprocessed_EEG_save_path.mkdir(exist_ok=True)

        ## INPUTS: flat_data_modality_dict
        (all_data_EEG, all_times_EEG), datasets_EEG, df_EEG = flat_data_modality_dict['EEG']  ## Unpacking

        active_EEG_IDXs, analysis_results_EEG = EEGData.preprocess(datasets_EEG=datasets_EEG, preprocessed_EEG_save_path=preprocessed_EEG_save_path, n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess)
        
        (all_data_MOTION, all_times_MOTION), datasets_MOTION, df_MOTION = flat_data_modality_dict['MOTION']  ## Unpacking

        (active_motion_IDXs, analysis_results_MOTION) = MotionData.preprocess(datasets_MOTION, n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess)

        dataset_MOTION_df, dataset_EEG_df = HistoricalData.add_bad_periods_from_MOTION_data(active_EEG_IDXs=active_EEG_IDXs, datasets_EEG=datasets_EEG,
                                                        active_motion_IDXs=active_motion_IDXs, datasets_MOTION=datasets_MOTION, analysis_results_MOTION=analysis_results_MOTION)

        """
        active_datasets_MOTION = [datasets_MOTION[i] for i in active_motion_IDXs]
        
        ## Find periods that overlap the motion data:
        ## INPUTS: active_motion_IDXs, active_datasets_MOTION
        dataset_MOTION_df = []
        # analysis_results_EEG
        for i, a_ds in enumerate(active_datasets_MOTION):
            ## Find all motion datasets that overlap:
            motion_dataset_IDX: int = active_motion_IDXs[i]
            # a_ds_EEG.last_samp
            abs_start_t = a_ds.times[0] + a_ds.info['meas_date'].timestamp()
            abs_end_t = a_ds.times[-1] + a_ds.info['meas_date'].timestamp()
            dataset_MOTION_df.append({'motion_dataset_IDX': motion_dataset_IDX, 'start_time': abs_start_t, 'end_time': abs_end_t})
            # display(abs_start_t, abs_end_t)

        dataset_MOTION_df = pd.DataFrame(dataset_MOTION_df)	

        ## Find periods that overlap the motion data:
        ## INPUTS: analysis_results_MOTION
        dataset_EEG_df = []
        # analysis_results_EEG
        # for i, a_ds_EEG in enumerate(datasets_EEG):
        for i, a_EEG_IDX in enumerate(active_EEG_IDXs):
            ## Find all motion datasets that overlap:
            a_ds_EEG = datasets_EEG[a_EEG_IDX]
            # datasets_MOTION
            abs_start_t = a_ds_EEG.times[0] + a_ds_EEG.info['meas_date'].timestamp()
            abs_end_t = a_ds_EEG.times[-1] + a_ds_EEG.info['meas_date'].timestamp()

            ## Find all motion datasets that overlap:
            mask = (abs_start_t <= dataset_MOTION_df['end_time']) & (dataset_MOTION_df['start_time'] <= abs_end_t) ## doesn't this mask miss cases where the a motion data starts or ends outside the EEG data?
            # mask = np.logical_or((abs_start_t <= dataset_MOTION_df['end_time']), (dataset_MOTION_df['start_time'] <= abs_end_t))
            df_MOTION_overlaps: pd.DataFrame = dataset_MOTION_df[mask].copy()
            # an_overlapping_motion_IDXs = df_MOTION_overlaps['motion_dataset_IDX'].to_list()
            an_overlapping_motion_IDXs = df_MOTION_overlaps.index.to_list()
            if debug_print:
                print(f'i: {i}, an_overlapping_motion_IDXs: {an_overlapping_motion_IDXs}')
            combined_annotations = None
            # Use the first dataset's orig_time as reference
            base_orig_time = None
            for an_overlapping_motion_IDX in an_overlapping_motion_IDXs: 
                if debug_print:
                    print(f'\tan_overlapping_motion_IDX: {an_overlapping_motion_IDX}')
                # motion_ds = datasets_MOTION[an_overlapping_motion_IDX]
                a_bad_periods_annot = analysis_results_MOTION[an_overlapping_motion_IDX]['bad_periods_annotations']['high_accel']
                if combined_annotations is None:
                    combined_annotations = a_bad_periods_annot.copy() ## first annotation
                    base_orig_time = combined_annotations.orig_time
                else:
                    # assert base_orig_time is not None
                    a_bad_periods_annot._orig_time = base_orig_time
                    combined_annotations += a_bad_periods_annot
            ## END for an_overlapping_motion_IDX in an_overlapping_motion_IDXs...
            ## Set the `combined_annotations` to the EEG dataset:
            if combined_annotations is not None:
                a_ds_EEG.set_annotations(combined_annotations)      

            if preprocessed_EEG_save_path is not None:
                if not preprocessed_EEG_save_path.exists():
                    preprocessed_EEG_save_path.mkdir(parents=True, exist_ok=True)
                a_raw_savepath: Path = preprocessed_EEG_save_path.joinpath(Path(a_ds_EEG.filenames[0]).name).resolve()
                print(f'saving to {a_raw_savepath}...', end='\t')
                a_ds_EEG.save(a_raw_savepath, overwrite=True)
                print(f'\tdone.')
            dataset_EEG_df.append({'dataset_IDX': a_EEG_IDX, 'start_time': abs_start_t, 'end_time': abs_end_t, 'motion_idxs': an_overlapping_motion_IDXs})
            # display(abs_start_t, abs_end_t)

        dataset_EEG_df = pd.DataFrame(dataset_EEG_df)	
    
        ## convert columns to datetime:    
        dataset_EEG_df = MNEHelpers.convert_df_columns_to_datetime(dataset_EEG_df, dt_col_names=["start_time", "end_time"])
        dataset_MOTION_df = MNEHelpers.convert_df_columns_to_datetime(dataset_MOTION_df, dt_col_names=["start_time", "end_time"])

        return (dataset_MOTION_df, dataset_EEG_df)


    @classmethod
    def add_additional_LOGGING_annotations(cls,
            active_EEG_IDXs, datasets_EEG,
            active_LOGGING_IDXs, datasets_LOGGING, analysis_results_LOGGING, logging_series_identifier: str = 'PHO_LOG', # ['PHO_LOG', 'WHISPER'],
            preprocessed_EEG_save_path: Optional[Path]=None, debug_print: bool = False,
        ):
        """ Find periods that overlap the motion data

        from phoofflineeeganalysis.analysis.EEG_data import EEGData
        from phoofflineeeganalysis.analysis.motion_data import MotionData
        from phoofflineeeganalysis.analysis.historical_data import HistoricalData

        n_most_recent_sessions_to_preprocess: int = 10

        ## BEGIN ANALYSIS of EEG Data
        preprocessed_EEG_save_path: Path = eeg_analyzed_parent_export_path.joinpath('preprocessed_EEG').resolve()
        preprocessed_EEG_save_path.mkdir(exist_ok=True)

        ## INPUTS: flat_data_modality_dict
        (all_data_EEG, all_times_EEG), datasets_EEG, df_EEG = flat_data_modality_dict['EEG']  ## Unpacking

        active_EEG_IDXs, analysis_results_EEG = EEGData.preprocess(datasets_EEG=datasets_EEG, preprocessed_EEG_save_path=preprocessed_EEG_save_path, n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess)

        (all_data_MOTION, all_times_MOTION), datasets_MOTION, df_MOTION = flat_data_modality_dict['MOTION']  ## Unpacking

        (active_motion_IDXs, analysis_results_MOTION) = MotionData.preprocess(datasets_MOTION, n_most_recent_sessions_to_preprocess=n_most_recent_sessions_to_preprocess)

        dataset_LOGGING_df, dataset_EEG_df = HistoricalData.add_additional_LOGGING_annotations(active_EEG_IDXs=active_EEG_IDXs, datasets_EEG=datasets_EEG,
                                                        active_LOGGING_IDXs=active_LOGGING_IDXs, datasets_LOGGING=datasets_MOTION, analysis_results_LOGGING=analysis_results_LOGGING, logging_series_identifier='PHO_LOG',
                                                        preprocessed_EEG_save_path=None)

        """
        active_datasets_LOGGING = [datasets_LOGGING[i] for i in active_LOGGING_IDXs]
        curr_logging_datset_index_col_name: str = f"{logging_series_identifier}_dataset_IDX"
        curr_logging_dataset_matching_indicies_col_name: str = f'{logging_series_identifier}_idxs' ## the column added to the EEG dataframe
        

        ## Find periods that overlap the motion data:
        ## INPUTS: active_motion_IDXs, active_datasets_MOTION
        dataset_LOGGING_df = []
        # analysis_results_EEG
        for i, a_ds in enumerate(active_datasets_LOGGING):
            ## Find all motion datasets that overlap:
            curr_dataset_IDX: int = active_LOGGING_IDXs[i]
            # a_ds_EEG.last_samp
            abs_start_t = a_ds.times[0] + a_ds.info['meas_date'].timestamp()
            abs_end_t = a_ds.times[-1] + a_ds.info['meas_date'].timestamp()
            # dataset_LOGGING_df.append({'motion_dataset_IDX': motion_dataset_IDX, 'start_time': abs_start_t, 'end_time': abs_end_t})
            dataset_LOGGING_df.append({'dataset_IDX': curr_dataset_IDX, curr_logging_datset_index_col_name: curr_dataset_IDX, 'start_time': abs_start_t, 'end_time': abs_end_t}) # 'motion_dataset_IDX': curr_dataset_IDX
            # display(abs_start_t, abs_end_t)
        ## END for i, a_ds in enumerate(active_datasets_LOGGING)...
        
        dataset_LOGGING_df = pd.DataFrame(dataset_LOGGING_df)	

        ## Find periods that overlap the motion data:
        ## INPUTS: analysis_results_MOTION
        dataset_EEG_df = []
        # analysis_results_EEG
        # for i, a_ds_EEG in enumerate(datasets_EEG):
        for i, a_EEG_IDX in enumerate(active_EEG_IDXs):
            ## Find all motion datasets that overlap:
            a_ds_EEG = datasets_EEG[a_EEG_IDX]
            # datasets_MOTION
            abs_start_t = a_ds_EEG.times[0] + a_ds_EEG.info['meas_date'].timestamp()
            abs_end_t = a_ds_EEG.times[-1] + a_ds_EEG.info['meas_date'].timestamp()

            ## Find all motion datasets that overlap:
            mask = (abs_start_t <= dataset_LOGGING_df['end_time']) & (dataset_LOGGING_df['start_time'] <= abs_end_t) ## doesn't this mask miss cases where the a motion data starts or ends outside the EEG data?
            # mask = np.logical_or((abs_start_t <= dataset_MOTION_df['end_time']), (dataset_MOTION_df['start_time'] <= abs_end_t))
            df_LOGGING_overlaps: pd.DataFrame = dataset_LOGGING_df[mask].copy()
            # an_overlapping_motion_IDXs = df_MOTION_overlaps['motion_dataset_IDX'].to_list()
            an_overlapping_logging_IDXs = df_LOGGING_overlaps.index.to_list()
            if debug_print:
                print(f'i: {i}, an_overlapping_motion_IDXs: {an_overlapping_logging_IDXs}')
            # combined_annotations = None
            for an_overlapping_logging_IDX in an_overlapping_logging_IDXs: 
                if debug_print:
                    print(f'\tan_overlapping_motion_IDX: {an_overlapping_logging_IDX}')
                # motion_ds = datasets_MOTION[an_overlapping_motion_IDX]
                curr_annot = active_datasets_LOGGING[an_overlapping_logging_IDX].annotations ## should this be into the active datasets of all? ANSWER: active_datasets_LOGGING
                if curr_annot is not None:
                    a_ds_EEG = MNEHelpers.merge_annotations(a_ds_EEG, curr_annot) ## why does this use `MNEHelpers.merge_annotations(...)`?
                    
                    # if combined_annotations is None:
                    #     combined_annotations = curr_annot.copy()                                                                                                
                    # else:
                    #     combined_annotations += curr_annot
                        
            ## END for an_overlapping_motion_IDX in an_overlapping_motion_IDXs...
            ## Set the `combined_annotations` to the EEG dataset:
            # if combined_annotations is not None:
            #     # a_ds_EEG.set_annotations(combined_annotations) ## overwrite existing annotations
            #     a_ds_EEG.set_annotations(a_ds_EEG.annotations + combined_annotations)

            if preprocessed_EEG_save_path is not None:
                if not preprocessed_EEG_save_path.exists():
                    preprocessed_EEG_save_path.mkdir(parents=True, exist_ok=True)
                a_raw_savepath: Path = preprocessed_EEG_save_path.joinpath(a_ds_EEG.filenames[0].name).resolve()
                print(f'saving to {a_raw_savepath}...', end='\t')
                a_ds_EEG.save(a_raw_savepath, overwrite=True)
                print(f'\tdone.')                
            ## END if preprocessed_EEG_save_path is not None...
            
            dataset_EEG_df.append({'dataset_IDX': a_EEG_IDX, 'start_time': abs_start_t, 'end_time': abs_end_t, curr_logging_dataset_matching_indicies_col_name: an_overlapping_logging_IDXs})
            # display(abs_start_t, abs_end_t)
        ## END for i, a_EEG_IDX in enumerate(active_EEG_IDXs)...
        
        dataset_EEG_df = pd.DataFrame(dataset_EEG_df)	

        ## convert columns to datetime:    
        dataset_EEG_df = MNEHelpers.convert_df_columns_to_datetime(dataset_EEG_df, dt_col_names=["start_time", "end_time"])
        dataset_LOGGING_df = MNEHelpers.convert_df_columns_to_datetime(dataset_LOGGING_df, dt_col_names=["start_time", "end_time"])

        return (dataset_LOGGING_df, dataset_EEG_df)


    @classmethod
    def build_file_comparison_df(cls, recording_files: List[Path]) -> pd.DataFrame:
        """ returns a dataframe with info about each file such as their modification time, etc
        
        Usage:
        
            pre_processed_EEG_recording_files = HistoricalData.get_recording_files(recordings_dir=sso.eeg_analyzed_parent_export_path)
            pre_processed_EEG_recording_file_df: pd.DataFrame = HistoricalData.build_file_comparison_df(recording_files=pre_processed_EEG_recording_files)
            pre_processed_EEG_recording_file_df

            ## OUTPUTS: pre_processed_EEG_recording_file_df, pre_processed_EEG_recording_files
            
            
            modern_found_EEG_recording_files = HistoricalData.get_recording_files(recordings_dir=sso.eeg_recordings_file_path)
            modern_found_EEG_recording_file_df: pd.DataFrame = HistoricalData.build_file_comparison_df(recording_files=modern_found_EEG_recording_files)
            modern_found_EEG_recording_file_df

            ## OUTPUTS: modern_found_EEG_recording_file_df, modern_found_EEG_recording_files
            
            
            
            
        """
        metadata_key_dict = {'ctime':'st_ctime', 'size':'st_size', 'mtime':'st_mtime'}
        datetime_col_names = ['start_t', 'ctime', 'mtime']
        _out_df = []
        for a_file in recording_files:
            if a_file.exists():
                raw = read_raw(a_file, preload=False)
                meas_datetime = HistoricalData.get_or_parse_datetime_from_raw(raw, allow_setting_meas_date_from_filename=True)
                start_time = meas_datetime.timestamp() if hasattr(meas_datetime, 'timestamp') else meas_datetime[0]
                a_file_metadata = a_file.stat()
                a_file_metadata_dict = {col_name:getattr(a_file_metadata, file_metadata_key) for col_name, file_metadata_key in metadata_key_dict.items()}
                _out_df.append({'src_file_name': a_file.stem, 'start_t': start_time, 'src_file': a_file.as_posix(), 'meas_datetime':meas_datetime, **a_file_metadata_dict})

        df = pd.DataFrame.from_records(_out_df) # , index='src_file_name'
        # df['timestamp_dt'] = pd.to_datetime(df['start_t'], unit='s') ## add datetime column
        # for a_col_name in list(metadata_key_dict.keys()):
        for a_col_name in datetime_col_names:
            # df[f'{a_col_name}_dt'] = pd.to_datetime(df[a_col_name], unit='s') 
            df[a_col_name] = pd.to_datetime(df[a_col_name], unit='s') 

        # df = df.sort_values(['meas_datetime', 'ctime', 'mtime', 'size', 'src_file_name', 'src_file'], na_position='first') # .reset_index(drop=True)    
        df = df.sort_values(['meas_datetime', 'ctime', 'mtime', 'size', 'src_file'], ignore_index=True, inplace=False, ascending=False, na_position='last') #.reset_index(drop=True)
        df = df.drop_duplicates(subset=['src_file', 'meas_datetime', 'start_t'], inplace=False, ignore_index=True, keep='first')

        return df
    

    @classmethod
    def discover_updated_recording_files(cls, eeg_recordings_file_path: Path, eeg_analyzed_parent_export_path: Path):
        """ discover recording files that have been updated since the last run of the script
        
        updated_file_paths, (pending_updated_recording_file_df, modern_found_EEG_recording_file_df, pre_processed_EEG_recording_file_df) = HistoricalData.discover_updated_recording_files(eeg_recordings_file_path=sso.eeg_recordings_file_path, eeg_analyzed_parent_export_path=sso.eeg_analyzed_parent_export_path)
        """
        pre_processed_EEG_recording_files = cls.get_recording_files(recordings_dir=eeg_analyzed_parent_export_path)
        pre_processed_EEG_recording_file_df: pd.DataFrame = cls.build_file_comparison_df(recording_files=pre_processed_EEG_recording_files)
        
        modern_found_EEG_recording_files = cls.get_recording_files(recordings_dir=eeg_recordings_file_path)
        modern_found_EEG_recording_file_df: pd.DataFrame = cls.build_file_comparison_df(recording_files=modern_found_EEG_recording_files)
        
        file_identifier_col_name: str = 'src_file_name'
        # file_identifier_col_name: str = 'src_file'
        modern_files = set(modern_found_EEG_recording_file_df[file_identifier_col_name].values).difference()
        prev_files = set(pre_processed_EEG_recording_file_df[file_identifier_col_name].values)

        modern_only_files = modern_files.difference(prev_files)

        prev_only_files = prev_files.difference(modern_files)
        # prev_only_files
        print(f'prev_only_files: {prev_only_files}')
        
        pending_updated_recording_file_df = deepcopy(modern_found_EEG_recording_file_df).join(deepcopy(pre_processed_EEG_recording_file_df).set_index(file_identifier_col_name), on=file_identifier_col_name, how='left', rsuffix='_prev')
        was_updated = (pending_updated_recording_file_df['size'] > pending_updated_recording_file_df['size_prev']) ## size should be strictly less than the prev_size, which the prev size has a bunch of other data.
        was_updated = np.logical_or(was_updated, (pending_updated_recording_file_df['mtime'] < pending_updated_recording_file_df['mtime_prev'])) # < means modification time is NEWER than the 'mtime_prev'
        was_updated = np.logical_or(was_updated, pending_updated_recording_file_df['src_file_prev'].isna())
        pending_updated_recording_file_df['was_updated'] = deepcopy(was_updated)
        
        # pending_updated_recording_file_df = pending_updated_recording_file_df[was_updated] ## get only the updated files
        pending_updated_only_recording_file_df = deepcopy(pending_updated_recording_file_df[was_updated])
        
        # updated_file_paths = [Path(v) for v in pending_updated_recording_file_df['src_file'].values.tolist()]
        updated_file_paths = [Path(v) for v in pending_updated_only_recording_file_df['src_file'].values.tolist()]
        
        return updated_file_paths, (pending_updated_recording_file_df, modern_found_EEG_recording_file_df, pre_processed_EEG_recording_file_df)
        
        

        

# datasets = []
# mne.viz.set_browser_backend("Matplotlib")

# data = read_raw("E:\Dropbox (Personal)\Databases\UnparsedData\EmotivEpocX_EEGRecordings\fif\20250808-052237-Epoc X-raw.fif", preload=True)
# datasets.insert(0, data)
# data = read_raw("E:\Dropbox (Personal)\Databases\UnparsedData\EmotivEpocX_EEGRecordings\fif\20250808-015634-Epoc X-raw.fif", preload=True)
# datasets.insert(1, data)
# data.plot(events=events, n_channels=15)
# datasets.insert(2, deepcopy(data))
# data = datasets[2]
# mne.concatenate_raws(data, datasets[0])
