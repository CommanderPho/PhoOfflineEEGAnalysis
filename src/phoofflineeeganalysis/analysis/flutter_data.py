import time
import re
from datetime import datetime, timezone

import uuid
from copy import deepcopy
from typing import Dict, List, Tuple, Optional, Callable, Union, Any
from nptyping import NDArray
from matplotlib import pyplot as plt

from pathlib import Path
import numpy as np
import pandas as pd

import mne
from mne import set_log_level
from copy import deepcopy
import mne

from phoofflineeeganalysis.analysis.motion_data import MotionData ## for creating single EDF+ files containing channel with different sampling rates (e.g. EEG and MOTION data)

# from phoofflineeeganalysis.EegProcessing import bandpower
from phoofflineeeganalysis.analysis.MNE_helpers import MNEHelpers
# from ..EegProcessing import bandpower
from numpy.typing import NDArray
# from nptyping import NDArray

set_log_level("WARNING")

## Decode the GENERIC with `from emotiv-lsl.emotiv_lsl.emotiv_epoc_x import EmotivEpocX`
import os, sys
# os.environ["PYTHONPATH"] = "C:/Users/pho/repos/EmotivEpoc/emotiv-lsl:" + os.environ.get("PYTHONPATH", "")

from phoofflineeeganalysis.helpers.indexing_helpers import reorder_columns_relative
from phoofflineeeganalysis.analysis.historical_data import HistoricalData


class GenericRawDebugFileProcessor:
    """ processes files generated by Flutter after 2025-09-10 which contain the raw (non-decoded/decrypted) data spat out from the BLE Flutter app.
    
    #TODO 2025-09-11 23:08: - [ ] EEFICIENCY MAJOR - hold an instance to `emotiv_epoc_x` instead of making fresh each time we want to decode (each file)

    CAPTURES: emotiv_epoc_x

    from phoofflineeeganalysis.analysis.historical_data import GenericRawDebugFileProcessor

    """
    # line_length_to_modality_dict = {65: 'MOTION', 85: 'EEG'}
    modality_to_line_length_dict = {'MOTION':65, 'EEG':85}    
    modality_to_num_valid_columns_dict = {'MOTION': 7, 'EEG': 31}
    emotiv_epoc_x = None
    
    @classmethod
    def decode_raw_generic_eeg_packets(cls, eeg_line_data):

        def _subfn_clean_packet(pkt: bytes) -> bytes:
            # Drop trailing 0xFFs
            pkt = pkt.rstrip(b'\xff')
            # Keep only full 16-byte blocks
            valid_len = (len(pkt) // 16) * 16
            return pkt[:valid_len]


        # skip timestamp (row[0])
        timestamps: NDArray = np.array([int(row[0].strip('\n')) for row in eeg_line_data])
        raw_bytes_list = [bytes(int(x.strip('\n')) for x in row[1:]) for row in eeg_line_data]

        ## len should be the same len(timestamps) == len(raw_bytes_list)


        raw_bytes_list_length = np.array([len(v) for v in raw_bytes_list])
        raw_bytes_list_length

        cleaned_raw_bytes_list = [_subfn_clean_packet(p) for p in raw_bytes_list]
        cleaned_raw_bytes_list_length = np.array([len(v) for v in cleaned_raw_bytes_list])
        cleaned_raw_bytes_list_length
        # OUTPUTS: timestamps, cleaned_raw_bytes_list

        # Clean packet using `emotiv_lsl`'s `emotiv_epoc_x.EmotivEpocX` class: _______________________________________________________________________________________________________________________________________________________________________________________________________________ #
        if cls.emotiv_epoc_x is None:
            print(f"Performing one-time init of `cls.emotiv_epoc_x`...")
            sys.path.append("C:/Users/pho/repos/EmotivEpoc/emotiv-lsl")
            from emotiv_lsl.emotiv_epoc_x import EmotivBase, EmotivEpocX
            crypto_key = bytearray(b'6566565666756557') # emotiv_epoc_x.get_crypto_key()
            print(f'crypto_key: {crypto_key}')
            cls.emotiv_epoc_x = EmotivEpocX.init_with_serial(serial_number=None, cryptokey=crypto_key, enable_electrode_quality_stream=False)
            cls.emotiv_epoc_x.enable_electrode_quality_stream = False
            

        # decoded_packets_list = [emotiv_epoc_x.decode_data(a_packet) for a_packet in raw_bytes_list]
        decoded_packets_list = [np.array(cls.emotiv_epoc_x.decode_data(a_packet)[0]) for a_packet in cleaned_raw_bytes_list] ## only extract the packet data (packet_data, eeg_quality_data)

        # Loaded variable 'decoded_packets_list' from kernel state
        decoded_packets_list_df: pd.DataFrame = pd.DataFrame(decoded_packets_list)
        decoded_packets_list_df['timestamp'] = timestamps
        decoded_packets_list_df = decoded_packets_list_df.drop_duplicates() # Drop duplicate rows across all columns (importantly including 'timestamp')
        decoded_packets_list_df = decoded_packets_list_df.sort_values(['timestamp']) # Sort by column: 'timestamp' (ascending)
        decoded_packets_list_df

        ## Find rows with any missing elmeents:
        df_is_na = decoded_packets_list_df.isna()
        # num_missing_elements = df_is_na.sum(axis='columns')
        num_valid_columns = np.logical_not(df_is_na).sum(axis='columns')

        
        num_valid_columns.value_counts()

        valid_eeg_indicies = (num_valid_columns == cls.modality_to_num_valid_columns_dict['EEG'])
        # valid_eeg_indicies
        ## Just the EEG rows with the right number of columns:
        valid_eeg_decoded_df: pd.DataFrame = decoded_packets_list_df[valid_eeg_indicies]
        valid_eeg_decoded_df


        ## valid_eeg_decoded_df - comes in with 30 columns (way more than the num of EEG channels, so we need to get only the EEG data (hope it's right):
        ## INPUTS: valid_eeg_decoded_df, emotiv_epoc_x
        # channel_names = deepcopy(cls.emotiv_epoc_x.eeg_channel_names)
        channel_names = deepcopy(HistoricalData.modality_channels_dict['EEG'])
        
        # eeg_channel_names
        # len(eeg_channel_names)
        # final_df_col_names = ['timestamp'] + eeg_channel_names
        final_eeg_decoded_df: pd.DataFrame = valid_eeg_decoded_df.iloc[:, :len(channel_names)] ## get the first data columns
        final_eeg_decoded_df.columns = channel_names ## set the EEG data column names
        final_eeg_decoded_df['timestamp'] = deepcopy(valid_eeg_decoded_df['timestamp']) # copy the timestamp column 
        _remaineder_decoded_df: pd.DataFrame = valid_eeg_decoded_df.iloc[:, len(channel_names):-2]
        _remaineder_decoded_df['timestamp'] = deepcopy(valid_eeg_decoded_df['timestamp']) # copy the timestamp column to the _remaineder_decoded_df too, just in case

        ## Move timestamp column to the end:
        final_eeg_decoded_df = reorder_columns_relative(final_eeg_decoded_df, column_names={'timestamp': 0}, relative_mode='start')
        _remaineder_decoded_df = reorder_columns_relative(_remaineder_decoded_df, column_names={'timestamp': 0}, relative_mode='start')

        ## OUTPUTS: final_eeg_decoded_df, _remaineder_decoded_df
        return (final_eeg_decoded_df, _remaineder_decoded_df)

    @classmethod
    def decode_raw_generic_MOTION_packets(cls, line_data):
        """ 2025-09-12 - Not throuroughly tested. Is decryption even needed for motion data? Or just decoding?
        """

        modality_name: str = 'MOTION'
        
        def _subfn_clean_packet(pkt: bytes) -> bytes:
            # Drop trailing 0xFFs
            pkt = pkt.rstrip(b'\xff')
            # Keep only full 16-byte blocks
            valid_len = (len(pkt) // 16) * 16
            return pkt[:valid_len]


        # skip timestamp (row[0])
        timestamps: NDArray = np.array([int(row[0].strip('\n')) for row in line_data])
        line_data: List[NDArray] = [np.array([int(x.strip('\n')) for x in row[1:]]) for row in line_data] ## lists of integers
        
        
        # raw_bytes_list = [bytes(int(x.strip('\n')) for x in row[1:]) for row in line_data]
        raw_bytes_list = [bytes(row) for row in line_data] ## arrays to bytes

        # _debug_out_file_path = Path(r'C:\Users\pho\repos\EmotivEpoc\PhoLabStreamingReceiver\data\raw_bytes_list.txt').resolve()
        # with open(_debug_out_file_path, "w") as f:
        #     f.writelines([str(v) for v in raw_bytes_list])
        #     # f.write(str(raw_bytes_list))
            
        # ## len should be the same len(timestamps) == len(raw_bytes_list)


        raw_bytes_list_length = np.array([len(v) for v in raw_bytes_list])
        
        cleaned_raw_bytes_list = [_subfn_clean_packet(p) for p in raw_bytes_list]
        cleaned_raw_bytes_list_length = np.array([len(v) for v in cleaned_raw_bytes_list])
        cleaned_raw_bytes_list_length
        # OUTPUTS: timestamps, cleaned_raw_bytes_list

        # Clean packet using `emotiv_lsl`'s `emotiv_epoc_x.EmotivEpocX` class: _______________________________________________________________________________________________________________________________________________________________________________________________________________ #
        if cls.emotiv_epoc_x is None:
            print(f"Performing one-time init of `cls.emotiv_epoc_x`...")
            sys.path.append("C:/Users/pho/repos/EmotivEpoc/emotiv-lsl")
            from emotiv_lsl.emotiv_epoc_x import EmotivBase, EmotivEpocX
            crypto_key = bytearray(b'6566565666756557') # emotiv_epoc_x.get_crypto_key()
            print(f'crypto_key: {crypto_key}')
            cls.emotiv_epoc_x = EmotivEpocX.init_with_serial(serial_number=None, cryptokey=crypto_key, enable_electrode_quality_stream=False)
            cls.emotiv_epoc_x.enable_electrode_quality_stream = False


        # decoded_packets_list = [emotiv_epoc_x.decode_data(a_packet) for a_packet in raw_bytes_list]
        decoded_packets_list = [np.array(cls.emotiv_epoc_x.decode_data(a_packet)[0]) for a_packet in cleaned_raw_bytes_list] ## .decode_data(...) IS CORRECT (not .decode_motion_data(...) 
        
        # Loaded variable 'decoded_packets_list' from kernel state
        decoded_packets_list_df: pd.DataFrame = pd.DataFrame(decoded_packets_list)
        decoded_packets_list_df['timestamp'] = timestamps
        decoded_packets_list_df = decoded_packets_list_df.drop_duplicates() # Drop duplicate rows across all columns (importantly including 'timestamp')
        decoded_packets_list_df = decoded_packets_list_df.sort_values(['timestamp']) # Sort by column: 'timestamp' (ascending)
        decoded_packets_list_df

        ## Find rows with any missing elmeents:
        df_is_na = decoded_packets_list_df.isna()
        # num_missing_elements = df_is_na.sum(axis='columns')
        num_valid_columns = np.logical_not(df_is_na).sum(axis='columns')
        # num_valid_columns.value_counts()

        valid_eeg_indicies = (num_valid_columns == cls.modality_to_num_valid_columns_dict[modality_name])
        # valid_eeg_indicies
        ## Just the EEG rows with the right number of columns:
        valid_decoded_df: pd.DataFrame = decoded_packets_list_df[valid_eeg_indicies]


        ## valid_eeg_decoded_df - comes in with 30 columns (way more than the num of EEG channels, so we need to get only the EEG data (hope it's right):
        ## INPUTS: valid_eeg_decoded_df, emotiv_epoc_x
                
        channel_names = deepcopy(HistoricalData.modality_channels_dict[modality_name])
        # eeg_channel_names
        # len(eeg_channel_names)
        # final_df_col_names = ['timestamp'] + eeg_channel_names
        final_motion_decoded_df: pd.DataFrame = valid_decoded_df.iloc[:, :len(channel_names)] ## get the first data columns
        final_motion_decoded_df.columns = channel_names ## set the EEG data column names
        final_motion_decoded_df['timestamp'] = deepcopy(valid_decoded_df['timestamp']) # copy the timestamp column 
        _remaineder_decoded_df: pd.DataFrame = valid_decoded_df.iloc[:, len(channel_names):-2]
        _remaineder_decoded_df['timestamp'] = deepcopy(valid_decoded_df['timestamp']) # copy the timestamp column to the _remaineder_decoded_df too, just in case

        ## Move timestamp column to the end:
        final_motion_decoded_df = reorder_columns_relative(final_motion_decoded_df, column_names={'timestamp': 0}, relative_mode='start')
        _remaineder_decoded_df = reorder_columns_relative(_remaineder_decoded_df, column_names={'timestamp': 0}, relative_mode='start')

        ## OUTPUTS: final_eeg_decoded_df, _remaineder_decoded_df
        return (final_motion_decoded_df, _remaineder_decoded_df)




    @classmethod
    def main_perform_decode(cls, a_recording_file: Path):
        """ #TODO 2025-09-11 22:51: - [ ] this was converted from an intermediate output data format, so parsing to data packet can be made direct

            final_eeg_decoded_df, _remaineder_decoded_df = GenericRawDebugFileProcessor.main_perform_decode(a_recording_file=a_recording_file)
        """
        print(f'GENERIC FILE PROCESSING')
        ## GENERIC TYPE:
        ## Process line-by-line to handle the variable line lengths
        line_parts_by_length: Dict[int, List[str]] = {}
        # _subfn_load_eeg_bytes(a_recording_file)

        with open(a_recording_file, 'r', newline='\n') as f:
            lines = f.readlines()
            for line in lines:
                line = line.strip('\n').strip('\r')
                line_parts = line.split(',')
                num_entries = len(line_parts)
                if num_entries not in line_parts_by_length:
                    line_parts_by_length[num_entries] = []
                line_parts_by_length[num_entries].append(line_parts)


        ## END with open ...
        # full_data = np.concatenate(all_data, axis=1)  # concatenate over time
        # full_times = np.concatenate(all_times)
        # num_records_per_length = {k:len(v) for k, v in line_parts_by_length.items()}        
        modality_to_processing_fn_dict = {'MOTION': cls.decode_raw_generic_MOTION_packets, 'EEG': cls.decode_raw_generic_eeg_packets}
        modality_out_linedata_df_dict = {'MOTION': None, 'EEG': None}   
        modality_out_dfs_dict = {'MOTION': None, 'EEG': None}
        for a_modality, a_length in cls.modality_to_line_length_dict.items():
            modality_line_data: List = line_parts_by_length.get(a_length, [])
            # modality_out_linedata_dict[a_modality] = modality_line_data
            modality_out_linedata_df_dict[a_modality] = pd.DataFrame(modality_line_data)
            modality_out_dfs_dict[a_modality] = modality_to_processing_fn_dict[a_modality](modality_line_data) ## uses `cls.decode_raw_generic_eeg_packets(...)`

        ## UNPACK LIKE:
        # decoded_df, _remaineder_decoded_df = modality_out_dfs_dict['MOTION']
        
        # eeg_line_data: List = line_parts_by_length.get(cls.modality_to_line_length_dict['EEG'], [])
        ## OUTPUTS: eeg_line_data
        # final_eeg_decoded_df, _remaineder_decoded_df = cls.decode_raw_generic_eeg_packets(eeg_line_data=eeg_line_data) ## uses `cls.decode_raw_generic_eeg_packets(...)`

        # return final_eeg_decoded_df, _remaineder_decoded_df
        return modality_out_dfs_dict, modality_out_linedata_df_dict


class FlutterExportedData:
    """ Uses `GenericRawDebugFileProcessor` as one of the three .csv file formats
    
    """
    @classmethod
    def try_load_flutter_recordings(cls, 
                                    flutter_eeg_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings').resolve(),
                                    flutter_motion_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/MOTION_RECORDINGS').resolve(),
                                    flutter_GENERIC_recordings_file_path: Path = Path(r'E:/Dropbox (Personal)/Databases/UnparsedData/EmotivEEG_FlutterRecordings/GENERIC_RECORDINGS').resolve(),
                                    should_load_data = True,
                                    should_process: bool=False,
                                    n_most_recent_sessions_to_preprocess: int = 5,
                                    ):

        # filenames are like: "eeg_data_2025-08-12T02-56-32.509841.csv" for EEG and "motion_data_2025-08-12T02-56-32.537110.csv" for motion All files are in .csv format
        # "timestamp	AF3	F7	F3	FC5	T7	P7	O1	O2	P8	T8	FC6	F4	F8	AF4"

        found_recording_file_modality_dict = {} ## for finding input files
        flat_data_modality_dict = {} ## For collecting outputs 

        if flutter_eeg_recordings_file_path is not None:
            assert flutter_eeg_recordings_file_path.exists()
            found_flutter_EEG_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_eeg_recordings_file_path, recordings_extensions=[".csv"])
            found_recording_file_modality_dict.update({'EEG': found_flutter_EEG_recording_files})

        # assert flutter_motion_recordings_file_path.exists()
        # found_MOTION_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_motion_recordings_file_path, recordings_extensions=[".csv"])
        # found_recording_file_modality_dict.update({'MOTION': found_MOTION_recording_files})

        if flutter_GENERIC_recordings_file_path is not None:
            assert flutter_GENERIC_recordings_file_path.exists()
            found_GENERIC_recording_files = HistoricalData.get_recording_files(recordings_dir=flutter_GENERIC_recordings_file_path, recordings_extensions=[".csv"])
            found_recording_file_modality_dict.update({'GENERIC': found_GENERIC_recording_files})


        # a_recording_file = found_flutter_EEG_recording_files[-1]
        # raw_df: pd.DataFrame = pd.read_csv(a_recording_file, low_memory=False, skiprows=0, header=0, parse_dates=['timestamp'])
        # # ch_names = raw_df.columns.to_list()[1:]
        # ch_names = raw_df.columns.to_list() ## Includes timestamp
        # sfreq = modality_sfreq_dict['EEG']
        # info = mne.create_info(ch_names = ch_names, sfreq = sfreq)
        # raw = mne.io.RawArray(raw_df.T, info)
        # raw
        # raw_df.columns
        if should_process:
            ## Load and process the found recording file types:
            for k, a_found_recording_files in found_recording_file_modality_dict.items():
                constrain_channels = HistoricalData.modality_channels_dict.get(k, None)
                # constrain_channels = None
                # flat_data_modality_dict[k] = HistoricalData.read_exported_csv_files(found_recording_files=a_found_recording_files, constrain_channels=constrain_channels, file_type=k, should_load_data=should_load_data,
                #                                                                      debug_n_max_files_to_load=n_most_recent_sessions_to_preprocess)

                flat_data_modality_dict[k] = HistoricalData.read_exported_csv_files(found_recording_files=a_found_recording_files, constrain_channels=constrain_channels, file_type=k, should_load_data=should_load_data,
                                                                                    debug_n_max_files_to_load=n_most_recent_sessions_to_preprocess)

                # flat_data_modality_dict[k] = modality_out_dfs_dict
                # (all_data, all_times), datasets, df = flat_data_modality_dict[k] ## Unpacking
                # modality_datasets, modality_df = flat_data_modality_dict[k] ## Unpacking

        return flat_data_modality_dict, found_recording_file_modality_dict

